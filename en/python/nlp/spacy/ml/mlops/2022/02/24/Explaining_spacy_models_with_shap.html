<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Explaining spaCy Models with Shap | TIL</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Explaining spaCy Models with Shap" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Using explainability to understand a NLP algorithm trained with spaCy" />
<meta property="og:description" content="Using explainability to understand a NLP algorithm trained with spaCy" />
<link rel="canonical" href="https://ycouble.github.io/til/en/python/nlp/spacy/ml/mlops/2022/02/24/Explaining_spacy_models_with_shap.html" />
<meta property="og:url" content="https://ycouble.github.io/til/en/python/nlp/spacy/ml/mlops/2022/02/24/Explaining_spacy_models_with_shap.html" />
<meta property="og:site_name" content="TIL" />
<meta property="og:image" content="https://ycouble.github.io/til/images/explain_nlp_cover.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-02-24T00:00:00-06:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://ycouble.github.io/til/images/explain_nlp_cover.png" />
<meta property="twitter:title" content="Explaining spaCy Models with Shap" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-02-24T00:00:00-06:00","datePublished":"2022-02-24T00:00:00-06:00","description":"Using explainability to understand a NLP algorithm trained with spaCy","headline":"Explaining spaCy Models with Shap","image":"https://ycouble.github.io/til/images/explain_nlp_cover.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://ycouble.github.io/til/en/python/nlp/spacy/ml/mlops/2022/02/24/Explaining_spacy_models_with_shap.html"},"url":"https://ycouble.github.io/til/en/python/nlp/spacy/ml/mlops/2022/02/24/Explaining_spacy_models_with_shap.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/til/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://ycouble.github.io/til/feed.xml" title="TIL" /><!-- the google_analytics_id gets auto inserted from the config file -->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-09HCYPQMC0"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-09HCYPQMC0');
</script>


<link rel="shortcut icon" type="image/x-icon" href="/til/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" /><script src="https://hypothes.is/embed.js" async></script>

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/til/">TIL</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/til/about/">Yoann Couble</a><a class="page-link" href="/til/search/">Search</a><a class="page-link" href="/til/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Explaining spaCy Models with Shap</h1><p class="page-description">Using explainability to understand a NLP algorithm trained with spaCy</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-02-24T00:00:00-06:00" itemprop="datePublished">
        Feb 24, 2022
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      7 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/til/categories/#en">en</a>
        &nbsp;
      
        <a class="category-tags-link" href="/til/categories/#python">python</a>
        &nbsp;
      
        <a class="category-tags-link" href="/til/categories/#nlp">nlp</a>
        &nbsp;
      
        <a class="category-tags-link" href="/til/categories/#spaCy">spaCy</a>
        &nbsp;
      
        <a class="category-tags-link" href="/til/categories/#ml">ml</a>
        &nbsp;
      
        <a class="category-tags-link" href="/til/categories/#mlops">mlops</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/ycouble/til/tree/master/_notebooks/2022-02-24-Explaining_spacy_models_with_shap.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/til/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/ycouble/til/master?filepath=_notebooks%2F2022-02-24-Explaining_spacy_models_with_shap.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/til/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/ycouble/til/blob/master/_notebooks/2022-02-24-Explaining_spacy_models_with_shap.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/til/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-02-24-Explaining_spacy_models_with_shap.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Explainabili...what-?">Explainabili...what ?<a class="anchor-link" href="#Explainabili...what-?"> </a></h2><p>One of the most important downsides to Deep Learning is the apparent loss of interpretability that it introduces.
As a matter of facts, Deep Learning models are no longer deterministic nor simple enough to be interpreted through their internal states. 
Models now have trillions of parameters with little to no sense at all.</p>
<p>On of the recent trends to tackle this issue is to use explainability techniques, such as LIME and SHAP which can both be applied to any type of ML model.
Both offer a large variety of tools to help understand the behavior of a model globally (what is learnt by the model) or locally (why the model took such or such decision for a given data).
One popular and helpful way to explain a prediction from a model is to highlight which features contributed the most to the prediction. LIME and SHAP offer different ways to do so:</p>
<ul>
<li><a href="https://github.com/marcotcr/lime">LIME</a> is an explainability technique that uses what we call a local surrogate model - i.e. a second model that locally approximates the model to explain - and infers explainability metrics from the second model. 
LIME is quite popular and very often used, even though the surrogate nature of the technique is often criticized (there is no guarantee that the surrogate model approximates well the actual model).</li>
<li><a href="https://shap.readthedocs.io/en/latest/">SHAP</a> is based on Shapeley Values, a game-theoretic concept that tests all possible combinations of features in order to determine the marginal contributions of each one.
SHAP has more theoretical guarantees and good properties, hence it is often considered more reliable than LIME, even though it is more time consuming.</li>
</ul>
<p>More details can be found on both techniques in <a href="https://medium.com/@kalia_65609/interpreting-an-nlp-model-with-lime-and-shap-834ccfa124e4">this post</a> or in <a href="https://christophm.github.io/interpretable-ml-book/">this excellent book from Christopher Molnar</a>.</p>
<p>In this article we'll show how to use SHAP on textual inputs, and more specifically for the popular open source NLP library <a href="https://spacy.io/">spaCy</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Explaining-Natural-Language-Processing-models">Explaining Natural Language Processing models<a class="anchor-link" href="#Explaining-Natural-Language-Processing-models"> </a></h2><p>The discipline of processing texts is called Natural Language Processing.
It contains (but is not limited to) using machine learning models to analyse texts.</p>
<p>Unlike tabular data, where each feature can be represented by a number and can therefore be given directly to a model, text data need to be pre-processed before being provided to a ML model.
This pre-processing text, called tokenization, basically consists of converting each word<sup class="footnote-ref" id="fnref-1"><a href="#fn-1">1</a></sup> (token) into a number.
For that the tokenizer requires a dictionary which will list all word to number matches.
This dictionary is called a vocabulary.</p>
<p>For example if my whole corpus is the following list of documents: <code>["I am Yoann Couble", "I work for PALO-IT", "I work with NLP for companies"]</code>, the vocabulary would be :</p>
<div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;I&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="nt">&quot;am&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="nt">&quot;Yoann&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="nt">&quot;Couble&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="nt">&quot;work&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="nt">&quot;for&quot;</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
    <span class="nt">&quot;PALO-IT&quot;</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span>
    <span class="nt">&quot;with&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
    <span class="nt">&quot;NLP&quot;</span><span class="p">:</span> <span class="mi">9</span><span class="p">,</span>
    <span class="nt">&quot;companies&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
<p>Therefore, tokenizing the last sentence would result in this vector : <code>[1, 5, 8, 9, 6, 10]</code>.</p>
<p>The tokenizer is important to use explainability techniques, since these techniques often resort to twisting the inputs to observe the effects on the outputs.
On text, varying the token integer representation would have no meaning at all.
So instead, the explainer removes tokens from the input and observes the impact on the output of the model.
With SHAP, the <a href="https://shap.readthedocs.io/en/latest/generated/shap.explainers.Permutation.html">permutation explainer</a> does exactly that and in a way that enables to approximate shapeley values of each feature.</p>
<p>As an example, permutation would mean passing "I ... with NLP for ...", "... work with ... for companies", etc. to the model and see by how much the result of the model changes.</p>
<div class="footnotes">
<hr />
<ol><li id="fn-1"><p>Some tokenization algorithms are however even more fine grained than that and split words into sub-words, phonemes, syllables to allow for more robust or versatile language representation.<a href="#fnref-1" class="footnote">&#8617;</a></p></li>
</ol>
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Explaining-the-results-of-a-spaCy-text-classification-model">Explaining the results of a spaCy text classification model<a class="anchor-link" href="#Explaining-the-results-of-a-spaCy-text-classification-model"> </a></h2><p>Now let's see in practice how to use SHAP to get some insights on a model trained with spaCy (see <a href="https://ycouble.github.io/til/en/python/nlp/spacy/ml/mlops/2022/02/01/Adding-a-text-classifier-to-a-spacy-pipeline.html">this article on how to do so</a>).
In this article we're going to use text classification as an example, and a custom pipeline specialized for this.
You can find the pipeline <a href="https://github.com/ycouble/til/blob/master/_notebooks/explain_nlp/model-best.zip">here</a> (you'll need to unzip it) and the dataset <a href="https://github.com/ycouble/til/blob/master/_notebooks/spacy_textcat/reddit_data.csv">there</a>.</p>
<h3 id="spaCy-wrappers-for-SHAP">spaCy wrappers for SHAP<a class="anchor-link" href="#spaCy-wrappers-for-SHAP"> </a></h3><p>SHAP supports text data, but has little to no support for spaCy models natively, so we'll need to create some wrappers and assemble precisely the different parts required by SHAP.</p>
<p>The permutation explainer, which is the one preferred by SHAP for text data, requires several parameters:</p>
<ul>
<li>a prediction function, which takes a list of texts and returns a list of results (in the case of a text classifier, this means the classes and their corresponding probability)</li>
<li>a tokenizer to build a Text masker for SHAP.</li>
</ul>
<p>These features are present in spaCy nlp pipelines but not as functions. They are embedded in the pipeline and produce results inside the document object.
Let's write some wrappers around the pipeline to conform to shap expectations.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">pip</span> install -qqq shap &quot;spacy&gt;3.2.0&quot; pandas
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Note: you may need to restart the kernel to use updated packages.
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="n">textcat_spacy</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;model-best&quot;</span><span class="p">)</span>
<span class="n">tokenizer_spacy</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">Tokenizer</span><span class="p">(</span><span class="n">textcat_spacy</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>

<span class="n">classes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">textcat_spacy</span><span class="o">.</span><span class="n">get_pipe</span><span class="p">(</span><span class="s2">&quot;textcat&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span>

<span class="c1"># Define a function to predict</span>
<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">texts</span><span class="p">):</span>
    <span class="c1"># convert texts to bare strings</span>
    <span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">]</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">textcat_spacy</span><span class="o">.</span><span class="n">pipe</span><span class="p">(</span><span class="n">texts</span><span class="p">):</span>
        <span class="c1"># results.append([{&#39;label&#39;: cat, &#39;score&#39;: doc.cats[cat]} for cat in doc.cats])</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">doc</span><span class="o">.</span><span class="n">cats</span><span class="p">[</span><span class="n">cat</span><span class="p">]</span> <span class="k">for</span> <span class="n">cat</span> <span class="ow">in</span> <span class="n">classes</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">results</span>


<span class="c1"># Create a function to create a transformers-like tokenizer to match shap&#39;s expectations</span>
<span class="k">def</span> <span class="nf">tok_wrapper</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_offsets_mapping</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">tokenizer_spacy</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">tok</span><span class="o">.</span><span class="n">norm</span> <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]}</span>
    <span class="k">if</span> <span class="n">return_offsets_mapping</span><span class="p">:</span>
        <span class="n">out</span><span class="p">[</span><span class="s2">&quot;offset_mapping&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[(</span><span class="n">tok</span><span class="o">.</span><span class="n">idx</span><span class="p">,</span> <span class="n">tok</span><span class="o">.</span><span class="n">idx</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">tok</span><span class="p">))</span> <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">out</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Define-the-SHAP-Explainer">Define the SHAP Explainer<a class="anchor-link" href="#Define-the-SHAP-Explainer"> </a></h3><p>Now can define the shap explainer.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">shap</span>

<span class="c1"># Create the Shap Explainer</span>
<span class="c1"># - predict is the &quot;model&quot; function, adapted to a transformers-like model</span>
<span class="c1"># - masker is the masker used by shap, which relies on a transformers-like tokenizer</span>
<span class="c1"># - algorithm is set to permutation, which is the one used for transformers models</span>
<span class="c1"># - output_names are the classes (although it is not propagated to the permutation explainer currently, which is why plots do not have the labels)</span>
<span class="c1"># - max_evals is set to a high number to reduce the probability of cases where the explainer fails because there are too many tokens</span>
<span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">Explainer</span><span class="p">(</span>
    <span class="n">predict</span><span class="p">,</span>
    <span class="n">masker</span><span class="o">=</span><span class="n">shap</span><span class="o">.</span><span class="n">maskers</span><span class="o">.</span><span class="n">Text</span><span class="p">(</span><span class="n">tok_wrapper</span><span class="p">),</span>
    <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;permutation&quot;</span><span class="p">,</span>
    <span class="n">output_names</span><span class="o">=</span><span class="n">classes</span><span class="p">,</span>
    <span class="n">max_evals</span><span class="o">=</span><span class="mi">1500</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-dataset">The dataset<a class="anchor-link" href="#The-dataset"> </a></h3><p>Let's import the dataset</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;reddit_data.csv&quot;</span><span class="p">)[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;Iâ€™m looking for datasets or api source that quantifies fan base, or preferably, bettorsâ€™ sentiment regarding a teamâ€™s performance or direction. Does anyone know of an API that tracks this? For now Iâ€™m looking specifically for NBA, but am also interested in MLB, NFL, and NCAA f-ball and b-ball.&#39;</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Explanations">Explanations<a class="anchor-link" href="#Explanations"> </a></h2><h3 id="Local-explanations">Local explanations<a class="anchor-link" href="#Local-explanations"> </a></h3><p>With the explainer and the data, we're now able to run the explanations from shap. Before plotting everything, let's get the shap values for one entry.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">shap_values</span> <span class="o">=</span> <span class="n">explainer</span><span class="p">(</span><span class="n">dataset</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span>
<span class="n">shap_values</span><span class="o">.</span><span class="n">output_names</span> <span class="o">=</span> <span class="n">classes</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The shap values contain 3 attributes:</p>
<ul>
<li>the <code>values</code> themselves (one value per class per word)</li>
<li>the <code>base_value</code> (which can be seen as a prior: what we would get for a empty string)</li>
<li>the <code>data</code>: the words as they are tokenized</li>
</ul>
<p>From this structure we can generate plots to help visualize the explanation.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fig_html</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">shap_values</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/til/images/copied_from_nb/explain_nlp/shap_1.png" alt="Shap Text Plot for label LanguageTechology" /><br />
<img src="/til/images/copied_from_nb/explain_nlp/shap_2.png" alt="Shap Text Plot for label datasets" /><br />
<img src="/til/images/copied_from_nb/explain_nlp/shap_3.png" alt="Shap Text Plot for label dataengineering" /></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see the shap text plots provide us two elements than can be interactively changed to display the shap values for each output label:</p>
<ul>
<li>The first plot is a force plot, where the contribution of each word in favor (red) or against (blue) the selected label is displayed in a cumulative manner.
The point where red and blue meet is the value predicted by the model for this label.</li>
<li>The second plot is the text with each word highlighted depending on its contribution to the final decision.</li>
</ul>
<p>That's it ! We've seen in this article how to adapt a spaCy pipeline to be able to use shap for our NLP explanations.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="ycouble/til"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/til/en/python/nlp/spacy/ml/mlops/2022/02/24/Explaining_spacy_models_with_shap.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/til/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/til/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/til/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Today I Learnt</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://gitlab.com/ycouble" target="_blank" title="ycouble"><svg class="svg-icon grey"><use xlink:href="/til/assets/minima-social-icons.svg#gitlab"></use></svg></a></li><li><a rel="me" href="https://github.com/ycouble" target="_blank" title="ycouble"><svg class="svg-icon grey"><use xlink:href="/til/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/yoann-couble" target="_blank" title="yoann-couble"><svg class="svg-icon grey"><use xlink:href="/til/assets/minima-social-icons.svg#linkedin"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
