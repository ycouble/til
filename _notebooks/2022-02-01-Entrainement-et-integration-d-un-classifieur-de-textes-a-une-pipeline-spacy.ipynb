{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entraînement et intégration d'un classifieur de textes à une pipeline SpaCy\n",
    "> \"Customiser une pipelines SpaCy avec vos propres composants de NLP\"\n",
    "\n",
    "- toc: false\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [fr, python, nlp, spacy, mlops]\n",
    "- hide: false\n",
    "- search_exclude: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Spacy est une bibliothèque de Traitement Automatique du Langage Naturel (TANL ou NLP en anglais) et un framework pour industrialiser des applications de NLP et de Machine Learning.\n",
    "\n",
    "Les ressources pour entraîner un modèle de machine learning sur du texte (classification, extraction d'entités, etc.) avec SpaCy ne manquent pas, mais très peu vont jusqu'au bout du chemin: l'intégration avec d'autres composants standards et pré-entraînés de SpaCy (ou d'autres fournisseurs), tels qu'un DependencyParser, tagger POS qui ne nécessitent pas de spécialisation ou de fine tuning.\n",
    "\n",
    "C'est pour combler ce manque que [ce notebook](https://github.com/ycouble/til/blob/master/_notebooks/2022-02-01-Entrainement-et-intégration-d-un-classifieur-de-textes-a-une-pipeline-spacy.ipynb) existe : **vous guider dans le process de configuration d'un composant de classification de textes, de l'entraînement depuis un script python et jusqu'à son intégration dans une pipeline SpaCy pré-entraînée afin qu'elle soit réutilisable depuis le reste de vos applications.**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requis\n",
    "Tout d'abord, installons les packages nécessaires pour ce tutoriel: spacy pour le NLP, pandas pour inspecter nos données, et sklearn qui va faciliter la séparation du jeu de données en splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"spacy>3.0.0\" pandas sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous devons également télécharger un modèle pré-entraîné de SpaCy : https://spacy.io/models/en#en_core_web_md. La ligne de commande suivante doit être exécutée depuis le même environnement que votre kernel de notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les données & la tâche de classification\n",
    "\n",
    "Nous travaillerons sur un dataset qui a été extrait à travers l'API de reddit. Le dataset a déjà été préparé et nettoyé pour qu'il puisse être facilement importé et converti en documents SpaCy.\n",
    "Vous pourrez le trouver [ici](spacy_textcat/reddit_data.csv).\n",
    "\n",
    "Le dataset est composé du corps de texte d'une selection de posts provenant de quelques subreddits liés à la data science.\n",
    "L'objectif de notre tâche de machine learning sera de deviner à partir du corps de texte de quel subreddit le post provient.\n",
    "Même si l'intérêt en soi est assez limité, c'est un bon point de départ pour démarrer et il présente l'avantage d'être déjà annoté.\n",
    "\n",
    "Jettons un oeil à ce qu'il y a dans le dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I’m looking for datasets or api source that quantifies fan base, or preferably, bettors’ sentiment regarding a team’s performance or direction. Does anyone know of an API that tracks this? For now I’m looking specifically for NBA, but am also interested in MLB, NFL, and NCAA f-ball and b-ball.</td>\n",
       "      <td>API</td>\n",
       "      <td>datasets</td>\n",
       "      <td>s0vufk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm making an ESG stock analysis program in Java, and so far the only free ESG API I've come across is ESGEnterprise, but I'm having trouble retrieving the data. Has anyone had any success/have any recs for other ESG APIs out there.</td>\n",
       "      <td>API</td>\n",
       "      <td>datasets</td>\n",
       "      <td>ruvj9n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey everyone! I’m one of the creators of Sieve _URL_ and I’m excited to be sharing it!\\nSieve _URL_ **is an API that helps you turn petabyte-scale video data into a high-quality dataset, automatically.**\\nIt helps store, process, and semantically search your video data. Just think _NUMBER_ cameras recording footage at _NUMBER_ FPS, _NUMBER_/_NUMBER_. That would be _NUMBER_ million frames generated in a single day. The videos might be searchable by timestamp, but finding moments of interest is like searching for a needle in a haystack. Sieve tags useful attributes like people, motion, lighting, etc on every frame!\\nWe built this visual demo (link here _URL_ a little while back which we’d love to get feedback on. It’s \\~_NUMBER_ hours of security footage that our API processed in &lt;_NUMBER_ mins and has simple querying and export functionality enabled. We see applications in better understanding what data you have, figuring out which data to send to labeling, sampling datasets for training, and building multiple test sets for models by scenario.\\nTo try it on your videos: _URL_ _URL_\\nVisual dashboard walkthrough: Click on our site link!</td>\n",
       "      <td>API</td>\n",
       "      <td>datasets</td>\n",
       "      <td>rup1uj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>I'm currently in the process of learning NLP. I am using catalyst on c#. \\nI was able to run the sample programs and it was able to determine if the word is an noun, adjective, etc. But I can't find any sample for what I need.\\nHere is a summary of what I would like to achieve. \\nI would like to extract certain information on a sentence. Lets say i have the following texts:\\n\"Sally ate an orange this morning. \"\\nOr \\n\"Sally is hiding behind the cabinet and she is eating an orange. \" \\nHow do i use the nlp to extract what sally ate?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LanguageTechnology</td>\n",
       "      <td>saas64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>I’ve been trying to do some basic keyword extraction and finding it harder than expected.\\nKeyBERT seems good but it requires a powerful GPU to be usably fast. That’s possible with AWS, but there’s a bit more set up.\\nI just tried PyTextRank, and I was surprised at the quality of the output - I wouldn’t say it was perfect either. Maybe I should set a threshold, like choose the top _NUMBER_ ranked keywords? It’s fine if we exclude potential good keywords just to have a smaller list of good ones.\\nHere’s a good article about _NUMBER_ different methods, which is helpful -\\n_URL_\\nIn theory, Spacy and BERT seem like the best options but they’re both a little complex. \\nI think KW extraction really only needs a few layers or as Spacy would call them pipelines.\\n_NUMBER_. accurate tokenization of words and punctuation symbols\\n_NUMBER_. accurate recognition of multi-word expressions  - think of it as “chunking”\\n_NUMBER_. Strong assessment of keyword “candidacy” for each MWE \\nOf course, a good algorithm can often skip steps. Like BERT is so smart it doesn’t need anything but the input text.\\nDoes anyone know of a simplest way to run a fast, effective keyword extraction?\\nI’m talking _NUMBER_ keywords in one second on a fast CPU.\\nThanks very much</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LanguageTechnology</td>\n",
       "      <td>s7qml8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>This _URL_ position is currently open and I wanted to share with you!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LanguageTechnology</td>\n",
       "      <td>s30ccv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>721 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           I’m looking for datasets or api source that quantifies fan base, or preferably, bettors’ sentiment regarding a team’s performance or direction. Does anyone know of an API that tracks this? For now I’m looking specifically for NBA, but am also interested in MLB, NFL, and NCAA f-ball and b-ball.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         I'm making an ESG stock analysis program in Java, and so far the only free ESG API I've come across is ESGEnterprise, but I'm having trouble retrieving the data. Has anyone had any success/have any recs for other ESG APIs out there.   \n",
       "2                                                                                                                 Hey everyone! I’m one of the creators of Sieve _URL_ and I’m excited to be sharing it!\\nSieve _URL_ **is an API that helps you turn petabyte-scale video data into a high-quality dataset, automatically.**\\nIt helps store, process, and semantically search your video data. Just think _NUMBER_ cameras recording footage at _NUMBER_ FPS, _NUMBER_/_NUMBER_. That would be _NUMBER_ million frames generated in a single day. The videos might be searchable by timestamp, but finding moments of interest is like searching for a needle in a haystack. Sieve tags useful attributes like people, motion, lighting, etc on every frame!\\nWe built this visual demo (link here _URL_ a little while back which we’d love to get feedback on. It’s \\~_NUMBER_ hours of security footage that our API processed in <_NUMBER_ mins and has simple querying and export functionality enabled. We see applications in better understanding what data you have, figuring out which data to send to labeling, sampling datasets for training, and building multiple test sets for models by scenario.\\nTo try it on your videos: _URL_ _URL_\\nVisual dashboard walkthrough: Click on our site link!   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ...   \n",
       "718                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      I'm currently in the process of learning NLP. I am using catalyst on c#. \\nI was able to run the sample programs and it was able to determine if the word is an noun, adjective, etc. But I can't find any sample for what I need.\\nHere is a summary of what I would like to achieve. \\nI would like to extract certain information on a sentence. Lets say i have the following texts:\\n\"Sally ate an orange this morning. \"\\nOr \\n\"Sally is hiding behind the cabinet and she is eating an orange. \" \\nHow do i use the nlp to extract what sally ate?   \n",
       "719  I’ve been trying to do some basic keyword extraction and finding it harder than expected.\\nKeyBERT seems good but it requires a powerful GPU to be usably fast. That’s possible with AWS, but there’s a bit more set up.\\nI just tried PyTextRank, and I was surprised at the quality of the output - I wouldn’t say it was perfect either. Maybe I should set a threshold, like choose the top _NUMBER_ ranked keywords? It’s fine if we exclude potential good keywords just to have a smaller list of good ones.\\nHere’s a good article about _NUMBER_ different methods, which is helpful -\\n_URL_\\nIn theory, Spacy and BERT seem like the best options but they’re both a little complex. \\nI think KW extraction really only needs a few layers or as Spacy would call them pipelines.\\n_NUMBER_. accurate tokenization of words and punctuation symbols\\n_NUMBER_. accurate recognition of multi-word expressions  - think of it as “chunking”\\n_NUMBER_. Strong assessment of keyword “candidacy” for each MWE \\nOf course, a good algorithm can often skip steps. Like BERT is so smart it doesn’t need anything but the input text.\\nDoes anyone know of a simplest way to run a fast, effective keyword extraction?\\nI’m talking _NUMBER_ keywords in one second on a fast CPU.\\nThanks very much   \n",
       "720                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          This _URL_ position is currently open and I wanted to share with you!   \n",
       "\n",
       "     tag           subreddit      id  \n",
       "0    API            datasets  s0vufk  \n",
       "1    API            datasets  ruvj9n  \n",
       "2    API            datasets  rup1uj  \n",
       "..   ...                 ...     ...  \n",
       "718  NaN  LanguageTechnology  saas64  \n",
       "719  NaN  LanguageTechnology  s7qml8  \n",
       "720  NaN  LanguageTechnology  s30ccv  \n",
       "\n",
       "[721 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_colwidth = None\n",
    "pd.options.display.max_rows = 6\n",
    "data = pd.read_csv(\"spacy_textcat/reddit_data.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets', 'dataengineering', 'LanguageTechnology']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's also store the subreddits which are going to be our labels for the text classification component\n",
    "cats = data.subreddit.unique().tolist()\n",
    "cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dataset est composé d'un peu plus de 700 posts et de leurs subreddits associé.\n",
    "Créons maintenant les datasets d'entraînement et de validation en y incluant les annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du dataset d'entraînement\n",
    "Tout d'abord écrivons une fonction permettant de transformer le dataset dans le format binaire pour spacy, que l'on va stocker temporairement en local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set, List, Tuple\n",
    "from spacy.tokens import DocBin\n",
    "import spacy\n",
    "\n",
    "# Load spaCy pretrained model that we downloaded before\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Create a function to create a spacy dataset\n",
    "def make_docs(data: List[Tuple[str, str]], target_file: str, cats: Set[str]):\n",
    "    docs = DocBin()\n",
    "    # Use nlp.pipe to efficiently process a large number of text inputs, \n",
    "    # the as_tuple arguments enables giving a list of tuples as input and \n",
    "    # reuse it in the loop, here for the labels\n",
    "    for doc, label in nlp.pipe(data, as_tuples=True):\n",
    "        # Encode the labels (assign 1 the subreddit)\n",
    "        for cat in cats:\n",
    "            doc.cats[cat] = 1 if cat == label else 0\n",
    "        docs.add(doc)\n",
    "    docs.to_disk(target_file)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Séparons jeux d'entraînement et de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.tokens._serialize.DocBin at 0x12b189100>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(data[\"text\"].values, data[\"subreddit\"].values, test_size=0.3)\n",
    "\n",
    "make_docs(list(zip(X_train, y_train)), \"train.spacy\", cats=cats)\n",
    "make_docs(list(zip(X_valid, y_valid)), \"valid.spacy\", cats=cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation et configuration du composant de classification de textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le workflow recommandé avec SpaCy utilise des fichiers de configuration. Ils permettent de configurer chaque composant de la pipeline, de choisir quels composant entrâiner etc.\n",
    "\n",
    "Nous utiliserons [ce fichier de configuration](spacy_textcat/config.cfg), qui utilises le classifieur de textes par défaut de SpaCy.\n",
    "La configuration peut être re-générée en suivant ce guide : https://spacy.io/usage/training#quickstart et nous l'avons customisé pour qu'il utilise ce model proposé par SpaCy également : https://spacy.io/api/architectures#TextCatBOW.\n",
    "\n",
    "Il y a deux parties qu'il est important de noter dans ce fichier :\n",
    "\n",
    "1. La définition de la pipeline (sous le header `nlp`): La pipeline ne contient que le composant textcat (de classification) puisque c'est celui pour lequel nous avons des données annotées et le seul que nous allons entraîner aujourd'hui. Un autre détail qui a son importance est le tokenizer qui, comme on peut le voir est spécifié à cet endroit, et est ici laissé à la valeur par défaut proposée par SpaCy. C'est le seul pré-requis pour notre composant `textcat`.\n",
    "\n",
    "```json\n",
    "[nlp]\n",
    "lang = \"en\"\n",
    "pipeline = [\"textcat\"]\n",
    "batch_size = 1000\n",
    "disabled = []\n",
    "before_creation = null\n",
    "after_creation = null\n",
    "after_pipeline_creation = null\n",
    "tokenizer = {\"@tokenizers\":\"spacy.Tokenizer.v1\"}\n",
    "```\n",
    "\n",
    "1. La spécification du modèle : Notons le paramètre  `exclusive_classes` qui a été mis à `true` puisque nos posts ne viennent que d'un seul subreddit. Notons aussi qu'il a fallu rajouter le prefix `components.textcat` dans les headers à la configuration donnée dans la documentation.\n",
    "\n",
    "```json\n",
    "[components.textcat]\n",
    "factory = \"textcat\"\n",
    "scorer = {\"@scorers\":\"spacy.textcat_scorer.v1\"}\n",
    "threshold = 0.5\n",
    "\n",
    "[components.textcat.model]\n",
    "@architectures = \"spacy.TextCatBOW.v2\"\n",
    "exclusive_classes = true\n",
    "ngram_size = 1\n",
    "no_output_layer = false\n",
    "nO = null\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Plus de détails sur les pipelines SpaCy</summary>\n",
    "The spacy pipeline is a modular and highly configurable workflow for processing texts. As shown below, there is a mandatory first step of tokenizing the text, then there are a succession of pipeline components (or pipes) that are executed in order, but do not necessarily rely on each other. It will be in the code of a component that the dependency would be set, e.g. by accessing previously set attributes in the doc element.\n",
    "![](spacy_textcat/spacy_pipeline.png)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the text classification component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike what can be found in most of the tutorials online or in the SpaCy docs where training is started from CLI, we're going to train the component from a config file and from a python script.\n",
    "This has the advantage of letting us start the training programatically, e.g. from a data pipeline (using airflow, dagster or such).\n",
    "\n",
    "However, we'll use the spacy pre-built training function from `spacy.cli.train` in order to benefit from all the checks and logging that are set up in the CLI. (Another approach would be to reuse parts of the code, calling directly the `spacy.training` module instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: output/spacy_textcat\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['textcat']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TEXTCAT  CATS_SCORE  SCORE \n",
      "---  ------  ------------  ----------  ------\n",
      "  0       0          0.67        3.31    0.03\n",
      "  0     200         97.52       46.14    0.46\n",
      "  0     400         59.67       61.38    0.61\n",
      "  1     600         19.23       73.74    0.74\n",
      "  1     800         11.17       75.77    0.76\n",
      "  2    1000          2.12       74.20    0.74\n",
      "  3    1200          1.19       75.33    0.75\n",
      "  4    1400          0.71       76.68    0.77\n",
      "  4    1600          0.35       75.91    0.76\n",
      "  6    1800          0.28       77.79    0.78\n",
      "  7    2000          0.21       77.91    0.78\n",
      "  9    2200          0.18       77.49    0.77\n",
      " 11    2400          0.06       79.36    0.79\n",
      " 13    2600          0.05       77.81    0.78\n",
      " 16    2800          0.03       77.81    0.78\n",
      " 19    3000          0.03       77.98    0.78\n",
      " 21    3200          0.03       77.05    0.77\n",
      " 24    3400          0.06       77.95    0.78\n",
      " 27    3600          0.02       76.41    0.76\n",
      " 30    3800          0.03       75.48    0.75\n",
      " 32    4000          0.02       77.60    0.78\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "output/spacy_textcat/model-last\n"
     ]
    }
   ],
   "source": [
    "from spacy.cli.train import train as spacy_train\n",
    "\n",
    "config_path = \"spacy_textcat/config.cfg\"\n",
    "output_model_path = \"output/spacy_textcat\"\n",
    "spacy_train(\n",
    "    config_path,\n",
    "    output_path=output_model_path,\n",
    "    overrides={\n",
    "        \"paths.train\": \"train.spacy\",\n",
    "        \"paths.dev\": \"valid.spacy\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a trained classification model !\n",
    "Spacy stores the model in folders, and usually saves both the best model and the last state of the model at the end of the training, in case we'd want to continue training from this step.\n",
    "In the `meta.json` file in the model folder you can find the internal scores that were computed, and we can see that we have ~80% Macro F score and a nice .93 AUC.\n",
    "\n",
    "We can import the newly trained pipeline from spacy to predict like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'datasets': 0.8466417193412781,\n",
       " 'dataengineering': 0.07126601785421371,\n",
       " 'LanguageTechnology': 0.08209223300218582}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "trained_nlp = spacy.load(\"output/spacy_textcat/model-best\")\n",
    "\n",
    "# Let's try it on an example text\n",
    "text = \"Hello\\n I'm looking for data about birds in New Zealand.\\nThe dataset would contain the birds species, colors, estimated population etc.\"\n",
    "# Perform the trained pipeline on this text\n",
    "doc = trained_nlp(text)\n",
    "# We can display the predicted categories\n",
    "doc.cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model predicts the subreddit `datasets` with 84% confidence !\n",
    "\n",
    "However, the rest of the trained pipeline is empty, NER, dependencies etc. have not been computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities ()\n",
      "sentences error: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.\n"
     ]
    }
   ],
   "source": [
    "print(\"entities\", doc.ents)\n",
    "try:\n",
    "    print(\"sentences\", list(doc.sents))\n",
    "except ValueError as e:\n",
    "    print(\"sentences\", \"error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are however, available with high quality in the pre-trained pipeline that we used earlier. But not the classification, obviously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities (New Zealand,)\n",
      "sentences [Hello\n",
      " I'm looking for data about birds in New Zealand., \n",
      ", The dataset would contain the birds species, colors, estimated population etc.]\n",
      "classification {}\n"
     ]
    }
   ],
   "source": [
    "doc_from_pretrained = nlp(text)\n",
    "print(\"entities\", doc_from_pretrained.ents)\n",
    "print(\"sentences\", list(doc_from_pretrained.sents))\n",
    "print(\"classification\", doc_from_pretrained.cats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question is, then, **how do we combine both pipelines** without having to implement a lot of glue code for nothing ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate the new component to an existing pipeline\n",
    "There are actually several ways to do this:\n",
    "1. Creating a pipe and loading the model from files, but this would require a different training process than what we did (on a model level and not pipeline level)\n",
    "```python\n",
    "pipe = nlp.add_pipe(\"textcat\")\n",
    "pipe.from_disk(\"path/to/model/files\") # Note, requires a different folder structure that what we've generated\n",
    "```\n",
    "2. Loading the pipeline, saving the model to disk/bytes and loading it back again from disk/bytes in a new pipe in the pretrained pipeline\n",
    "```python\n",
    "trained_nlp.get_pipe(\"textcat\").to_disk(\"tmp\")\n",
    "nlp.add_pipe(\"textcat\").from_disk(\"tmp\")\n",
    "# OR\n",
    "nlp.add_pipe(\"textcat\").from_bytes(\n",
    "    trained_nlp.get_pipe(\"textcat\").to_bytes()\n",
    ")\n",
    "```\n",
    "3. Creating the pipe with a source pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities (New Zealand,)\n",
      "sentences [Hello\n",
      " I'm looking for data about birds in New Zealand., \n",
      ", The dataset would contain the birds species, colors, estimated population etc.]\n",
      "classification {'datasets': 0.8466417193412781, 'dataengineering': 0.07126601785421371, 'LanguageTechnology': 0.08209223300218582}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yco/.pyenv/versions/myreddit/lib/python3.8/site-packages/spacy/language.py:707: UserWarning: [W113] Sourced component 'textcat' may not work as expected: source vectors are not identical to current pipeline vectors.\n",
      "  warnings.warn(Warnings.W113.format(name=source_name))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlp_merged = spacy.load(\"en_core_web_md\")\n",
    "nlp_merged.add_pipe(\"textcat\", source=trained_nlp)\n",
    "doc_from_merged = nlp_merged(text)\n",
    "\n",
    "print(\"entities\", doc_from_merged.ents)\n",
    "print(\"sentences\", list(doc_from_merged.sents))\n",
    "print(\"classification\", doc_from_merged.cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Removing the warning</summary>\n",
    "To remove the vector mis-alignment, you'd have to train the pipeline by passing the pre-trained tok2vec component from the pre-trained model. In our case it's not a problem since the vectors are probably very similar, but nonetheless, I'll try to update the post when I can with the solution to this warning.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point we can store and reuse the pipeline at will !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We've seen in this tutorial how to train and integrate a text classification component to a pre-trained pipeline, in a fully programmatic way.\n",
    "\n",
    "Hope you liked the post and feel free to contact me if you want more details in the comments !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myreddit",
   "language": "python",
   "name": "myreddit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
