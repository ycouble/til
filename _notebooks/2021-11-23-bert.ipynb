{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"La Famille BERT et les Transformers\"\n",
    "> \"Qu'est-ce que BERT ? Quel lien avec les transformers ? Pourquoi les utilise-t-on ? Comment ?\"\n",
    "\n",
    "- toc: false\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [fr, ml, nlp]\n",
    "- hide: false\n",
    "- search_exclude: false\n",
    "- image: images/bert_cover.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Une petite r√©volution\n",
    "Ces derni√®res ann√©es, la recherche en Intelligence Artificielle a √©t√© compl√®tement transform√©e par une avanc√©e technologique: les transformers.\n",
    "Avant de comprendre comment et pourquoi cette transformation s'est op√©r√©e, prenons un instant pour examiner comment la recherche en IA √©value une avanc√©e technique.\n",
    "\n",
    "La recherche en IA se concentre depuis plusieurs ann√©es sur les t√¢ches dites difficiles de la compr√©hension du monde: les syst√®mes de recommandation de contenu, l'extraction d'informations √† partir de textes (discipline du traitement automatique du langage ou Natural Language Processing, NLP en anglais) ou d'image (Vision par ordinateur ou Computer Vision, CV) et plus r√©cemment √©galement sur l'audio.\n",
    "Chacune de ces discipline a d√©termin√© un certain nombre de t√¢ches d'extraction d'information que l'on cherche √† faire effectuer √† un programme, comme par exemple :\n",
    "- **La d√©tection d'objets** (localiser une personne dans une image), **la segmentation d'image** (identifier la portion de l'image repr√©sentant une route), **la classification d'images** (dire si il s'agit d'une photo de lave-linge ou d'un frigo) en vision par ordinateur\n",
    "- **La classification de texte** (dire si un texte est positif ou n√©gatif), **l'extraction d'entit√©s nomm√©es** d'un texte (identifier les noms de personnes, d'organisation, de pays etc.), **le r√©sum√© de texte** ou **la r√©ponse aux questions** sur un texte dans le domaine du traitement du langage naturel.\n",
    "Pour chacune de ces t√¢ches ont √©t√© d√©termin√©s des jeux de donn√©es de r√©f√©rence et des m√©thodologies d'√©valuation de l'efficacit√© d'un algorithme √† r√©aliser la t√¢che en question.\n",
    "\n",
    "Jusque l√†, les meilleures performances √©taient atteintes par des mod√®les sp√©cialis√©s pour chaque t√¢che pr√©cise, et l'entra√Ænement de chaque mod√®le √©tait un d√©fi en soi.\n",
    "C'est dans ce contexte que les Transformers ont apport√© une petite r√©volution: en 2018 dans l'article pr√©sentant BERT (*BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*), Google pr√©sente une m√©thodologie de pr√©-entra√Ænement et de sp√©cialisation qui, appliqu√©e √† une architecture √† base de Transformers, surpasse l'√©tat de l'art sur chacune des t√¢ches classiques de traitement du langage naturel, et ce d'une marge cons√©quente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pourquoi les Transformers et BERT fonctionnent-ils si bien ?\n",
    "La contribution du papier de BERT a √©t√© dans un premier temps de combiner avec succ√®s plusieurs techniques d√©j√† connues:\n",
    "1. Les Transformers, une architecture de Deep Learning qui avait d√©j√† fait ses preuves. Cette architecture est bas√©e sur le m√©canisme d'attention, qui permet de prendre en compte tout le contexte d'un mot en accordant √† priori autant d'importance √† des mots √©loign√©s qu'√† des mots proches, contrairement aux mod√®les r√©currents qui faisaient l'√©tat de l'art avant l'arriv√©e des Transformers, en 2016 (cf. *Attention is All You Need*, Google 2016 dans les r√©f√©rences.)\n",
    "\n",
    "![attention](bert/attention.png \"M√©canisme d'attention\")\n",
    "\n",
    "2. Le Transfer Learning, qui consiste √† entra√Æner un mod√®le sur une t√¢che tr√®s g√©n√©rique (souvent non supervis√©e) sur un dataset gigantesque (par exemple le corpus de Wikip√©dia), puis √† sp√©cialiser le mod√®le en le modifiant sur une seconde t√¢che diff√©rente de mani√®re supervis√©e et sur un dataset plus r√©duit.\n",
    "\n",
    "De plus, leur contribution est aussi et surtout dans la m√©thodologie de pr√©-entrainement du mod√®le de langage (le premier mod√®le), qui se fait en combinant deux t√¢ches g√©n√©riques non supervis√©es:\n",
    "1. La compl√©tion de phrases √† trou, c'est √† dire trouver le meilleur mot pour compl√©ter une phrase du type: \"je suis all√© au ____ ce matin pour acheter mes l√©gumes\".\n",
    "2. La pr√©diction de la prochaine phrase dans un texte.\n",
    "\n",
    "Ces deux t√¢ches peuvent s'effectuer gr√¢ce √† un entra√Ænement auto-supervis√© sur des datasets gigantesques comme l'int√©gralit√© de Wikipedia, des corpus de sites crawl√© ou √† partir des r√©seaux sociaux publics (Twitter, Reddit ...). En passant en revue tous ces contenus, l'algorithme devient de plus en plus performant √† d√©terminer les mots qui vont g√©n√©ralement ensemble en fonction d'un contexte assez large (de l'ordre de la phrase ou du paragraphe).\n",
    "\n",
    "### Qu'est-ce que √ßa change ?\n",
    "Le gros changement apport√© par BERT est cette id√©e de mod√®le de langage: la plupart des t√¢ches de NLP ont en commun d'essayer de comprendre le contexte d'un mot et de trouver les relations entre les mots d'une phrase, et c'est ce que tente d'apporter le mod√®le de langage. Concr√®tement, cela signifie que pour n'importe quelle t√¢che de NLP, il suffit maintenant de r√©utiliser le mod√®le de langage pour avoir cette compr√©hension g√©n√©rale, puis de sp√©cialiser la \"t√™te\" de l'architecture pour une t√¢che donn√©e.\n",
    "\n",
    "La seconde cons√©quence est d'ordre pratique: avec un mod√®le comme BERT, l'entra√Ænement d'un mod√®le sur une t√¢che sp√©cifique se r√©sume g√©n√©ralement √† n'entra√Æner que la t√™te de l'architecture, donc √† un co√ªt d√©risoire par rapport √† un mod√®le complet comme pr√©c√©demment.\n",
    "\n",
    "Maintenant, il est devenu extr√™mement facile d'utiliser et d'entra√Æner un mod√®le tr√®s performant pour toute une diversit√© de t√¢ches de NLP. Cela a √©t√© grandement facilit√© par la publication du code et des mod√®les pr√©-entra√Æn√©s, et par l'arriv√©e de Hugging Face, une entreprise de l'open source fond√©e par deux fran√ßais qui propose une interface standard et un hub de mod√®les pour les transformers.\n",
    "\n",
    "![bert](bert/bert_training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation des transformers\n",
    "BERT et les autres transformers sont con√ßus pour √™tre r√©utilisable en grande partie pour diff√©rentes t√¢ches, il ne reste donc qu'√† r√©adapter les entr√©es et sorties √† la t√¢che vis√©e.\n",
    "Nous allons regarder dans cet article le cas de la sp√©cialisation et de l'utilisation d'un mod√®le de transformer pour une t√¢che d'extraction d'entit√©s nomm√©es (NER) √† partir de diff√©rentes biblioth√®ques open-source de NLP.\n",
    "\n",
    "### La t√¢che d'extraction d'entit√©s nomm√©es: donn√©es et annotation\n",
    "Le but de l'extraction d'entit√©s nomm√©es est de reconna√Ætre dans un texte les mots ou ensembles de mots qui correspondent √† des villes, pays, dates, langages de programmation, personnes ou tout autre cat√©gorie pour lesquels on ne peut g√©n√©ralement pas lister l'ensemble des mots qui repr√©sentent ces entit√©s.\n",
    "\n",
    "Par exemple, dans la phrase \"My name is Wolfgang and I live in Berlin\", la t√¢che de NER devrait permettre d'extraction des mots ‚ÄúWolfgang‚Äù, class√©s en tant que personne et ‚ÄúBerlin‚Äù comme ville.\n",
    "\n",
    "Il s'agit en fin de compte d'une t√¢che de classification de token (un token est un mot ou une partie de mot), supervis√©e, et il y a plusieurs m√©thodes d'annotation des donn√©es. Nous allons pr√©senter uniquement la m√©thode IOB, acronyme pour Inside, Outside et Begin, o√π chaque mot re√ßoit une annotation O ou I/B-[type d'entit√©]. Pour notre exemple cela donnerait √ßa:\n",
    "\n",
    "```\n",
    "My name is Wolfgang and I live in Berlin\n",
    "O    O   O   B-PER   0  0   0   0 B-CITY\n",
    "```\n",
    "\n",
    "Ou sur un second exemple:\n",
    "```\n",
    "I'm Yoann Couble and I write about Natural Language Processing in Python\n",
    "O O B-PER I-PER   O  0   O     O   B-TECH   I-TECH    I-TECH   O  B-LANG\n",
    "```\n",
    "\n",
    "L'objectif de l'algorithme sera donc de d√©terminer pour chaque token l'annotation √† positionner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ü§ó Hugging Face Transformers\n",
    "Hugging Face a construit toute une API pour faciliter l'utilisation des transformers. La librairie propose des pipelines pr√©-d√©finies et une biblioth√®que de [mod√®les](https://huggingface.co/models) explorables par t√¢ches et langue et de [datasets](https://huggingface.co/datasets) pr√™tes √† l'emplois.\n",
    "L'avantage de Hugging Face est la grande diversit√© de mod√®les exp√©rimentaux ou √©prouv√©s qui sont disponibles sur le hub et la facilit√© d'utilisation et d'exp√©rimentation qu'il permet.\n",
    "\n",
    "La configuration d'un mod√®le se fait tr√®s facilement (sans code) √† l'aide d'un fichier de configuration: https://huggingface.co/dslim/bert-base-NER/blob/main/config.json\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"_num_labels\": 9,\n",
    "  \"architectures\": [\n",
    "    \"BertForTokenClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"layer_norm_eps\": 1e-12,\n",
    "  \"max_position_embeddings\": 512,\n",
    "  \"model_type\": \"bert\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 12,\n",
    "  \"output_past\": true,\n",
    "  \"pad_token_id\": 0,\n",
    "  \"type_vocab_size\": 2,\n",
    "  \"vocab_size\": 28996,\n",
    "  # Labels\n",
    "  \"id2label\": {\n",
    "    \"0\": \"O\",\n",
    "    \"1\": \"B-MISC\",\n",
    "    \"2\": \"I-MISC\",\n",
    "    \"3\": \"B-PER\",\n",
    "    \"4\": \"I-PER\",\n",
    "    \"5\": \"B-ORG\",\n",
    "    \"6\": \"I-ORG\",\n",
    "    \"7\": \"B-LOC\",\n",
    "    \"8\": \"I-LOC\"\n",
    "  },\n",
    "  \"label2id\": {\n",
    "    \"B-LOC\": 7,\n",
    "    \"B-MISC\": 1,\n",
    "    \"B-ORG\": 5,\n",
    "    \"B-PER\": 3,\n",
    "    \"I-LOC\": 8,\n",
    "    \"I-MISC\": 2,\n",
    "    \"I-ORG\": 6,\n",
    "    \"I-PER\": 4,\n",
    "    \"O\": 0\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Ici l'architecture choisie est `BertForTokenClassification` ce qui correspond √† notre t√¢che de NER. Et on retrouve aussi les diff√©rents hyper-param√®tres des transformers (nombre de t√™tes d'attention, param√®tres d'attention, ...). Pour pouvoir r√©utiliser les poids d'entra√Ænement d'un mod√®le pr√©-entra√Æn√©, il faut faire attention √† ne changer que ce qui ne casse pas la compatibilit√© avec le mod√®le pr√©-entra√Æn√©.\n",
    "\n",
    "Dans ce notebook nous n'allons pas entra√Æner de nouveau mod√®le, mais utiliser directement [le mod√®le qui a √©t√© entra√Æn√© avec la configuration pr√©sent√©e au-dessus](https://huggingface.co/dslim/bert-base-NER)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pr√©-requis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade -q torch transformers \"spacy>=3.2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "\n",
    "# R√©cup√©ration du mod√®le et d'un tokenizer adapt√© (peut prendre du temps car il faut t√©l√©charger le mod√®le qui est assez volumineux)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# D√©finition de la pipeline\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-PER',\n",
       "  'score': 0.99901396,\n",
       "  'index': 4,\n",
       "  'word': 'Wolfgang',\n",
       "  'start': 11,\n",
       "  'end': 19},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': 0.999645,\n",
       "  'index': 9,\n",
       "  'word': 'Berlin',\n",
       "  'start': 34,\n",
       "  'end': 40}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utilisation sur un exemple\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "ner_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notons que seuls les tokens classifi√©s avec une entit√© sont montr√©s, le reste a re√ßu une annotation \"O\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilisation des transformers avec SpaCy\n",
    "Spacy est une biblioth√®que open-source permettant d'industrialiser des applications de traitement du langage naturel.\n",
    "L'avantage de Spacy par rapport √† Hugging Face est qu'il va √™tre possible de mutualiser un m√™me mod√®le de langage √† base de transformers pour plusieurs mod√®les sp√©cialis√©s pour diff√©rentes t√¢ches sur un m√™me document.\n",
    "\n",
    "Spacy fournit √©galement des mod√®les pr√©-entra√Æn√©s et bien int√©gr√©s √† Spacy. Ce sont les mod√®les finissant en `_trf` comme [celui-ci bas√© sur camembert-base](https://spacy.io/models/fr#fr_dep_news_trf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir l'utiliser, il faut t√©l√©charger le mod√®le dans le m√™me environnement virtuel / kernel (dans mon cas, un environnement pyenv)\n",
    "\n",
    "```bash\n",
    "pyenv activate transformers_3.8.6\n",
    "python -m spacy download en_core_web_trf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">My name is \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Wolfgang\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " and I live in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Berlin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "doc = nlp(\"My name is Wolfgang and I live in Berlin\")\n",
    "\n",
    "displacy.render(doc, \"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Une adoption qui s'√©tend maintenant au del√† du langage\n",
    "Nous avons pu voir et comprendre l'apport de BERT et des transformers au paysage du NLP, et l'impact des transformers continue de faire son chemin, avec la vision qui a eu t√¥t fait de les adopter (mod√®ls VIT, VILT, CLIP etc.) ainsi que l'audio et la vid√©o.\n",
    "\n",
    "Maintenant, la recherche sur les transformers se plonge dans la convergence multi-modale (fusion Vid√©o-Audio-Texte), avec notamment le tr√®s r√©cent Data2Vec de Facebook AI Research cette ann√©e.\n",
    "Cela dit, le probl√®me difficile de la compr√©hension du monde est loin d'√™tre r√©solu, et la recherche continue d'√™tre tr√®s active sur le sujet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Ressources et r√©d√©rences sur le sujet\n",
    "### R√©f√©rences\n",
    "* Attention is all you need (2016, Transformer paper): https://arxiv.org/abs/1706.03762\n",
    "* BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018): https://arxiv.org/abs/1810.04805\n",
    "* Illustrated transformer: http://jalammar.github.io/illustrated-transformer/\n",
    "* BERT model doc on hugging face: https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "* https://spacy.io/usage/embeddings-transformers\n",
    "\n",
    "### Exemples\n",
    "* Exemple de configuration sur le hub de huggingface: https://huggingface.co/dslim/bert-base-NER/blob/main/config.json\n",
    "* Transformers pour le NER FR https://huggingface.co/models?language=fr&pipeline_tag=token-classification&sort=downloads&search=ner\n",
    "* Use huggingface transformers within spacy: https://reposhub.com/python/deep-learning/explosion-spacy-transformers.html\n",
    "\n",
    "### Cours et ressources formatrices\n",
    "* https://www.coursera.org/learn/attention-models-in-nlp/home/welcome\n",
    "* https://huggingface.co/course/chapter1\n",
    "* https://course.spacy.io/en/chapter4\n",
    "* https://www.youtube.com/playlist?list=PL75e0qA87dlG-za8eLI6t0_Pbxafk-cxb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "proto-38dev",
   "language": "python",
   "name": "proto-38dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
