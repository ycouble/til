{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"La Famille BERT et les Transformers\"\n",
    "> \"Qu'est-ce que BERT ? Quel lien avec les transformers ? Pourquoi les utilise-t-on ? Comment ?\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [fr, nlp]\n",
    "- hide: false\n",
    "- search_exclude: false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/yco/.pyenv/versions/proto-38dev/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade -q pandas torch transformers \"spacy>=3.2.0\" spacy-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Ressources\n",
    "### R√©f√©rences\n",
    "* Attention is all you need (2016, Transformer paper): https://arxiv.org/abs/1706.03762\n",
    "* BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018): https://arxiv.org/abs/1810.04805\n",
    "\n",
    "\n",
    "* Illustrated transformer: http://jalammar.github.io/illustrated-transformer/\n",
    "* BERT model doc on hugging face: https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "* https://spacy.io/usage/embeddings-transformers\n",
    "\n",
    "### Exemples\n",
    "* Exemple de configuration sur le hub de huggingface: https://huggingface.co/dslim/bert-base-NER/blob/main/config.json\n",
    "* Transformers pour le NER FR https://huggingface.co/models?language=fr&pipeline_tag=token-classification&sort=downloads&search=ner\n",
    "* Facebook Language AI Research NER: https://huggingface.co/flair/ner-french (Org, Per, Loc, Names)\n",
    "* https://towardsdatascience.com/easy-fine-tuning-of-transformers-for-named-entity-recognition-d72f2b5340e3\n",
    "* https://skimai.com/how-to-fine-tune-bert-for-named-entity-recognition-ner/\n",
    "* Use huggingface transformers within spacy: https://reposhub.com/python/deep-learning/explosion-spacy-transformers.html\n",
    "\n",
    "### Cours et ressources formatrices\n",
    "* https://www.coursera.org/learn/attention-models-in-nlp/home/welcome\n",
    "* https://huggingface.co/course/chapter1\n",
    "* https://course.spacy.io/en/chapter4\n",
    "* https://www.youtube.com/playlist?list=PL75e0qA87dlG-za8eLI6t0_Pbxafk-cxb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why Transformers / BERT ?\n",
    "* Les Transformers s'appuient sur le m√©canisme d'attention, qui a prouv√© son efficacit√©\n",
    "* Les Transformers sont une architecture de deep learning g√©n√©rique, applicable √† toutes les t√¢ches de NLP\n",
    "* Les t√¢ches de NLP (classification, NER, traduction, QA, ...) se ressemblent beaucoup, et une grande partie de l'effort est commun: comprendre la structure d'un langage, les relations entre les mots etc.\n",
    "    * Il y a un int√©r√™t important √† avoir un moyen de mutualiser cette approche pour transf√©rer la connaissance\n",
    "    * Le word / sentence embeddings sont un premier pas dans ce sens, mais pas sur les relations entre les mots ni sur le sens des phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# What are they ?\n",
    "* Un transformer est une architecture de deep learning qui utilise le m√©canisme d'attention: l'importance de chaque mot de la phrase par rapport √† chaque autre mot est prise en compte\n",
    "\n",
    "![attention](bert/attention.png \"M√©canisme d'attention\")\n",
    "![scaled dot prod](bert/scaleddotprod.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* L'architecture Transformer est compos√©e d'un encoder et d'un decoder, tous deux bas√©s sur le m√©canisme d'attention. Dans le d√©codeur l'attention est dite causale (ou masqu√©e), c'est √† dire qu'on ne regarde que les mots pass√©s.\n",
    "\n",
    "![transformer](bert/transformer_archi.png)\n",
    "![multi head attn](bert/multihead.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* BERT est initialement une approche de pre-training pour capturer les relations dans un langage, l'entra√Ænement se fait sur une t√¢che auto-supervis√©e (self-supervised) ce qui permet de l'entra√Æner sur de grande quantit√© de donn√©es. Le fine tuning se fait ensuite en utilisant le transfer learning\n",
    "\n",
    "![bert](bert/bert_training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use Transformers\n",
    "BERT et les autres transformers sont con√ßus pour √™tre r√©utilisable en grande partie pour diff√©rentes t√¢che, il ne reste donc qu'√† r√©adapter les entr√©es et sorties √† la t√¢che vis√©e\n",
    "\n",
    "## ü§ó Transformers\n",
    "Hugging Face a construit toute une API pour faciliter l'utilisation des transformers. La librairie propose des pipelines pr√©-d√©finies et une biblioth√®que de mod√®les et de datasets pr√™tes √† l'emplois.\n",
    "\n",
    "La configuration d'un mod√®le se fait tr√®s facilement (sans code) √† l'aide d'un fichier de configuration: https://huggingface.co/dslim/bert-base-NER/blob/main/config.json\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"_num_labels\": 9,\n",
    "  \"architectures\": [\n",
    "    \"BertForTokenClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"layer_norm_eps\": 1e-12,\n",
    "  \"max_position_embeddings\": 512,\n",
    "  \"model_type\": \"bert\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 12,\n",
    "  \"output_past\": true,\n",
    "  \"pad_token_id\": 0,\n",
    "  \"type_vocab_size\": 2,\n",
    "  \"vocab_size\": 28996,\n",
    "  # Labels\n",
    "  \"id2label\": {\n",
    "    \"0\": \"O\",\n",
    "    \"1\": \"B-MISC\",\n",
    "    \"2\": \"I-MISC\",\n",
    "    \"3\": \"B-PER\",\n",
    "    \"4\": \"I-PER\",\n",
    "    \"5\": \"B-ORG\",\n",
    "    \"6\": \"I-ORG\",\n",
    "    \"7\": \"B-LOC\",\n",
    "    \"8\": \"I-LOC\"\n",
    "  },\n",
    "  \"label2id\": {\n",
    "    \"B-LOC\": 7,\n",
    "    \"B-MISC\": 1,\n",
    "    \"B-ORG\": 5,\n",
    "    \"B-PER\": 3,\n",
    "    \"I-LOC\": 8,\n",
    "    \"I-MISC\": 2,\n",
    "    \"I-ORG\": 6,\n",
    "    \"I-PER\": 4,\n",
    "    \"O\": 0\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Ici l'architecture choisie est `BertForTokenClassification` ce qui correspond √† une t√¢che de NER (affecter √† chaque token un label). Et on retrouve aussi les diff√©rents hyper-param√®tres des transformers (nombre de t√™tes d'attention, param√®tres d'attention, ...). Pour pouvoir r√©utiliser les poids d'entra√Ænement d'un mod√®le pr√©-entra√Æn√©, il ne faut faire attention √† ne changer que ce qui ne casse pas la compatibilit√© avec le mod√®le pr√©-entra√Æn√©.\n",
    "\n",
    "Note: Les labels sont d√©finis avec le mode d'anotation \"IOB\" (I: Inside entity, O: Outside entity, B: Beginning of entity) ex: Je suis Yoann Couble serait annot√© \"O O B-PER I-PER\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1: use a pre-defined / pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"dslim/bert-base-NER\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get model and associated tokenizer from model hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use on example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': 'Wolfgang', 'score': 0.9990139603614807, 'entity': 'B-PER', 'index': 4, 'start': 11, 'end': 19}, {'word': 'Berlin', 'score': 0.9996449947357178, 'entity': 'B-LOC', 'index': 9, 'start': 34, 'end': 40}]\n"
     ]
    }
   ],
   "source": [
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2: use the raw BERT model and customize its heads\n",
    "\n",
    "Start from a [raw BERT model without any head](https://huggingface.co/transformers/model_doc/bert.html#bertmodel), but where the weights can be re-used.\n",
    "Create a child class and define the `__init__` and `forward` methods, see [example from BertForTokenClassification](https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertForTokenClassification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example from BertForTokenClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class BertForTokenClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        ...\n",
    "        # See doc linked above\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Within Spacy\n",
    "Spacy est compl√®tement int√©gr√© dans huggingface, il est donc possible de les r√©utiliser √† l'int√©rieur des pipelines spacy.\n",
    "\n",
    "Spacy fourni √©galement des mod√®les pr√©-entra√Æn√©s et bien int√©gr√©s √† Spacy. Ce sont les mod√®les finissant en `_trf` comme [celui-ci bas√© sur camembert-base](https://spacy.io/models/fr#fr_dep_news_trf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the model within your virtual environment / kernel (in my case, it's a pyenv environment)\n",
    "\n",
    "```bash\n",
    "pyenv activate 3.8.6\n",
    "python -m spacy download en_core_web_trf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">My name is \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Wolfgang\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " and I live in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Berlin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "doc = nlp(\"My name is Wolfgang and I live in Berlin\")\n",
    "\n",
    "displacy.render(doc, \"ent\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "proto-38dev",
   "language": "python",
   "name": "proto-38dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
