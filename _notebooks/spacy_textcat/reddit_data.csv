text,tag,subreddit,id
"I’m looking for datasets or api source that quantifies fan base, or preferably, bettors’ sentiment regarding a team’s performance or direction. Does anyone know of an API that tracks this? For now I’m looking specifically for NBA, but am also interested in MLB, NFL, and NCAA f-ball and b-ball.",API,datasets,s0vufk
"I'm making an ESG stock analysis program in Java, and so far the only free ESG API I've come across is ESGEnterprise, but I'm having trouble retrieving the data. Has anyone had any success/have any recs for other ESG APIs out there.",API,datasets,ruvj9n
"Hey everyone! I’m one of the creators of Sieve _URL_ and I’m excited to be sharing it!
Sieve _URL_ **is an API that helps you turn petabyte-scale video data into a high-quality dataset, automatically.**
It helps store, process, and semantically search your video data. Just think _NUMBER_ cameras recording footage at _NUMBER_ FPS, _NUMBER_/_NUMBER_. That would be _NUMBER_ million frames generated in a single day. The videos might be searchable by timestamp, but finding moments of interest is like searching for a needle in a haystack. Sieve tags useful attributes like people, motion, lighting, etc on every frame!
We built this visual demo (link here _URL_ a little while back which we’d love to get feedback on. It’s \~_NUMBER_ hours of security footage that our API processed in <_NUMBER_ mins and has simple querying and export functionality enabled. We see applications in better understanding what data you have, figuring out which data to send to labeling, sampling datasets for training, and building multiple test sets for models by scenario.
To try it on your videos: _URL_ _URL_
Visual dashboard walkthrough: Click on our site link!",API,datasets,rup1uj
"Hi guys,
I'm a new here.
Recently I am trying to develop something using Python.
What I need is CO2 emission data.
Could you guys recommend me an API of CO2 emission data?
Plotting CO2 emission data on y axis and x axis can be countries of the year, months, years of the country something like this.",API,datasets,rrbl7b
"Hi,
When pulling historical data from API's I often spend a lot of time reading their documentation, and writing boilerplate code to pull their data in.
As an alternative, I've built an API where you can query historical data using natural language, e.g. `w2v.get_one` or `w2v.get`
Under the hood I connect with many data providers, and process their data so that each result is a single time series. I calculate sentence embeddings for each result to make them easily queryable.
Does this seem interesting to anyone?",API,datasets,rz0888
"Show off, complain, and generally have a chat here.
Discuss whatever you've been playing with lately. Share/ask for tips suggestions and in general talk about services/tools/sites you find interesting.
Here you can rant, go off-topic, or self promote even but please be civil.
P.S: Suggestions for this subreddit are always welcome.",META,datasets,shgekp
"We have built 180Protocol, an open-source toolkit for data sharing and creation of unique data sets. It targets enterprise use cases and improves the value and mobility of sensitive business data.
Our alpha release is live on GitHub _URL_ Developers can quickly build distributed applications that allow data providers and consumers to securely aggregate and exchange confidential data. Developers can easily utilize confidential computing  to compute data aggregations from providers. Input/Output data structures can also be easily configured. When sharing data, providers get rewarded fairly for their contributions and consumers get unique data outputs.
Read more on our Wiki _URL_",code,datasets,s9d3yt
"Allen Downey's python boks and videos are all excellant 
Here is a video tutorial by him on survival analysis
_URL_ _URL_
The notebooks
_URL_ _URL_
The dataset on lightbulbs he uses
_URL_ _URL_
And his twitter
_URL_ _URL_
I have no connection with him other than liking his work.",code,datasets,s0l6ml
_URL_,dataset,datasets,sg44o3
"This dataset contains daily and annual gold rates from _NUMBER_ and _NUMBER_ respectively. It contains rate of gold per troy ounce in six major countries' currency. The countries included are India, UAE, Europe, Great Britain, China, US). Time series analysis and prediction can be performed on this dataset. Any views or inputs are always welcome!! 
Link to the dataset: _URL_ _URL_",dataset,datasets,seo5yn
"Hello m8s
I crawled reviews of _NUMBER_ different chocolate bars as well as metadata and US and Canadian chocolate producers. 
The data is available here _URL_
Also, the simple crawler that captured this data is available here _URL_
As always, credits go to the Manhattan Chocolate Society:
> Manhattan Chocolate Society, Flavors of Cacao \Internet\]. Available from: [_URL_ _URL_ 
Happy new year;
Cheers",dataset,datasets,rux7cw
"
_URL_ _URL_
I have put up some  parse trees online as a dataset. Not something as substantial as Penn Treebank, since the trees are not human-edited, but still way more parse trees than from Penn to feed into your later-stage NLP algorithms, free of charge or hassle.
The current format is straight from where they were generated. Suggestions of alternative formats based on ease of use would be heavily appreciated!",dataset,datasets,s1dit6
Pls how can I get a student-bot conversation dataset?,dataset,datasets,s8vcbb
"Hi everyone,
Can someone suggest to me some real-world datasets for anomaly detection? I have surfed the web enough for this and I am looking for those unique datasets for specifc domain  which have some tested anomalies.",dataset,datasets,s6nol0
"Canadian Tire is one of Canada's largest retailers with _NUMBER_ locations, and at least one location in all provinces/territories other than Nunavut.
Dataset includes:
* Store url
* Location Name
* Street Address
* City
* Province
* Postal Code
* Country
* Longitude
* Latitude
* Region
* Collection Date
* Store Phone Number
* Store Services 
* Store Hours
_URL_ _URL_",dataset,datasets,sbx2uo
"As part of my Master's thesis, I was to work on a few novel research topics. The core idea behind it was to train modified sparse versions of RNN, LSTM, and GRU for sequence learning purposes. While doing the research, I found out that the paper that introduced LSTM _URL_ worked on Reber Grammar Sequences, and realized this would be a good dataset to work with.
However, I could not find a dataset of Reber sequences, and therefore, I went on to create one for my thesis. I successfully defended my Master's thesis with very good grades, and therefore, now I am free to make this dataset public.
I have uploaded the dataset on Kaggle at: _URL_ _URL_
Its corresponding visualizations are at: _URL_ _URL_",dataset,datasets,sa6ak0
"I am looking for dataset related to object detection to detect object after house/office has gone through fire. Do any one have lead on this kind of data are available or not , or any leads regarding this kind of system. 
It would be helpful if I get any lead on this.",dataset,datasets,s0m46l
"Hi r/datasets,
CEO of DoltHub here. We just finished our basketball database bounty called SHAQ. Here's the data if anyone wants to use it:
_URL_ _URL_
And here's the write up on how it did:
_URL_ _URL_
We're on to our next data bounty, US Housing Prices, _URL_ _URL_ if the idea of getting paid to build databases intrigues you.",dataset,datasets,s0ud6e
"Hi im trying to predict twitter use age through the tweet text that they use. Is their any dataset I can use? 
Thank you!",dataset,datasets,s9mlwy
"Hello 
I hope this is ok to post in here. I have been working on a product launch for a brand new traffic product that provides a deep level of traffic data granularity through connected car data. It covers the whole of the US with hourly, day of week, monthly and directional traffic data. 
We’re launching this on Feb _NUMBER_, _URL_ _URL_ join the event to find out more",dataset,datasets,sdawol
"This is self-promotional in a way, but it fits the philosophy here.
At dolthub we're promoting our bounties to build collaborative datasets. This month we're hosting a competition to build the world's largest open dataset for housing data -- tracking it down to the sale.
_URL_
Feel free to join us on discord: _URL_",dataset,datasets,ryk6zd
"I was linking the Manifesto Project](_URL_ dataset to the (_URL_ which is a very painful enterprise. Luckily, I found the [Party Facts project _URL_ They had already linked those two datasets and many others. I hope someone finds it useful.",dataset,datasets,sbve6d
im searching video games sales since _NUMBER_ - _NUMBER_,dataset,datasets,s2fjbk
is there any benchmark dataset with object bounding boxes and its attributes,dataset,datasets,s9966h
"I published a dataset of 5m source code files from 15k open source files. Its long term goal is to enable identifying causality in software engineering. 
Data and code _URL_ 
Describing paper: End to End Software Engineering Research _URL_
People from ML, NLP, causality, and SE, might find it interesting.
The dataset enables investigating code similarity , program difficulty, defect predictions, etc.
The code is extracted every two months in order to investigate the difference. By the difference one can investigate if a change in a possible cause  leads to influence  _URL_ in which context. 
I plan to keep extending the dataset and would like to get feedback on it - ease of use, new use cases, etc.
If you have related dataset that can be merged with, related data that you would like to obtain or ideas for research directions, please contact me.",dataset,datasets,rvupxk
"hello, I am taking a course at my university and want to find data for a project. 
I essentially want to find out what are the most popular videos on YouTube that have been related to motorsports .
It would be really helpful to have the name of the video, date published, number of views, number of comments.
Any help would be greatly appreciated.",dataset,datasets,scg3gs
"Scraped all public votes in German federal parliament/Bundestag . A total of _NUMBER_ voting sessions are recorded. For each of the voting sessions, the votes of each of the around _NUMBER_ parliamentary member are recorded by name of the member. Note that the voting is not strictly along the party lines. Available as excel files and zip:
_URL_ _URL_
Original source :
_URL_ _URL_",dataset,datasets,s0ypi4
"Hello! I'm sharing a dataset of metadata for _NUMBER_ TikTok videos, scraped between _NUMBER_-_NUMBER_-_NUMBER_ and _NUMBER_-_NUMBER_-_NUMBER_. All the data was publicly available with no login required at the time of scraping. The data is available as flat JSON, and as a MySQL database. There are probably minor inconsistencies between the two formats, but they should be _NUMBER_% similar. Everything in the JSON file is unaltered response from TikTok, the MySQL database is a bit more trimmed down.
Total uncompressed size is around 200GB
magnet:?xt=urn:btih:475ea4ba18becf5e5f54cd0200999c7c45674fe6&dn=tiktok-_NUMBER_%5F07-_NUMBER_&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80%2Fannounce
## Other Stats
In addition to the videos, there is metadata on:
- _NUMBER_ sounds
- _NUMBER_ challenges 
- _NUMBER_ authors 
## Credits
Thanks to David Teather _URL_ for his TikTok-API project!
_URL_",dataset,datasets,sfq1zk
"Is there a data set or data base out there that contains comments from social media such as tiktok or Instagram?
Ive just browsed google about it but I haven't found anything",dataset,datasets,rwel5l
"Hi all,
**Would anyone know a good multimodal healthcare/medical data set?** 
The purpose is to conduct a study on machine learning models trained on multimodal health data.
Many thanks!",dataset,datasets,sd8rbe
"I am trying to build an app which would give me product details when i search it by name.
Example: I search for Apple iphone _NUMBER_
I should get all the informations such as description, Image, etc with it.
Please let me know if there is some open source datasource available
Please note that this information should not limited just to electronics good",dataset,datasets,s3rbof
"Hi, what I'm looking for is pretty simple but I've been unable to get to it. I want to do a bit of a cohort analysis on covid cases. I want to know what percentage of daily covid cases are occurring among the:
* unvaccinated
* post dose _NUMBER_
* post dose _NUMBER_
* post dose _NUMBER_
* etc..
Ideally for the UK. Thanks",dataset,datasets,rvzq1k
"Hello!
I'm looking to find a dataset that shows what percentage of demographics voted towards either major US political party in the past election. I've seen something that looks like this _URL_ but I was wondering if there was something free that was put up by an official govt source. If anyone found something like this, please share! I'd really appreciate it :) Thanks!",request,datasets,s44007
"hi all any good leads on datasets for fuel prices ad fuel consumptions? at least till dec _NUMBER_
thanks",request,datasets,rtxpp1
"I've noticed that it's become quite difficult to find a good open dataset for real-estate data. Mainly the features I'm looking for are:
* Bedrooms
* Bathrooms
* Amount of space 
* Price 
Anything else would be bonus. Any recommendations appreciated! Thanks!",request,datasets,rvgaxy
Hello! I want to make a diagram on how much the world spends for what. I read that we spend as much money each year on tobacco or soft drinks as we need each year to close the biodiversity funding gap. Does anyone have a dataset that uses similar examples? Thank you very much!,request,datasets,rzeyww
"Hi, looking for a data set that contains global air pollution for the years _NUMBER_-_NUMBER_
could only find till year _NUMBER_
any help would be appreciated :)",request,datasets,sgbkzf
industrial process datasets for anomaly detection. live stream data or static past data.,request,datasets,sar2yl
"Hey all!
I am looking for as much data about global wine as possible. Ideally I can get as much of this data in geospatial shape files of the wine regions as possible, but data detailing them could be used to create the region. 
Any data sets that include information on producing countries/regions, varietals, producers/wineries etc are helpful. 
Likely this data will be in pieces by country or region but I am willing to combine the data into a complete data set. 
Production and export numbers/values aren't required but wouldn't be turned away.
I have found a few sources that have some of the data by have not found any opensource geospatial data so far.
Thanks everyone!",request,datasets,rt2fjt
"I am looking for one or more datasets to build a business dashboard. Something that allows me to create some KPIs and progress charts for sales, billing, production, etc.",request,datasets,scls6v
"Hi community,
I planned to build a machine learning models that can read text biographies and extract out certain attributes such as the degree and the industry a person is working in. This is for a postgrad thesis i am working on.
I thought this linkedin dataset would be perfect to train such a model:
_URL_ _URL_
Unfortunately, the link is down :( Would anyone have an archival copy of this dataset ? Happy to buy a beer for that kind soul",request,datasets,s492ru
Looking for a dataset of books sold on Amazon. Any help woild be appreciated. I would also be interested in ways to scrap the data myself.,request,datasets,rsjzyk
"Does anyone know where to find this data? I'd prefer to not license the data from places like schooldigger and not having much luck apart from state-specific websites—I need it standardized for all schools nation-wide. 
I've looked at _URL_ _URL_ and they have great demographic and geographic data, but I'm not seeing the assessment data . 
Thanks!",request,datasets,rs3ur0
I’m trying to look for a dataset about spotify music trend analysis that has a big dataset linked up and at least from _NUMBER_. A full analysis would be helpful as well but just like a dataset is good enough and it has around _NUMBER_ samples . Thanks,request,datasets,s6ymzt
"Hi, i am looking for datasets that give me a comprehensive list of car makes, models and any assisted driving features they have such as lane keeping, cruise control and blind spot detection. Do you have any suggestions for where to find this data?",request,datasets,rs0euz
"I want to see the data for how each member of the house over or underperformed the presidency results, is it possible to find that?",request,datasets,rsl44b
Could someone please tell me if there exists a dataset of segmented images of ct scan for lung cancer detection?,request,datasets,sbklrg
"I'm looking for a data set that meets the criteria named in the title  based on latitude and longitude. I would say that at least _NUMBER_ rows for North America is a must , so imagine a 10x10 grid over North America with some information about each square.
Preferable size _NUMBER_ rows, but anything above _NUMBER_ would do .
Most important is the type of terrain , mean elevation and temperature.",request,datasets,sa7goq
Hello everyone! I'm looking for a dataset consisting images of various sneakers for my thesis project. Is there anything you know of?,request,datasets,s5opid
"Hey, I 3rd year student and now I’m doing my business intelligence project. I am interested in METAVERSE tokens but I found only a few datasets. If you have some datasets recommendations, pls recommend me. THANKS!!",request,datasets,s5ugnf
"I'm looking for video of people entering/exiting buses. Camera mounted inside of the bus on the ceiling, and/or door frame, aimed down towards the door and floor.
General task is to count the number of people entering and exiting a bus. Labels are not needed, but helpful.",request,datasets,rw9yr9
"Good day and thanks for reading. 
Is there an api or source for drug prices around the world? I’d like it to be an api that gets updated but I’d settle for a dataset that is recent. 
Thanks !",request,datasets,rsx6jp
"Hello everyone! So I'm in the lookout for historical documents datasets, that are preferably annotated for object detection.
Most of what I can find is either non annotated, or annotated for text retrieval. But in any case, please share with me anything related to the topic, it might just be useful, thank you!",request,datasets,s1g4aj
"I'm looking for the average daily land surface temperature of the entire Earth, calculated once per day",request,datasets,rvnylm
"Hi everyone, 
I’m searching any datasets of sports matches, like : football, hockey, tennis, basketball, etc. where I can have all the detail and statistics of a game. 
Is there anyone who know where I can find these datasets ? 
I’m newbie in web scrapping but if someone know a method to build my own dataset thanks to web scrapping I really want to know",request,datasets,s7qicr
Does anyone know where to find a dataset  about art  created/assited by AI...?,request,datasets,s4b5no
"Hi guys!
I am working on a project and i need a marketing strategy dataset to both analyse the efficiency of a marketing campaign and to create customer segments based on the strategy they prefer. The ones i found all used only phone calls/emails and i'm looking for one that contains digital marketing as well.
Would really appreciate the help!",request,datasets,sb4uiy
Looking for a dataset for the songs used in movie trailers or anything somewhat related.,request,datasets,rseek1
"Are there any datasets out there of antlered big game? Specifically, I am looking for images of deer/elk/moose/sheep after they've been shot.",request,datasets,rw4yoj
"I submitted a post asking for spatial datasets yesterday, but didn't provide enough information for people to give really useful examples.
I'm looking for at least _NUMBER_ datasets which have the following properties:
- The two datasets cover the same region in part or in full
- The data in both datasets is in the form of individual points
 - Not aggregated over large areas ex: states/counties or grids
 - Either the existence of a point in a location is the entirety of the data provided by a point, or a variable gives the value at each point
- The data should satisfy assumptions for Kriging
 - Data will be used to demo a framework in R  which performs Kernel Density Estimation, Kriging, and various forms of interpolation on two datasets and compares the resulting models
An example of the type of data I'm searching for would be a dataset containing the address  at which a crime occurred, where each point denotes a crime. ",request,datasets,shk608
"Hey everyone, I really appreciate the awesome community that all of you have built here together. This is my first post here and it is extremely important. 
I would really appreciate, if you could help me find EEG datasets. 
P.S. Please send for EEG, and kindly avoid iEEG.
Thanks in Advance.",request,datasets,sezs5d
"Looking for a data set that contains game by game attendance data. ESPN only provides season totals/averages, and I am having a hard time finding a data set that provides the game by game data. Can anyone help?",request,datasets,s7u1d2
I have only searched of this on google but I haven't had any luck,request,datasets,rwfv9i
"Hi there, I’m starting a blog and I am writing a piece where data on Twitter usage by US state would be incredibly useful. As recent as possible, I am seeking a state by state count of active Twitter accounts.",request,datasets,s0cav7
"Does anyone have any recommendations for how I can get hockey equipment data in North America? The data would be used to calculate market share for sticks, skates, helmets, protective equipment, and goalie equipment by brand and by geography .
 
**If you have good sources / data providers that would be able to provide information on an ongoing basis that would be great. One example I'm aware of is Nielson, but I'd have to look into if they offer this information.**",request,datasets,rxsq4a
"Hello 
I hope this is ok to post in here. I have been working on a product launch for a brand new traffic product that provides a deep level of traffic data granularity through connected car data. It covers the whole of the US with hourly, day of week, monthly and directional traffic data. 
We’re launching this on Feb _NUMBER_, _URL_ _URL_ join the event to find out more",request,datasets,sdazfk
"Hey,
There's plenty of datasets online that give stats around football games and they tend to go as granual as goals, cards, penalties and subs.
I'm looking for one that goes as far as to show every event, including passes, shots etc. Similar to you see on real time betting apps like bet365, flashscore
I don't need real time data so don't want to look into 3rd party services that offer this. Just a historical dataset of a league would be enough.
Any help would be greatly appreciated.
Thanks",request,datasets,s66j37
Looking for something that I could do ??? to in order to have some kind of comparative study of idioms... if for nothing else a new look at things.. thank you!,request,datasets,rrovod
"Hello everyone. I am looking for spatial data which tracks police use of force. A great example of what I am looking for is the NYPD Stop, Question, and Frisk data, which lays out firearm use, taser deployment, and OC spray with XY coordinates for each calendar year. Unfortunately, recent reports have uncovered that around _NUMBER_% of the data are missing, making it unusable.
Anyone ever come across a similar dataset?",request,datasets,s1h8cp
Hi may I please know whether it is possible to find datasets about Singapore’s transport system where many deep insights can be found?,request,datasets,s1alaw
I’m making a machine learning app for a school project where users upload a picture of possible bed bugs infestation and I want to create with tensor flow lite but I can not find a dataset can anyone help me :/,request,datasets,s0miot
"I'm looking for a dataset that contains a list of Jackpot wins that occurred in Las Vegas over time. 
It would look something like this but updated: _URL_ _URL_
I googled and found a bunch of articles, but wasn't able to find a list of wins.
Does anybody know where I can find such dataset? Please and thank you!",request,datasets,rzal4s
"looking for data sets on Missouri genealogical data birth, marriage, death, burial, etc. Any time period.",request,datasets,ryufpf
"Hi all,
I am looking for a dataset that consists of cities and their time zones. I am working on a fraud call dataset with fraud call times in GMT for all cities across several hundred countries. I want to convert the fraud call times into their local times and analyse if I can find any pattern. I really appreciate any help you can provide.",request,datasets,sbdu5w
"Hello. I'm doing project on tea bud identification and tea leaf disease identification. I've been searching for datasets for past few weeks & mailed few authors who made papers on this topic, unfortunately I can't find anything related to this.  Would really appreciate any kind of reply. Thanks in advance.",request,datasets,s7p1z8
I was wondering if anyone could find a report about Covid-_NUMBER_ or anything related to it that was published before December 30th _NUMBER_. I am putting together a timeline for something for college and was wondering if any article predates that. The earliest I could find is December 30th _NUMBER_.,request,datasets,rtzqsh
"where can I find one? I prefer to be in Kaggle, I just need a dataset to make a project about panel regression, I still new in DS and can't get data from big sites, need prepared dataset. thanks",request,datasets,seaen3
"Pretty much the title. I'm very new to this but have this deliverable to the client about providing a dashboard for fundraising. I just want to use any made up sample data but a bit extensive and with a few metrics. I looked on kaggle, data world but even if i find something close to fundraise it's just very different from what I'm looking for. I'm probably not searching correctly but please help me if you can find any dataset for this. Also, it should not be paid :)
Thanks!",request,datasets,s0u0vo
"I found the Healthiest Communities data and I would like to use it for a project looking at disparities in all counties across the US. It looks like I can easily download the top five hundred scoring counties, but I’m hoping to get the full data set. Does anyone know where I can find that, or a contact to submit a request?
Edit to include link to what I’ve found available: _URL_",request,datasets,sh3tdh
"I was wondering if there were any datasets floating around on player count for games in the pro circuit - up to date as of _NUMBER_, ideally with demographic information included - but just player count by month/day would be good enough.
I'm writing a paper on how gaming participation affects stadium attendance of major gaming tournaments etc.
Any help would be appreciated!",request,datasets,rv7v65
"Any recent dataset on US crime rate per state? Could it be found as government open data? Where would you go for this? 
Thanks!",request,datasets,sc3ko3
"Hello, I would like some assistance with regards to ordering data for orders. I have a screenshot of said data which I will try to upload in the comments section.
The issue is:
-orders are collected in column A using Product code numbers.
-data in Column B and C correlate. Column B are the product codes and column C product description for all the products in the store.
-evidently, not all products are ordered.
- I need to link the product codes for the products ordered to the corresponding product description in column c
Of note:
- these are not the actual products ordered. Just an example
- the data cleaning will be required for the past _NUMBER_ years
- some of the product descriptions and codes have been updated and are thus missing in the columns b and c. 
- this is only an example. The actual excel files contain over _NUMBER_ rows for orders placed and _NUMBER_ for the product code and description references.
 
Is there a simple way to clean up this data rather than doing it manually? I’m quite new to this so I would be very grateful for some guidance.",request,datasets,s96lzd
I am doing a project which requires the use of a dataset that has the origin and destination of people going to work. I was originally going to use LODES but I found some problems with it when looking at it in just specific parts of states. I found that most Census blocks did not have data. Are there any alternatives you know of or other suggestions?,request,datasets,rucfwq
For my dissertation im looking for datasets related to the sharing economy. Please help me out!,request,datasets,sclac3
I want to get started working with data viz in the new year and I've always loved animals so this would be a fun project to dip my toes into the water. Any datasets that reflect what I'm looking for would be awesome,request,datasets,rtpls9
I am looking for datasets for Aspect based Sentiment Analysis. I already know of the Laptop and Restaurant datasets but was wondering if there are any others,request,datasets,rtwktm
"Looking for a dataset of car and electric car sales, preferably a minimum of the last _NUMBER_ years. Purpose: to draw a training dashboard. Could be useful parameters: production volumes, shipments, sales, prices, buyers, countries, delivery times, etc.",request,datasets,rtbkb2
"I've been looking for a NASCAR dataset that has per-race stats for every NASCAR season that is normalized an mostly ready to go for a database. I'm looking for a dataset that at least has the NASCAR Cup series, but a dataset that has Xfinity and/or Trucks would be more valuable.",request,datasets,s778my
"Hello , I am looking for dataset for vibiration readings of servo motors detailed its health  to use in a predictive maintenance model and I would appreciate any help regarding this matter, thank you in advance .",request,datasets,s4ejxc
Looking to conduct a research on minority owned banks amd their effects on surrounding areas and their people. Need data on the county level to do a DiD regression.,request,datasets,ru8tpy
"Title. Is there market share or sales of total hybrid vehicle sales anywhere? If not, is there regional data?",request,datasets,rxr0xc
"I am looking for Infrared Scans and data sets. Ideally from arial photogrammetry and have radiometric and GIS meta data associated with them. We are developing software for disaster management, emergency services and SAR. location doesn't matter but content would be ideal the more interesting the better example) scans of animals or humans, wildfires, urban and rural areas. 
Any leads I can get the better we and sort through bugs in the software.",request,datasets,shdcx5
"phonemes are just sounds that make up words, for an open source speech to text library I’m making! Thanks.",request,datasets,s82xiv
"Title says it all. I am doing some research into how the human pose changes when a person is concealing a firearm. So, I am preferably looking for a dataset of people concealing their firearms. I need a full-body photo or video footage of this. Because I'm using pre-trained pose detection, I don't really need any specific labelling, preferably just something to say whether they are carrying or not.
Thanks in advance!",request,datasets,s0ferq
"I'm looking for datasets related to mental health, ideally tweets  where users self-reported their diagnoses  or share that they attempted suicide, then including these users' tweet histories. That means I'm looking for datasets with users annotated with their mental illness. 
",request,datasets,sa42im
"Hello all, help needed or at least a push in the right direction for statistical data on pleasure crafts community .",request,datasets,sg93sb
"I’ve got an exciting idea for my capstone. Some friends and I are planning to renovate a garage and add an ADU. For the capstone, I would like to design a model which can predict building costs . 
I’m looking for data but coming up short. I’ve looked through Kaggle with limited success. Also, I’ve found some papers on Google Scholar describing different techniques of estimating building costs, yet I’m having trouble getting ahold of any actual datasets _EMOJI_ 
Do you have any ideas as to where I might search? 
Another exciting idea I’m considering is building a web scraper to collect the data. Any thoughts on this? Would it be too wide of a scope for the project? 
Not sure how to move forward exactly and any help would be great. Thanks!",request,datasets,sbyub2
"The idea is to determine impossible travel given two countries and duration.
Edit :was able to obatin the data using geopandas",request,datasets,rwio84
"Hi,
I’m part of an art group from Switzerland currently studying at HSLU Design & Arts (_URL_ _URL_
The group consists of:
Karim Beji (_URL_ [_URL_ _URL_
Emanuel Bohnenblust (_URL_ _URL_
Lea Karabash (_URL_ _URL_
Yen Shih-hsuan (_URL_ [_URL_ _URL_
At the moment, we are working on a project on the topic if AI can augment the happiness of humans. To answer this question, we are mainly working with chatbots. The end result is going to be an exhibition at the end of March. 
For that exhibition, we want to conduct a trial in which people from over the world chat with a chatbot to find out if and how it augments the mood of the participants. 
We would give you access to a GPT-_NUMBER_  chatbot and ask you to a) record yourself through a webcam  while you are chatting and b) simultaneously screen record the chat window. 
In the exhibition we would have a) a book with all the chats and b) small videos with your faces  to assess your mood. 
We would have a Zoom meeting beforehand to discuss everything.
Looking forward to your message!",request,datasets,sgyqay
"Hello, 
I'm looking for a large dataset  of pixel art character, but the characters need to have the same shape concept , i would even prefer if the image was only of the character's face. 
Does anyone knows any datasets or websites that would suit my goal ?
Preferred size would be 24x24",request,datasets,sd7vo9
"Hey guys,
does anybody have a dataset for aircraft price. It doesn't matter of used or new onces. I have just a dataset with small aircraft, like c172 or piper... But I need a price/cost dataset with large airplanes like _NUMBER_, _NUMBER_ or older aircraft types. 
If anybody can help me find remotely similar it would be a huge help. Thank you!",request,datasets,rssq7r
"Can anyone point me to any barometric time series datasets that have a high time resolution? Ideally, there would be at least _NUMBER_ data point per second. I *might* be able to get by with _NUMBER_ minute intervals, but this seems a bit coarse. I can't seem to find any archived historical data like this.",request,datasets,sb06hr
"Hey everybody,
Looking for a US mortgage default/foreclosure rate dataset by US county broken down by year from _NUMBER_ through _NUMBER_. 
Any help in locating this dataset would be greatly appreciated.",request,datasets,s5nwxb
"I need the dataset for AI training. Preferably, each song should be more than _NUMBER_ minutes long.",request,datasets,sd73gi
I need Electronic components identification and function dataset to be used in relational database,request,datasets,rrqj1k
"I'm working on a project that needs data of air conditioners' electricity consumption under various conditions. 
So far, I found a dataset on Kaggle _URL_ for Chillers and it's great. But I couldn't find any for normal split air conditioners or VRF systems. 
Any ideas where I can find some data? Thanks in advance.",request,datasets,rwev88
"Hi there, 
I am a total newbie and this is the first time, I am trying to create a Business Intelligence and Data Visualization portfolio. The portfolio will consist of _NUMBER_-_NUMBER_ projects and it will be hosted on a website. The project pages will then be linked to LinkedIn. The goal is to then apply to freelancing websites like Upwork.com.
I am looking for datasets which I can use to create porftolio projects where I can show off my skills. I am looking for a website where I can find such datasets where I can choose one from. If someone here has a dataset I can use that would be great as well.
If I had to narrow down my interest, I would say that my primary interest is providing small businesses business intelligence services as a freelancer. So a dataset of a small business where I can create a dashboard along with a business intelligence report is the best fit.",request,datasets,ryecov
"I'm searching for a huge quantity of well-written sentences , that preferably don't have factual information such as dates, names or events. So potentially they could be merely descriptive, and if possible, available in languages such as French, Spanish, German or similar.
Thanks in advance.",request,datasets,rv957a
"Need sample data for timekeeping. Specifically time in, time out, on/off lunch, durations. 
TIA",request,datasets,rwrwtc
"As the title says, I'm searching for a dataset that has the digitized line items from receipts. When I search for receipt datasets I am only able to find images for training OCR models but I only need the actual text from the line items. This would be the same data one would get from using an OCR to scan the receipt but all I need is the line item text along with its price.
TL:DR 
I need the results from using an OCR not the data to train an OCR from scratch",request,datasets,rxjrg1
"I know the NCDC has a pretty robust selection for hourly and monthly data, but are there datasets with more frequent readings out there? The only option I've found is here _URL_ but the locations are pretty limited.",request,datasets,sdj6vs
FSOCO or any other Formula Student dataset with FSG/FSE styled cones for driverless,request,datasets,s55itk
"I'm preparing for a PoC to create energy building mangement system for industrial buildings. I'm looking for a time-series dataset of production facility / factory. Ideally, data from the building would be divided into sections / floors, where each section / floor has it's own measurements taken. 
I searched open-source datasets available online but haven't found anything of that nature. Does anyone know such publicly available dataset?",request,datasets,rtntc6
"Hey, I am doing a statistics course for my MSC and I am wondering if you guys could point me in the direction of species abundance data for at least _NUMBER_ sample sites, and the abiotic data for each of the sample sites. I greatly appreciate any feedback you guys have!",request,datasets,shl078
Anyone know where I might find a dataset for wheat yields by province in Afghanistan for the past _NUMBER_ years? Bonus if other grain crops are also included. Thanks.,request,datasets,sekvh4
"Hello,
I'm looking for a dataset with main insights already found but badly visualised, does someone have one ?
Thanks a lot",request,datasets,s3bf3m
I'm currently pursuing Master's in computer Science and engineering and my area of research is Technologic Innovation in Hybrid Education and learning. Now to test my hypothesis I require student dataset of students who are part of Hybrid Education based model. Any idea where can I get this kind of dataset or how can I proceed to get the dataset?,request,datasets,rte16w
"Hey guys!
I was searching for the ozone layer data for the pst few days and I couldn't find any open source reliable data that have the data updated after COVID!
Most of them stopped before it and no updates after that!
Could any of you point me to a good place to start my search because I think I'm searching the wrong way here.",request,datasets,sdt6f2
"I’m looking for data on sunlight levels by U.S. states, preferably also by county. I’ve only been able to find data from >_NUMBER_ years ago so I’m not sure if I’m looking in the right places for this data",request,datasets,s6rkod
"I'm working on research for datamining spatial datasets, and I've been butting my head against a wall due to a lack of good data. What are some good publicly available spatial datasets? I would prefer large datasets, and either datasets with multiple variables or  multiple datasets covering the same area.",request,datasets,sgr03o
"Hi,
I'm looking for a place where I can download for free as many datasets as possible about pretty much any subject imaginable. Preferably datasets that come in CSV or Excel files and that can be downloaded in bulk, instead of one by one... Any suggestions?
Thanks in advance",request,datasets,rzod4w
"I am looking for a dataset that has mapping from job titles to skill keywords required for the job. I have already explored ONET so, looking for something else.",request,datasets,rs10qz
Hi. Where can i find _NUMBER_ years worth of data for housing prices in canada by province?,request,datasets,s7yf5r
"The data set must have:
* _NUMBER_ or more predictors, and
* _NUMBER_ response variables to be predicted
* The response variables must have some sort of shared dynamics
Not sure what to be looking for, really! Kaggle has a lot of data sets but I'm not sure if any would fit the aforementioned requirements. I'd appreciate it if someone could suggest data sets.
Some topics that sound interesting: climate change, sales forecasting, weather forecasting,.. though I'm not sure where to find these data sets and if they fit the _NUMBER_ requirements above.",request,datasets,sggoos
"I'm working on an app for a client of mine and we need an updated list of prescription drug prices. I have contacted GoodRx but they won't work with ventures that don't currently have a user base. 
My client is a doctor so I'm not sure if we could use that to get access to some data.
Any help or info to point me in the right direction is appreciated.",request,datasets,sbm5ex
"I need for a project to make a wrong statistical analysis of a dataset that looks like its conclusions are true . I mean, for example use a simpson paradoxe or a wrong interpretation of p-value to fake prove something. Have you got any fun ideas ?",request,datasets,s3tnbk
I am looking for a font recognition dataset in Arabic to recognize the font of a given text image. Is there is any dataset for this task,request,datasets,s4uhwg
"Does anyone have a suggestion , as I am looking for a data set of venture capital firms and accelerators ? specially ones that work with minority entrepreneurs ... thanks",request,datasets,rvb350
"Including store names, etc...
Lots of info here, but not at address level: _URL_
I've found this: _URL_ but there's lots of missing data...
Another incomplete dataset: _URL_",request,datasets,rwruo1
"Need help creating a data set I’ll be using. analogies below to help avoid industry specific jargon usage:
I have a group of friends who all have different food preferences and budgets . I need to create a dataset for their preferences so that if on a given night I say I’d like to go get Italian and pay $_NUMBER_ I would be able to pivot the data and see whose budget I’m within and who likes Italian food and who dislikes Italians food.
The problem is I can’t figure out how to organize the preference section of the data set. While min and max price are easily added as a single cell in a spread sheet for each person. Some of the individuals like _NUMBER_ foods and dislike _NUMBER_. What is the best way to organize the data so I can easily analyze the data set to figure out which individuals I would and would not like to ask to go to dinner with.
Any videos or help would be greatly appreciated
Thanks for any help that comes my way",request,datasets,rx8aua
"Hi Everyone. 
I’m new to data analysis and was wondering if someone could give me some tips for a specific problem. 
I have a small dataset of venture funds that invested in different companies. I’d like to find out which funds are most similar to each other and which companies were invested in by funds who are similar to each other. 
I’m not sure how exactly to get this information. I believe I’m trying to find a qualitative standard deviation? 
The closest thing I could find online is the following: _URL_
Unfortunately I’m not sure if this is what I’m looking for and how I would even apply it to my dataset. 
Please let me know if this issue is similar to something you’ve dealt with before. Thank you!",question,datasets,ruf6k6
"At my old job, it seemed like I'd have a new project with a new dataset every few weeks. The hardest part of my job was understanding the data not completing the project. 
Last year, I built a data catalog using the no-code platform bubble and shared it here. We ended up with quite a few people testing it out and using it on personal projects. In the last _NUMBER_-months, I took the original platform I built and leveraged some open-source platforms like Amundsen to rebuild a modern data catalog focused on making data documentation transparent, collaborative, and straightforward for anyone or company. 
We have a sandbox environment with dummy data that we're looking for user feedback on. If anyone is interested in giving it a spin, please let me know! We're planning to release a public version for anyone to use early next year.
 
Happy New Year, and I appreciate anyone willing to give it a try.",question,datasets,rrblgb
"Hey everyone,
I am wondering if anybody might know of a dataset that contains non fatal car accidents incidents for each U.S. states. I know FARs is a good source but all those accidents resulted in a fatality, and I am wanting to do some analysis on non fatal accidents. 
Any suggestions would be greatly appreciated.",question,datasets,s936qg
Just wanted to know what are the best tools available in the market for web scraping including both the free and the paid ones.,question,datasets,rr71t4
"Hello everyone, I am new to NLP and i am currently working on training a custom NER model. 
Can anyone give me advice on how to set it up so that any new data that is being scraped gets annotated automatically and stored in the database?
I am using SimpleTransformers library for the training part
Thank You very much in advance.",question,datasets,rsnqps
"I am currently looking for county-level economic data that contain information like the unemployment rate, income, GDP growth, etc. Any suggestions would be greatly appreciated.",question,datasets,sekogt
"Hi, I need recent Instagram dataset for mi University project, Someone can to help me?",question,datasets,s03zmg
"im looking for a dataset that can give me insight on where car accident impact happens most. ie. are they head-on, rear-ends, side-swipes, etc.
i know Tesla is collecting a bunch of data about this but I don’t think it’s available to us.",question,datasets,rvys9b
"I am taking an IT class right now and I'm trying to figure out what data normalization is in mircrosoft access in SIMPLE terms 
What is the difference between 1st, 2nd, and 3rd normal form?",question,datasets,sbvuyz
Would greatly appreciate if anyone could help me out in how to map any of the ORBIS company IDs  to an industry classifier .,question,datasets,s5o845
"I am looking for a public domain dataset with astronomical images of stars. They doesn't have to be fancy or high resolution, I just need original photos of stars and the name for each star for a computer science project. Can you suggest me such a resource?",question,datasets,saqm4f
"Preferably from some scientific or technical background.
Thought of the UCI Reuters Newsgroup dataset. But it is too well-known and onlay available in a strange XML format.",question,datasets,ryagsp
"I have a graph that shows average IMDB movie rating vs. runtime. It shows that movies that are _NUMBER_ minutes long are generally rated lower than shorter and longer movies.
I have some questions I would like help with:
* Is this relationship helpful for movie producers to use?
* What factors could be influencing this result?
* What sort of actions should be taken for producers to improve how movies are received by audiences?
* What sort of analytical approach should be tried to dissect this result?
I would like to get your thoughts on how you approach this problem.
Graph: _URL_ _URL_",question,datasets,sf9zf2
"Hi there,
I am grabbing the table below from data.census.gov _URL_ 
 Census - Table Results _URL_ 
I'm noticing a lot of my data in ""meaning of sex code"" and ""meaning of race code"" equals ""Total"". I'm struggling to understand what this actually means. Does anybody on here have experience with this survey and could guide me in the right direction?",question,datasets,s3xuep
"Hello everyone,
I am searching for covid _NUMBER_ datasets. I have found some sources which had what seemed very granular and rich datasets . However, and after analysing many of them, it always turns out that a lot of data is missing
Can you guys share the most complete dataset you have handled so far ?
Thanks in advance",question,datasets,s0gycg
"I'm hoping it's acceptable to ask this question here.
My organization has a dataset of about _NUMBER_ records. We'd like to build a system where the data can be updated by multiple  parties, and replicated among all subscribers. We're concerned about data loss, data precision loss, and mid-air collisions, etc.
Aside from a centralized storage/API for the data, are there any established solutions for distributed storage and replication among multiple subscribers that would account for versioning, loss prevention, and replication?
I hope this question makes sense.",question,datasets,s1vbc5
"Hi! I aim to make an statistics with the results of natural and anthropogenic risk factors and develop a hierarchy of risk factors to identify the places where each risk factor poses low, medium and high threats. Which statistical test would you recommend me to do? Thank you!",question,datasets,sa8vzp
"Hi 
 Where could i found a dataset for cereal disease forcasting ?
Ps: satellite imagery 
Thanks",question,datasets,s96ca7
Hi! I am trying to figure out if there is any data provider that maybe I could use to find targeting details that a specific company uses for their ads?,question,datasets,s1997c
Have there been any other studies done that are similar to the GSS?,question,datasets,sec7gc
"I've recently started using scrapy and selenium to build my own datasets, but I've been having trouble finding websites that actually report or contain the raw data. Most websites provide just a generalized statistic of the data, which I would rather not use. 
One good website that I found was _URL_ _URL_ but I haven't found any others like it. If anyone knows of good websites I'd greatly appreciate it if you shared them.",question,datasets,rqintr
"Hi all, I'm sorry if this isn't the right sub for this.
I downloaded the PRC Data Breach Chronology _URL_ .csv dataset which I'm planning to use for my Master's Thesis. The dataset features a column named ""Description of Event"" in which, as the name suggests, a text description of each incident is provided; unfortunately, part of these descriptions are not properly delimited by quotation marks, which means that every time there's a comma in an unquoted text description the column breaks, which in turn makes the dataset unusable for my purpose. This happens whether I open the file in Excel or in RStudio.
I'm fairly new to data analysis and have really little experience in handling csv files so I can't come up with a viable solution other than manually removing all the corrupted rows using Excel , and I can't seem to find any solution via web search.
Is there any other way to solve this issue?Thank you all in advance!
EDIT: here is a screenshot of the problem I get for more clarity. _URL_ _URL_",question,datasets,rs62t3
"Is there any research out there that shows the average age people with lower income have kids, or that shows the average income of people who have kids under a certain age, like _NUMBER_ ? Are there any resources for finding something like this out? I imagine I could pour over census data but I wouldn't even know where to start with that. 
Any help would be appreciated, thanks in advance.",question,datasets,seose3
"How to find monthly google searches for a specific keyword I have seen people say to use google keywords planner but that needs an AD Campaign I am looking for something else which is free
I just want to make something like the most popular anime from the last _NUMBER_ years for just a fun side project, the video on youtube which you can find by searching ""Most popular anime from _NUMBER_-_NUMBER_"" has instructions on how he did it in the description but I am unable to find it Thanks",question,datasets,rrt8xd
"Hello friends,
I need help. I am currently using Europeam Values Study data set  and i did crosstab for two variables - country code and political party support.
The problem is that i have been given all the countries and all the political parties.
I would like to sort varibles in a way that i see only a specific country and the support for the political parties only in that country.
Thank you im advance",question,datasets,s4iglw
"Hi all,
I am looking for a dataset containing phone calls recordings, preferably in English. Does anyone know of something public? 
Thanks!!!",question,datasets,rr1ip4
"Hello everyone,
as per the title i am trying to find some useful datasets from the automakers containing data such as production levels , money spent on research, different costs and so on.
Does anyone where i could find such dataset? 
Thanks for the help!",question,datasets,sbu2bt
"Will I ever need to know networking, information security, java, or web development in analytics? I have a professor who told me that you never know if they could come in handy or not, but I would prefer not to take them if they aren't going to help me out in the future. 
\`",question,datasets,sf1qqx
"Let's say there is a poll that _NUMBER_ people voted on. The
topic and choices with votes in parentheses are as
following:
Which brand of TV is better?
_NUMBER_. Samsung 
_NUMBER_. Sony 
_NUMBER_. Same thing 
How would you interpret the data?
_NUMBER_. ""Samsung"" would be the answer bc it had the most votes
OR
_NUMBER_. ""Same thing"" be the answer be it encompasses the two
individual options in a singular choice and the two
individual options  are fairly evenly distributed
Side question: 
How does choice _NUMBER_ affect votes? Couldn't I
have drawn the conclusion they are about the same just from having choices _NUMBER_ and _NUMBER_ polled?",question,datasets,s9nc3y
I have details about patients in a csv. Each row has data about a separate visit. How can I group all the visits by patient so that all their medical history is available from that? I want to use the dataset for an ML model to see if the model can predict a disease before the doctor diagnoses it. But I'm confused about how the input data should look if each row had details about a separate visit and the number of visits differ by patient. Have you seen any examples of what I'm trying to do?,question,datasets,sgc7or
Do you think there is a market for building off the shelf computer vision datasets? If so what areas are the most in need?,question,datasets,s37rdn
"i am coding a budgeting applet that takes data from a users account activity. so far, only .csvs i have found online have super vague transaction titles such as “purchase” “insurance” and i was wondering if anyone had something more specific? such as individual store names or spending categories.",question,datasets,s0thrl
"I've got a good custom scrapper in python for the rental market in my area but I've been procrastinating on finishing the project. I need to validate every post manually and compare it to similar posts to flag reposts 
Is my only option to write a flask app if I want a nice visual data browser to go in and validate all my data? I already relate all similar posts during the scrapping so it just needs to display _NUMBER_ post at a time from the list that have a false in the manually checked col. I need to be able to edit all the fields and view the saved pictures and it needs to display the list of similar posts somewhere on the screen with just the relation confirmed col editable.
Right now the stapler saves to sqllite and the pictures col is a relative path/name but that can be changed if needed.",question,datasets,rrzxs8
"Hi community :)
I don't know if this is the right subreddit, please excuse me, I'm new in the hole data game. 
I'm searching for some cost of capital data for my bachelors paper. 
Do you know a website with some data of companies from _NUMBER_ to _NUMBER_ or longer. 
It's really hard to find something, but may I have better luck with good old reddit advise. :)",question,datasets,rw0qiz
Does anybody know of a publicly available and easily accessible data source that has demographic information  on NBA players as well as their on-court statistics? I know basketball reference is a really great source for player stats but I'm not sure it has the demographic information  that I might need.,question,datasets,s04039
"Hi! I've started a  course in data analysis, and the final assessment is a project requiring ""real world data"". I'm honestly not sure where to start looking for what I want .
Is there a FAQ/list of popular data sources? I don't necessarily need it to be free, but I'm not a millionaire either, so go easy on me :)
Thanks!",question,datasets,shbzwe
"Is there a gov source  to get the list of businesses registered ? Looking for U.K, U.S and Australia.",question,datasets,rvwtn5
"So my vacation plans fell through, I was supposed to travel but because of COVID, I've had to cancel. Which means I have _NUMBER_ weeks to do absolutely nothing. I want to occupy myself. I'm not much of an idea guy, but am a seasoned engineer.
I want to work on building either an API or a dataset. Any field, the only requirement is that is it be useful to someone or some people. I'm not asking to be paid either, completely free.
So if you have any ideas on what might be worthwhile or useful to work on.. or even just lots of fun, please let me know. I have way too much time on my hands. Thanks.",question,datasets,rzl8ye
"I am trying to run reports from this data source: Census - Table Results _URL_ 
This is data to tell the race and sex of business owners in a certain region. My struggle is that Race and Sex columns include a lot of data results that are ""total"". What does this mean? Anybody have any experience with this or census data in general?",question,datasets,s3y6du
"First time poster in this sub, and I'm only an amateur with data compared to many.
But, want I want to do is figure out the partisan lean for each of the new state house districts in Texas. This type of analysis is easily available for the federal districts at sites like _NUMBER_ , but I want to do it at the state house level.
Then, as a second step, it'd be great to have the correct data to draw my own state congressional districts and derive partisan leans . But, this would be a much larger project for the future.
Anyway, I'm struggling to figure out where to even start. I could get precinct data from the MIT data lab, but state districts break up even precincts, so they wouldn't be totally accurate.
Thanks in advance for any thoughts or help!",question,datasets,rxmkvq
"I need dataset with multiple tables to do query optimization analysis and write article about it, including datset characteristics and step-by-step approach. Can't find one. 
Thanks.",question,datasets,rs4wol
"Hi guys,
I am doing the review questions from Thompson: foundations of behavioral statistics, chapter _NUMBER_, question _NUMBER_. I cannot seem to conceptualize the correct answer. Would appreciate any help as I need to understand this review for the final exam at the end of term :)
""For a given dataset, how will using mean substitution for missing values impact the coefficient of skewness for data that were initially skewed? How will using mean substitution for missing values impact the coefficient of kurtosis? How does mean substitution differentially impact post-substitution means, SD, skewness, and kurtosis?""
Thank you!",question,datasets,s8w054
"I am following this great tutorial to learn how to create my own **sentiment analysis** pipeline using Python **SpaCy** package: _URL_ _URL_
This tutorial uses the ""Large Movie Review Dataset"" for training & test sets. Where do I find more datasets for different applications?
I need to analyze the sentiment of texts that users highlight in a **diabetes** brochure and website, to see if I can predict from the highlighted text whether a user is a diabetes patient or only just diagnosed or a friend or family member of a patient, etc...
What dataset should I use instead of the movies dataset used in this tutorial?
Thanks",question,datasets,sdt0jx
"I spent over a month creating a hand cropped and labeled dataset with _NUMBER_ images and _NUMBER_+ train labels from an 80s Anime. Ultimately my use case had too much error to even be close to useable at that scale , but maybe all of that work could be useful to someone for something simpler .
My question is how strongly does Reddit  defend Fair Use? The images are from a copyrighted work, but would likely be classified as fair use by any rational person. .
Also related, if the site won't be overly skiddish, does anyone have good file hosting recommendations for the total file size of _NUMBER_.7GB  and _NUMBER_.6GB ?",question,datasets,se9yie
"Just thought I'd share this Goodreads dataset here _URL_ It took me quite a lot of internet sleuthing to find an interesting, complete and large dataset to practice machine learning and more specifically recommender systems.
This data was originally pulled from Goodreads in _NUMBER_ by Zygmunt Zajac . It contains detailed metadata information for **_NUMBER_ books** , as well as _NUMBER_ million individual numerical ratings collected from _NUMBER_ users. There is no demographic information available for users, but the different files included in the release form an interesting basis for a recommender system.
I have released an expansion pack of sorts for this dataset, that adds book descriptions, genres and other features, enabling the use of various NLP strategies. **See here for the augmented dataset. _URL_ Cheers.",resource,datasets,sa9vuu
"Hi there, i currently developing ai chatbot, but i finetuning on clear data, my chatbot doesn't support nsfw topics, so i am searching for nsfw chats. If u have some parsed chats of humans it could be usefull too!",resource,datasets,ru7owv
"The dataset provides information on games that have been prohibited by various countries for a variety of reasons, resulting in governments making political decisions and establishing strict regulations against games that involve violence or violate religious or cultural feelings. This is a list of video games that have been prohibited or outlawed by various countries throughout the world. Governments that have outlawed video games have been condemned for increasing digital piracy, reducing commercial prospects, and infringing on people’s rights.
Click here to visit the dataset _URL_",discussion,datasets,s1d4om
"I collected news articles over the past _NUMBER_ years. Currently I have about _NUMBER_ million datapoints.
I played with it, did a lot of aggregations but now I would appreaciate a new set of eyes to look over the dataset and brainstorm possible visualisation ideas.
The Dataset is build like this:
||**url**|**title**|**author**|**pub\_date**|**categories\_json**|**publisher**|**lang**|**title\_clean**|**body\_clean**|
|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|
|**\_id**||||||||||
|**_NUMBER_**|_URL_ _URL_ Street parties inquiry: Opposition MPs...||_NUMBER_-_NUMBER_-_NUMBER_ _NUMBER_:_NUMBER_:_NUMBER_+_NUMBER_:_NUMBER_|\|bbc|en|\[downing, street, parties, opposition, mps, fi...|\|
|**_NUMBER_**|_URL_ _URL_ Look Up: Mixed reviews for Leonardo DiCa...||_NUMBER_-_NUMBER_-_NUMBER_ _NUMBER_:_NUMBER_:_NUMBER_+_NUMBER_:_NUMBER_|\|bbc|en|\|\[jennifer, lawrence, leonardo, dicaprio, film,...|
|**_NUMBER_**|_URL_ _URL_ a genocide taking place in Europe?|RT|_NUMBER_-_NUMBER_-_NUMBER_ _NUMBER_:_NUMBER_:_NUMBER_+_NUMBER_:_NUMBER_|\|rt|en|\|\[backdrop, war, scare, ukraine, statement, pre...|
|**_NUMBER_**|_URL_ _URL_ Maradona: Diego Maradona's younger brothe...||_NUMBER_-_NUMBER_-_NUMBER_ _NUMBER_:_NUMBER_:_NUMBER_+_NUMBER_:_NUMBER_|\|bbc|en|\|\[hugo, maradona, brother, legend, diego, heart...|
|**_NUMBER_**|_URL_ _URL_ Senate overwhelmingly passes massive defenc...||_NUMBER_-_NUMBER_-_NUMBER_ _NUMBER_:_NUMBER_:_NUMBER_+_NUMBER_:_NUMBER_|\|aljazeera|en|\|\[legislation, president, joe, biden, approval,...|
title\_clean and body\_clean are tokenized by spacy.
body is excluded for this reddit post because its too long, but its html code.",discussion,datasets,sg9lv8
"Carried on from Second Discussion Thread _URL_
> Carried on from Original Thread _URL_
> 
> > You have probably seen most of these, but I thought I'd share anyway:
> 
> > **Spreadsheets and Datasets:**
> > 
> > * _URL_ _URL_
> > * John Hopkins University Github _URL_ confirmed case numbers.
> > * Google Sheets From DXY.cn _URL_ 
> > * Kaggle Dataset _URL_
> > * Strain Data _URL_ repo
> > * _URL_ _URL_ 
> > * ECDC _URL_ 
> > 
> > **Other Good sources:**
> > 
> > * BNO _URL_ Seems to have latest number w/ sources. 
> > * What we can find out on a Bioinformatics Level _URL_
> > * DXY.cn Chinese online community for Medical Professionals _URL_ \*translate page.
> > * John Hopkins University Live Map _URL_
> > * Mutations _URL_ 
> > * Protein Data Bank File _URL_
> > * Early Transmission Dynamics _URL_ Provides statistics on the early cases, median age, gender etc.
> > 
> > **\IMPORTANT UPDATE:** *From February 12th the definition of confirmed cases has changed in Hubei, and now includes those who have been clinically diagnosed. Previously China's confirmed cases only included those tested for* 
> > 
> > **There have been a bunch of great comments with links to further resources below!** 
> > \
- COVID-_NUMBER_ Mobility Data Aggregator](_URL_ [^ _URL_
- County level mask mandate data set](_URL_ [^ _URL_ 
- NYT county level cases and mask usage](_URL_ [^ _URL_
- Please check the comments of the previous threads for more datasets.",discussion,datasets,n3ph2d
"We're an early stage MIT based startup company called Sync Computing. We just came out of stealth and are looking for early user feedback.
_URL_ _URL_
Hopefully this tool will be useful for people. We have a case study now of how it works with Duolingo here. _URL_",Blog,dataengineering,sdaml5
"We have recently released a project to generate markdown documentation for data pipelines. The tool uses yaml files to load definitions, so as long as you can export your pipeline metadata and dependencies to yaml, Lineage will be able to generate a documentation site. You can then use the rendered markdown as part of a static site using MkDocs or VuePress.
Why not Apache Atlas? Apache atlas is great as an interactive tool but we often needed something to generate static documentation and lineage charts as part of a CI/CD process.
Why not DBT? DBT has a great documentation module but requires that the pipelines are modelled in DBT in order to work.
Would love any input or feedback!
_URL_ _URL_",Blog,dataengineering,s327cq
"A data pipeline to analyze weather and electricity prices.
_URL_ _URL_
Would you want to use a similar data pipeline?
View Poll _URL_",Blog,dataengineering,s60oad
"Learn how to use Snowplow Micro to implement end to end behavioral data tracking testing in your development practices. Register here _URL_ 
_NUMBER_\/_NUMBER_\/_NUMBER_, _NUMBER_:_NUMBER_ GMT _URL_
As usual, you'll have an opportunity to ask questions at the end.
Look forward to seeing you on Jan 19th, _NUMBER_:_NUMBER_ GMT.",Blog,dataengineering,s19nl7
"Disclaimer: I work at Monte Carlo. 
Our team put this post together to help data engineers communicate why - and how - dashboards and reports break to downstream stakeholders. 
Next time you're pinged about bad data or a wonky spreadsheet, forward this along as an FYI. :) 
_URL_ _URL_",Blog,dataengineering,s8y33y
"I wrote a few thoughts on why standardization in data ingestion is a thing. My experience comes from building products for data ingestion and I wanted to share the reasons it makes a lot of sense for companies to try and introduce standardization in data ingestion.
There are two different types of arguments presented, one type has to do with the benefits of building a superior product and the other one has to do with Go to Market motions.
And I forgot something important the first time I posted. The link to the post, here it is. 
_URL_ _URL_",Blog,dataengineering,shgm73
"Hi all,
Often see 'how do I get into the field' posts, and whilst they're no doubt useful to some, I seldom see interview advice, learnings etc. Perhaps they just don't appear in my feed, but thought it might be useful to talk about my experiences in broad terms. 
Worth mentioning that I'm a senior DE, GCP certified among a few other certs useful in this space and I'm going for other senior roles which use a broader tech stack and can help me develop. 
Learning _NUMBER_: 
no one knows what they are looking for.
Why do I say this? Well, it seems as though each company has its own definition of what a data engineer does. It could be that in some companies a DE role involves only analytical engineering, whilst in others its pipeline management only and in others its a hybrid dev ops, pipeline and analytics engineer. 
I consider myself to have most of the relevant skills in this space but the conversations I've had with hiring managers  have been so widely varied, that it's worth familiarising yourself with the concepts of dev ops/ infra management/ analytics. 
One company stated in their job spec that they were happy for someone to have an understanding of Kafka and would be trained on the job, whilst in fact wanted a streaming expert. So whilst I had already recognised this an area to develop for myself, I would say that you should be more than familiar with streaming concepts  if streaming is in the job spec. 
Learning _NUMBER_: 
Have some code ready to discuss with your interviewer. 
My recruiter got in touch with a position last week and followed it up a few times. I've had other recruiters do this amazing thing called prep and have had them run through a list of things we'd be doing in the interview. I asked this particular recruiter about this and they replied but just the day before, and told me that they wanted me to go through some code with the hiring manager. Lucky I had something I could share, but I would suggest you have a personal project ready just in case. 
Learning _NUMBER_: 
People are using AWS more than any other cloud. 
Not a problem, just an observation. 
Learning _NUMBER_: 
Some hiring managers are just there to feel good about their _NUMBER_ years experience and shit all over you. 
It's worth being ballsy with these people and start asking them technical questions in return. They may have a solid understanding of architecture but they won't know it all. Just because you don't know the answer to their question doesn't invalidate you. I had to ask a hiring manager WHY the problem they were asking was even designed that way. Of course you're there to evidence your skills but make sure you challenge, even if its just for you. 
Learning _NUMBER_: 
Most of the directors I spoke to are fucking clueless. 
Learning _NUMBER_: 
Make sure you brush up on your basics. 
I've not interviewed for a while and my mind went blank when I was asked about functional programming. It's one of those things one might read over in a document or whatever but commit that shit to memory. Other basic questions I identified as the 'basics' was 
How do you define structured / unstructured data?
What makes a database relational? 
What is OOP? 
What is setverless? 
What is distributed processing? - this came in various forms. 
What is insertfiletype? When would you use this file type? 
How do you describe denormalised data? 
These are questions that at first, I found profoundly tricky answering because I didn't have any nice quick answer for. But after a quick Google for some consolidation and revision , I was able to better summarise. I could certainly tell you about parquet, but is it useful to know that it was created by Apache? Probably not. 
Learning _NUMBER_: 
ALWAYS make sure the company you're going to will support your personal development beyond the scope of your role. 
If they aren't prepared to do this, they are too corporate and bureaucratic and often will never flex for you. As a data engineer, you're in demand and can call the shots. 
And finally, out of the _NUMBER_ I've been on I think _NUMBER_ went well. I've been invited back for _NUMBER_ second rounds already. _NUMBER_ of these interviews were today alone andi think all went really well, so we shall see. I'm not in any rush to leave the company im at, I just wanted to see if I could fly.
Edit: thanks all for the engagement. Just wanted to mention I'm in the UK. I'm sure much of my experience can be transferred to any location but just thought I'd mention as the processes might vary.",Blog,dataengineering,sexcgm
"_URL_
In the long-awaited, WorkflowAsCode function is finally launched in version _NUMBER_.2 as promised, bringing good news to users who need to dynamically create and update workflows in batches.
In addition, the new version also adds the WeCom alarm group chat message push, simplifies the metadata initialization process, and fixes issues that existed in the former version, such as failure of service restart after forced termination, and the failure to add a Hive data source.
# New Function
## WorkflowAsCode
First of all, in terms of new functions, version _NUMBER_.2 released PythonGatewayServer, which is a Workflow-as-code server started in the same way as apiServer and other services.
When PythonGatewayServer is enabled, all Python API requests are sent to PythonGatewayServer. Workflow-as-code lets users create workflows through the Python API, which is great news for users who need to create and update workflows dynamically and in batches. Workflows created with Workflow-as-code can be viewed in the web UI just like other workflows.
The following is a Workflow-as-code test case:
`# Define workflow properties, including name, scheduling period, start time, tenant, etc.with ProcessDefinition as pd:` 
`# Define _NUMBER_ tasks, which are all shell tasks, the required parameters of shell tasks are task name, command information, here are all the shell commands of echo` 
`task_parent = Shell` 
`task_child_one = Shell` 
`task_child_two = Shell` 
`task_union = Shell` 
 `# Define dependencies between tasks` 
`# Here, task_child_one and task_child_two are first declared as a task group through python's list` 
`task_group = ` 
`# Use the set_downstream method to declare the task group task_group as the downstream of task_parent, and declare the upstream through set_upstream` 
`task_parent.set_downstream` 
 `# Use the bit operator << to declare the task_union as the downstream of the task_group, and support declaration through the bit operator >>` 
`task_union << task_group`
When the above code runs, you can see workflow in the web UI as follows:
`--> task_child_one` 
`/ \` 
`task_parent --> --> task_union` 
`\ /` 
`--> task_child_two`
## _NUMBER_ Wecom alarm mode supports group chat message push
In the previous version, the WeChat alarm only supported the message notification; in version _NUMBER_.2, when the user uses the Wecom alarm, it supports pushing the group chat message in the app to the user.
# _NUMBER_ Optimization
## _NUMBER_ Simplified metadata initialization process
When Apache DolphinScheduler is first installed, running create-dolphinscheduler.sh requires a step by step upgrade from the oldest version to the current version. In order to initialize the metadata process more conveniently and quickly, version _NUMBER_.2 allows users to directly install the current version of the database script, which improves the installation speed.
## _NUMBER_ Remove “_NUMBER_”  in complement dates
Removed the “_NUMBER_” day in the complement date to avoid user confusion when the UI date always displays _NUMBER_ when the complement is added.
# _NUMBER_ Bug Fixes
\ fix logger memory leak in worker 
\ Compatible with historical version data source connection information 
\ Memory constraints cause errors when upgrading from _NUMBER_.5 to _NUMBER_.2 
\ Service restart fails after a forced termination 
\ Process definition version create time is wrong 
\ Failed to execute PROCEDURE node 
\ Add default configuration of quartz and zookeeper in common configuration items 
\ In the dependency node, an error is reported when there is an option that does not belong to the current project 
\ Workflow replication error 
\ Workflow is always running when worker sendResult succeeds but the master does not receive error report 
\ H2 in Standalone Server will automatically restart after a few minutes, resulting in abnormal data loss 
\ Error reported when executing MySQL table creation statement 
\ Dependent node retry delay does not work 
\ Failed to add a Hive data source
* Download: _URL_ _URL_
* Release Note: _URL_ _URL_
# _NUMBER_ Thanks
As always, we would like to thank all the contributors  who have worked to polish Apache DolphinScheduler _NUMBER_.2 as a better platform. It is your wisdom and efforts to make it more in line with the needs of users.
_URL_
# The Way to Join
There are many ways to participate and contribute to the DolphinScheduler community, including: 
Documents, translation, Q&A, tests, codes, articles, keynote speeches, etc.
We assume the first PR  to contribute to be simple and should be used to familiarize yourself with the submission process and community collaboration style.
* So the community has compiled the following list of issues suitable for novices: _URL_ _URL_
* List of non-newbie issues: _URL_ _URL_
* How to participate in the contribution: _URL_ _URL_
* Community Official Website 
_URL_ _URL_
* GitHub Code repository: _URL_ _URL_
Your Star for the project is important, don’t hesitate to lighten a Star for Apache DolphinScheduler _URL_ _EMOJI_️",Blog,dataengineering,s23dyo
"Secoda can now be synced with a Git repository, letting you customize how you develop and deploy Secoda and help your team adhere to application development lifecycle best practices. Sync Secoda to a Git repository, so you can manage Secoda workspace as code.
Data teams have been speaking about managing data as code and treating data as a product. Lots of conversations in _NUMBER_ circulated around bringing software development best practices into data. With this new feature, we allow data teams to manage their data catalogue the same way software engineering teams manage products. Although this is a V1 of the feature, we think it's a monumental improvement in the workflow in Secoda. In the future, we're hoping to build the ability to create full Dev/Staging/Production Secoda states to bring the best practices for version control and knowledge management to your data knowledge.
With this approach, data engineers can manage the version history of Secoda as well as manage their own data in their own instance. Teams can start monitoring the changes across Secoda and retract any changes before they impact the master branch. In the future, this will allow data teams to run tests & perform QA on the staging instance while end-users can access the application on the production instance.
Here's the full article on this feature: _URL_ _URL_",Blog,dataengineering,s2dulf
"I wrote a post on how we can leverage open source entity resolution with TigerGraph to solve for AML, KYC, Customer _NUMBER_ and other analytics scenarios. Hope you find it useful.
_URL_ _URL_",Blog,dataengineering,s97w0b
"In recent years, data modeling has seen something of a renaissance within the data landscape. Transformation  is on everyone’s lips, and powerful solutions like dbt _URL_ in combination with leading ingestion tools like Snowplow, have unlocked the power of transforming data to thousands of organizations. Data practitioners spend a lot of time thinking about how best to model their data, the best tools to use and the talent to hire to perform key transformations. 
So what’s driving this new wave of enthusiasm for data modeling, and why is it such an important part of the modern data stack _URL_",Blog,dataengineering,s9ckr3
"Today, Apache DolphinScheduler announced the official release of version _NUMBER_.3. In this version, DingTalk alert plugin adds signature verification and enables data sources to get links from multiple sessions. In addition, _NUMBER_.3 also optimizes cache management, complement time, data source password display in logs, etc., and fixes several key vulnerabilities.
_URL_
For more details, please refer to _URL_ _URL_",Blog,dataengineering,sdtimu
"_URL_
>*Since graduating from the Apache Incubator on March _NUMBER_, _NUMBER_, Apache DolphinScheduler has grown with the community for ten months. With the joint participation of the community, Apache DolphinScheduler has grown into a mature scheduling system product that has been tested in the production environment of hundreds of enterprises after several iterations.* 
*What progress has Apache DolphinScheduler made in nearly a year? Today we’re going to review the changes that have taken place in the Apache DolphinScheduler and its community with this Apache report.*
# Base Data:
**Founded:** _NUMBER_–_NUMBER_–_NUMBER_  
**Chair:** Lidong Dai 
**Reporting schedule:** January, April, July, October 
**Next report date: Wed Jan _NUMBER_ _NUMBER_** 
**Community Health Score :** **_NUMBER_ ** _URL_
# Project Composition:
* There are currently _NUMBER_ committers and _NUMBER_ PMC members in this project.
* The Committer-to-PMC ratio is roughly _NUMBER_:_NUMBER_.
# Community changes, past quarter:
* No new PMC members. Last addition was Calvin Kirs on _NUMBER_–_NUMBER_–_NUMBER_.
* ShunFeng Cai was added as committer on _NUMBER_–_NUMBER_–_NUMBER_
* Zhenxu Ke was added as committer on _NUMBER_–_NUMBER_–_NUMBER_
* Wang Xingjie was added as committer on _NUMBER_–_NUMBER_–_NUMBER_
* Yizhi Wang was added as committer on _NUMBER_–_NUMBER_–_NUMBER_
* Jiajie Zhong was added as committer on _NUMBER_–_NUMBER_–_NUMBER_
# Community Health Metrics:
* Notable mailing list trends _URL_
* Commit activity _URL_
* GitHub PR activity _URL_
* GitHub issues _URL_
* Busiest GitHub issues/PRs _URL_
# Notable mailing list trends:
dev@dolphinscheduler.apache.org mailto:dev@dolphinscheduler.apache.org had a _NUMBER_% increase in traffic in the past quarter :
_URL_
# Commit activity:
_NUMBER_ commits in the past quarter 
_NUMBER_ code contributors in the past quarter 
_URL_
# GitHub PR activity:
_NUMBER_ PRs opened on GitHub, past quarter 
_NUMBER_ PRs closed on GitHub, past quarter 
_URL_
# GitHub issues:
_NUMBER_ issues opened on GitHub, past quarter 
_NUMBER_ issues closed on GitHub, past quarter 
_URL_
# Busiest GitHub issues/PRs:
* dolphinscheduler/pull/_NUMBER_ _URL_ server integrate into worker server*
* dolphinscheduler/pull/_NUMBER_ _URL_ fix snowFlake bug*
* dolphinscheduler/pull/_NUMBER_ _URL_ Recover UT in AlertPluginManagerTest.java \*
* dolphinscheduler/issues/_NUMBER_ _URL_ \ hive sql execute failed*
* dolphinscheduler/pull/_NUMBER_ _URL_ improve install.sh if then statement*
* dolphinscheduler/issues/_NUMBER_ _URL_ \ Failed to create hive datasource using ZooKeeper way in _NUMBER_.1*
* dolphinscheduler/pull/_NUMBER_ _URL_ Auto create workflow while import sql script with specific hint*
* dolphinscheduler/pull/_NUMBER_ _URL_ upgrade the MySQL driver package for building MySQL jdbcUrl*
* dolphinscheduler/pull/_NUMBER_ _URL_ replace node-sass with dart-sass*
* dolphinscheduler/pull/_NUMBER_ _URL_ docker.scarf.sh to track docker user info*
# The Way to Join US
There are many ways to participate and contribute to the DolphinScheduler community, including: 
**Documents, translation, Q&A, tests, codes, articles, keynote speeches, etc.**
We assume the first PR  to contribute to be simple and should be used to familiarize yourself with the submission process and community collaboration style.
So the community has compiled the following list of issues suitable for novices: _URL_ _URL_
List of non-newbie issues: _URL_ _URL_
How to participate in the contribution: _URL_ _URL_
Community Official Website 
_URL_ _URL_
GitHub Code repository: _URL_ _URL_
Your Star for the project is important, don’t hesitate to lighten a Star for Apache DolphinScheduler _EMOJI_️",Blog,dataengineering,s3lm3g
"### GPU Multiprocessing for Parameter Optimization
>**TL;DR:** The code is here _URL_
Hi Guys, 
I published a notebook using a trick for running multiprocessing on GPU devices. 
Might be interesting for some here, so have fun:
As we know, GPU makes everything faster. Moreover, oftentimes the GPU device we use is so powerful that our code doesn't utilize the GPU code to _NUMBER_% potential. In such cases, running multiple GPU instances in parallel can come in handy and save us some extra time.
I've made a notebook that introduces a method for running parallel jobs on GPU devices. It is a bit tricky since most packages that support GPU usage aren't built from the ground up for such use cases.
**The ""trick"" is simple:** We simply do not call the GPU in the main process, the first time we call for any GPU utilization should be from the child processes. 
! _URL_
To demonstrate the power of this concept, I made a notebook that runs Parallel Hyperparameters Optimization on the GPU for LightGBM. As it is assumed, the GPU enables us to train our models faster  and by 
leveraging that in combination with the parallel execution we get to search a large space of parameter configurations for fully maximizing our model's performance .
The Optimization framework is optuna, the popular framework for optimizing the hyperparameters. Again, this is just an example of the approach. You can use the same approach for many other GPU-enabled use cases. 
Personally, This concept helped me over and over as it is easily transferable to many different GPU-based models including Neural Networks , Other GBM models , Feature Engineering , and more.",Blog,dataengineering,s6ruee
Preferably one that sends an email daily or weekly with snippets of info to read on? I’m more inclined to read something send to my email vs visiting something - because I’ll stop or decide I have more important things to do when I’m on the computer,Blog,dataengineering,sdcn59
"_URL_
Recently, TWOS officially announced the approval of _NUMBER_ full members and _NUMBER_ candidate members, Apache DolphinScheduler, a cloud-native distributed big data scheduler, was listed by TWOS.
Apache DolphinScheduler is a new-generation workflow scheduling platform that is distributed and easy to expand. It is committed to “solving the intricate dependencies among big data tasks and visualizing the entire data processing”. Its powerful visual DAG interface greatly improves the user experience and can configure workflow without complex code.
Since it was officially open-sourced in April _NUMBER_, Apache DolphinScheduler  has undergone several architectural evolutions. So far, the relevant open-source codes have accumulated _NUMBER_+ Stars, with _NUMBER_+ experienced code contributors, _NUMBER_+ non-code contributors participating in the project, which includes PMCs and Committers of other Apache top-level projects. The Apache DolphinScheduler open source community continues to grow, and the WeChat user group has reached _NUMBER_+ people, and _NUMBER_+ companies and institutions have adopted Apache DolphinScheduler in their production environment.
# TWOS
At the “_NUMBER_ OSCAR Open Source Industry Conference”, China Academy of Telecommunication Research of MIIT  officially established TWOS. TWOS is composed of open-source projects and open-source communities, which aims to guide the establishment of a healthy, credible, sustainable open source community, and build a communication platform providing a complete set of open source risk monitoring and ecological monitoring services.
To help enterprises reduce the risk of using open source software and promote the establishment of a credible open source ecosystem, CAICT has created a credible open-source standard system, which carries authoritative evaluation on enterprise open source governance capabilities, open-source project compliance, open-source community maturity, open-source tool detection capabilities, Opensource risk management capabilities of commercial products.
After being screened by TWOS evaluation criteria, Apache DolphinScheduler was approved to be a candidate member, which shows its recognition of Apache DolphinScheduler’s way of open-source operation, maturity, and contribution, and encourages the community to keep active.
On September _NUMBER_, _NUMBER_, the first batch of members joined TWOS, including _NUMBER_ full members such as openEuler, openGauss, MindSpore, openLookeng, etc., and _NUMBER_ candidate members like Apache RocketMQ, Dcloud, Fluid, FastReID, etc., with a total of _NUMBER_ members:
_URL_
Only two communities were selected for the second batch of candidate members — Apache DolphinScheduler and PolarDB, an open-source cloud-native ecological distributed database contributed by Alibaba Cloud.
The Apache DolphinScheduler community is very honored to be selected as a candidate member of TWOS, which is an affirmation and incentive for the entire industry to build the community a better place. The community will make persistent efforts and strive to become a full member as soon as possible., and provide more value for China’s open-source ecological construction together, with all the TWOS members!",Blog,dataengineering,s0g7qi
"We use a variety of ""orchestrators"" for our spark jobs including dagster, cron, and airflow. At this point I don't want to be tied to a particular orchestrator but I would like a central view into all jobs. Is there any product that allows us to push metadata  and then query that metadata? I don't need this registry to actually run or schedule any jobs. The Prefect cloud UI is a good example of what I'm looking for but its tied specifically to Prefect.",Help,dataengineering,sa1ur5
"Hello all, 
We are very close to releasing a pipelining tool to open source. Since our team is more oriented towards ml or ds, the tutorial we currently have is ml focused and considered very popular in that space .
Though there are certain data transformations that I can demo using pipeline,I was curious if data engineering community has some hallmark pipeline tutorials or examples that I can implement using our software. 
The ones I found on airflow or prefect  are a bit too simple. 
Thanks for your help. 
Cheers,",Help,dataengineering,s1yj71
"I'm officially a data analyst currently, but I have taken on the role and duties of a data engineer at my job. I use SQL basically every day. I use python all the time. I have work related examples of ETL pipelines I have built entirely on my own. I have taught myself to do all this without college or any mentors / training. I know I can do the work AT LEAST at an entry level capacity. However when I got on zoom for my first technical interview I blanked on everything. Forgetting basic terms and definitions. Rambled on about irrelevant junk without mentioning the important things. The white boarding section was a complete disaster because I apparently rely on the IDE to catch _NUMBER_% of my errors. I wouldn't blame them for denying me based off just that one interview. BUT I CAN DO THE WORK! IM DOING IT RIGHT NOW! I just panicked I guess.
Is there anything I can do to fix this? Should I send them samples of code I have written for work and just try to explain myself? Should I just call it first time jitters and hope it doesn't happen again? Is there anything I can do stop that from happening again? Has anyone else experienced this? Has anyone hired someone who bombs the technical interview? Any advice is welcome.",Help,dataengineering,sa9672
"Coming from a non CS background I'm trying myself to apply best SWE practices to data and I'm even planning on writing as a pet project a small Python-based ETL program. 
Is there any repository that can be used as a example that applies SOLID principles and uses some design pattern? I'm interested in learning how typical design patterns  are implemented. 
I'll be also grateful if someone provided me with guidelines  to continue my search. This is not a homework assignment nor I'm trying to copy anything, just to learn, but if anyone is uncomfortable with providing me real code I will anyway appreciate whatever help he/she may provide.",Help,dataengineering,sdh9n4
"So, I was working on a side project and needed to store nested data structure .
I came across SQL anti-patterns which goes through ways to handle this and came to conclusion that I should be using closure tables.
Are there similar books or resources that go through patterns like this ? 
Basically I am looking for book or resources that goes over complex data modelling solutions like this.",Help,dataengineering,s73s70
"We are ingesting a postgres table with a column formatted like this:

I’m trying to convert this into a snowflake array so we can flatten it etc. Is there a function or easy way to do this without using string functions? 
Feel like I’m missing something obvious here.",Help,dataengineering,s9eqiq
"Is there a place/resource for data engineering specific design patterns? 
Specific problem here: trying to build a system that can launch different versions  of a pipeline at will — mostly for testing of various commits or for releases. These different pipelines would need to spin up associated services and then spin down after completed or upon request in case of failure — curious if a design pattern for this existed already ? — leads me to the general question of resolved for DE design patterns exist?
Thanks!",Help,dataengineering,s1d1pt
Does anyone know what is the common practices to push changes from one environment to the other in Synapse? Specifically SQL script as it's going to get converted into json when it's in github.,Help,dataengineering,s3kerp
"Hi guys I would like to migrate an access file that is located in a hard drive into a web database. Ideally the web database would be simple enough to generate reports and add new records. 
Im familiar with back end programming but less familiar with the front end. What would be a starting point to initiate the migration process? What apps, tools and resources would I need to accomplish this task? 
Thank you so much for your answers!",Help,dataengineering,s4dzem
"I have a batch data pipeline project w/c requires the data on a near realtime  basis. 
This pipeline will move the data from one system to another using REST APIs, with minimal transformation required. 
The volume of the data I'll be processing was very trivial, at the moment we are receiving max _NUMBER_ rows per day w/c was expected to increase for about _NUMBER_ - _NUMBER_ rows per day the next coming months. 
Lastly, I will be storing my data into a backup storage  after uploading it to the target system. 
I am using Python to develop my pipelines, but this is the first time for me to utilize cloud services.
So with that in mind, there are _NUMBER_ solutions which I was thinking: 
I. Use Cloud Functions, Cloud Storage, & Cloud Scheduler stack 
_NUMBER_ GCF _NUMBER_ will request the data from system _NUMBER_ then dump it into GCS 
_NUMBER_ GCF _NUMBER_ will fetch the data from staging GCS then upload it into both system _NUMBER_ and backup GCS 
II. SAME with _NUMBER_ but I will utilize our existing Big Query for backing up the data. 
_NUMBER_ GCF _NUMBER_ will request the data from system _NUMBER_ then dump it into GCS 
_NUMBER_ GCF _NUMBER_ will fetch the data from staging GCS then upload it into both system _NUMBER_ and big query 
III. SAME as _NUMBER_ or _NUMBER_ but replacing the scheduler with an existing Staled Airflow instance. 
I'm also planning to use dbt in the future, if that would affect the solution. 
What I'm aiming with my infrastructure are low cost, and ease of maintainability & monitoring. 
Feel free to suggest another GCloud Service if that can also solve the problem. 
Any advice is very much appreciated.",Help,dataengineering,s25mrp
"Hi, everyone. I've been learning about companies like Confluent, Materialize, Snowflake, Fivetran, Hightouch, et al. I started by learning about data ingestion tools and have been learning more about database software itself now.
I'm non-technical so need help from time to time on topics. One thing I'm wondering about is nodes / clusters. What does this mean? Can someone describe them or link me to a ""picture"" of what a node is?
What does it mean when somebody needs a software to have multi-node availability .
This is a step more technical than I'm used to so this would be super helpful to understand. Thanks so much in advance! I'll upvote every comment as appreciation!
Edit - I should add, I have all day. I will read whatever you suggest, whether it's blog post or textbook.",Help,dataengineering,sgqvm1
"So, some context behind the salary increase. I've been working as a data engineer for about _NUMBER_ months now. It's been a bit of an up and down route so far, and the learning curve for the new technology stack has been steep but really fun . It's been challenging in the sense that every member of the data engineering team, bar me, has moved to other companies, with usually higher paying salaries. Then, I've had to take the little experience I have in the sector and maintain the entire stack, while interviewing and onboarding new data engineers.
Things are now looking like they are stabilising a bit with the new starters getting into the flow of things, and I've had a recent conversation with my manager where she praised me on keeping the team and work going. In addition, she mentioned that sometime next week there will be a meeting put in place to go through giving me a salary raise.
I'm currently on £_NUMBER_ per year base pay, and I've just started being paid for being on call on weekends, to the tune of £_NUMBER_ per day. One of the people who left at the beginning of this month, thankfully felt comfortable enough to talk about his salary here and what he's moving to. He was on £55k while here, but has moved to £70k for a remote job in London. In terms of experience, he had a lot more experience in Python, but almost none in data modelling or SQL.
I started applying for other jobs at the start of this month and am currently due to have some final stage interviews this Tuesday, but I have also had some rejections already. The pay for both of these ranges from £_NUMBER_-70k.
I didn't actually want to move as I enjoy the job, but I also want to be paid at a fair market salary. The reality is that, even if I was get some offers through from these interviews it wouldn't be in time for the meeting that's happening next week.
How would you go about getting the best salary? Are there other ways to use leverage within these situations? Any good resources to read through/watch before the meeting? Should I lean on that I've stuck around ? Any other UK based DEs on here think my salary expectations are realistic?",Help,dataengineering,sesp75
"I’m in the process of building a data processing pipeline where the arrival of files to an S3 bucket results in the publishing of a message to SQS . My message consumer runs on EC2, and is currently a dockerized Python script that reads the message and triggers a processing pipeline made up of several ETL functions. The specific ETL functions performed can vary from file to file depending on some external requirements managed by dynamoDB and are defined in the SQS message, so basically the message says perform operations _NUMBER_, _NUMBER_ and _NUMBER_ in that order. I’ve been looking for a more robust existing framework to better orchestrate my data pipeline however most seem best suited for batch processing jobs . Are there any recommendations for well supported frameworks that would suite this purpose? Switching to a batch operation isn’t ideal, but any ideas for a better architecture would be appreciated.",Help,dataengineering,sf5f94
"Hi, currently I am doing ETL process using synapse Analytics. We are reading delta files from data lake and doing a incremental load on SQL table using spark pool. Jdbc throughout is very low and want to avoid it any suggestions.
Using spark dataframe because transformation are very complex and everything is parameterized.",Help,dataengineering,s2sjhz
"If I intend to migrate a Databricks from Azure to GCP, are there migration tools native to Databricks to perform the migration? Or do the GCP offer tools in this case to perform migration? What components do I need to migrate if I run Datalake on Azure?",Help,dataengineering,s9dkiv
"I'm trying to create a very basic app that uses streaming data and I'd like to create a distributed data lake the cheapest way possible.
Is there any way to create a Hadoop cluster just with the laptops I have around at home  for free?
I've been searching for guides but I couldn't find a guide that explains how to do what I want to do, any source or tutorial recommendations? Any tips on how to start?",Help,dataengineering,s9q2d2
"I’ve recently gotten into playing around with sql after learning some backend. I’m currently loading raw transactions from a blockchain which stores the amount of two assets which will eventually be queried into an ohlc candlestick data set that is to be used on a front end. The timeframe consists of the common ones 5min, 30min, _NUMBER_ hour, and so on. Do i need to specify the chunk intervals explicitly? Looked up on the internet but still don’t understand a thing. I read that by default the chunks are set to _NUMBER_ days, does that mean any transactions from the blockchain that I loaded < _NUMBER_ days will not be shown when queried? Thanks!",Help,dataengineering,sfh5cd
"I'm trying to create an archiving pipeline which essentially does the following:
_NUMBER_. Call stored procedure  from a SQL Azure DB that returns a resultset
_NUMBER_. Archive the result from #_NUMBER_ onto storage account 
_NUMBER_. Extract ID column from #_NUMBER_ into an array of int
_NUMBER_. Use result from #_NUMBER_ as a parameter to call a stored procedure  in the same SQL Azure DB
So far, I've tried the Copy Data activity/tool _URL_ which satisfies steps _NUMBER_ & _NUMBER_. However, I'm not sure how to get the outputs from that step and can't find any documentation at Microsoft.
Is it not the correct usage? Do I have to manually do it instead?
Also, I'd like to do some validation in between steps .
I've managed to try the bare/general stored procedure activity but also can't find where to retrieve its output for use in the next step. I'm pretty new to Data Factory and don't really work with data engineering/piplines so please bear with me.",Help,dataengineering,sd4i63
Is it safe to use hdp over local machine? My pc goes like i5 7th gen 16gig ram...and everytime i login into the vm my processor creates x3 more the noise than ever. Will the use of hdp over long term destroy the soul out of my hardware?,Help,dataengineering,sferxx
"I'm trying to teach myself data engineering, I learn better by doing projects so I thought about doing a rather complex project that involves streaming data and it might be too ambitious for a total noob like myself.
What I'm trying to do is to create an architecture that grabs tweets, news and the prices of several cryptocurrencies and then try to give a forecast of their prices taking the sentiment of the tweets and classified texts as vectors from the news as predicting variables, this forecast gets updated with new predictions in near real time, and I thought something like this, but to be honest I'm not sure whether it makes sense or not.
The architecture I came up with _URL_
I wonder if it would be possible to do forecasting in the same process of doing the sentiment analysis and text classification vectorization with Spark, or if I'd need to store the vectors and the value of the sentiment analysis in a NoSQL database and then doing the forecasting taking the information from this database.
I'd really appreciate any insights or recommendations!",Help,dataengineering,s2kki7
"I have my data engineer _NUMBER_ interview coming up next week, I want to know what should I study for the same as I'm not able to find interview experience for freshers.",Help,dataengineering,s4dybk
"Looking for some input from the community on how you’d approach this problem. We have a Kafka topic that we’d like to start consuming. Ideally I want to just dump all data off the topic into the raw zone in my data lake partitioned by day and then run a spark job to run transformation on the previous days partition. The issue I am finding is the nature of the data that comes over the Kafka topic. Short and long of it, is the topic actually contains many different message types all with different json array structures  so it’s really like many topics worth of messages all sent over one single topic. For transformation I need to extract each message type and it’s payload and deposit it into its own table in the curated zone within my data lake. I could do the message type extraction before dumping into raw, sure but I’d rather just consume straight raw data and then deal with it. My question is how would you deal with this situation? Can I use spark to read a partition of raw data, extract the individual messages and then write them all to dedicated partitions by message type in the next zone? Few more details, I’m not using spark to read directly from Kafka, I use NiFi to ingest, daily volume is well over 100gb and number of records will be over a billion per day.",Help,dataengineering,s4kh1s
"I am unable find answers regarding how does one download data for dashboard building in an automated manner.
Let's say I have want to download data from Twitter everyday at midnight **without having my computer turned on all the time. How would I do it? I want to use a cloud based solution**
I would imagine a hypothetical example as:
A script that downloads Twitter from the API -> Store in S3  -> Stage them into tables -> Load them in a data warehouse -> Output as a dashboard showing some statistics
However, what I do not understand is the step before the script that downloads Twitter data and the hosting of the dashboard. What is exactly making the script download and the dashboard updating?
Sorry for this noob question as I am not a data engineer or CS major by training. I'm still learning this on my own. Look forward for some advice! I hope to build a crypto dashboard",Help,dataengineering,sfk93e
"So I'm a bit of a newb to ADF and just building a pipeline which takes a CSV and upserts it to a table in my database. I have the pipeline working and I've set up error reporting which is working nicely. What I'd also like to do is to capture the changes I've made in another table.
So for example if I append two rows with client refs _NUMBER_ and _NUMBER_ to my table, and change the firstname field for client ref number _NUMBER_ in my table called dbo.client then I'd like there to be another table called dbo.client\_changes which looks like this:
|client\_ref|change date|change made|
|:-|:-|:-|
|_NUMBER_|_NUMBER_/_NUMBER_/_NUMBER_|row added|
|_NUMBER_|_NUMBER_/_NUMBER_/_NUMBER_|row added|
|_NUMBER_|_NUMBER_/_NUMBER_/_NUMBER_|firstname: updated|
I imagine this is quite a common use case. Just wondering if there are any tutorials or anything that people have used successfully in the past.
Thanks.",Help,dataengineering,s7o2p4
"Hey guys,
I currently work as a Data Analyst with a background in Economics and find the engineering side of things very interesting and want to get into DE. I want to learn and have found good roadmaps/resources such as Seattle Data Guy and AwesomeDataEngineering but I needed some advice.
I understand DE is essentially specialized software engineering and I thought that I should start my learning through TheOdinProject . I'm currently going through the program which teaches HTML/CSS, JS and databases. My question was this: am I learning programming in a very roundabout way? in r/learnprogramming many say to start with html/css  and to learn backend after  to learn efficiently. I'm currently on this path, by learning HTML/CSS, JS, making projects and then to learn more in-depth python, data warehousing, distributed systems and so on.
Would I be better off just going straight into learning Python and data-engineering specific things? if you guys have ideas and examples please let me know!
Also, is DE a far-fetch'd career for me given that I only worked as an analyst and have a non-CS undergrad? 
Thanks for your help!",Help,dataengineering,sbbp7r
"here is the rough idea, we have a service that is producing data and want to use a Deep learning-based python script to train on that data and use the trained model to predict some parameters and feed them back to some other service. . We want to create a pipeline to,
_NUMBER_. Run our container-based service on the cloud
_NUMBER_. store all the data centrally
_NUMBER_. train the DL model on the cloud
I would like to know what open source tools would be best suited for such task, any help would be greatly appreciated! Thanks!",Help,dataengineering,s9kwls
"I am looking for training course on Palantir platform
Thanks",Help,dataengineering,s7cz86
"I was wondering if there is a way to feed streaming data from Spark Streaming to TensorFlow, I've read it's possible using Kafka but I don't know if what I thought here, using Kafka for the Spark output, would make much sense:
_URL_
What I want to do is forecasting the cryptocurrencies values for the next _NUMBER_ minutes taking their previous values, sentiment analysis floats  and classified text vectors  as predicting variables and use a recurrent neural network in TensorFlow for forecasting.
What I also thought is sending this data to a Cassandra database or similar but I don't know if I can pull data in streaming from the database using TensorFlow, would it be possible? I'm very new to this so I'm trying to learn as much as possible with this project of mine, thank you!",Help,dataengineering,s4rej7
"I get the general idea of Idempotent dags  but wanted to know if anyone had any tips on some of the extra details below? In this example we are reading data from a table in big query, calculating some additional columns and inserting into a destination table on a daily basis.
**Handling the initial run?**
At the moment, we have specific tasks to constuct the initial table if it doesn't exist. Is there a better way of doing this
**How to handle new calculated columns?**
If we add new calculated columns to our insertion SQL, what's the easiest way to backill en masses? Rewrite the table from scratch or is there a way to schedule previous runs.
**Handling inserts that may change previous data**
Say that an insert calculates values over a partition for a customer dating back, possibly, to the start of the available data. 
The nature of this means that a value calculated for a row maybe be correct at the time of insert, but may be overwritten a month later by another customer interaction. 
What would be the best way of writing an Idempotent dag that could backfill older inserts without overwritting newer ones",Help,dataengineering,savf55
"Hello, 
My company is replacing box with sharepoint and onedrive. Some dashboards are fed with files from Box since box connector on Tableau allows scheduled refresh thanks to Tableau server. 
OneDrive connector only works for private OneDrive, not shared ones. SharePoint connector only supports SharePoint lists, no stored files. 
 
I tried to think out of the box, so these are the solutions that came up to my mind. 
Could you please let me know if there are any limitations that I haven't foreseen? Thanks a lot. 
 
_NUMBER_)Keep the same logic of benefiting of Tableau server by hosting extracts and scheduling their refresh.
What are other common accessible location to access file from Tableau servers? GoogleDrive or Dropbox. 
=>Replace Box by GoogleDrive or Dropbox. 
Cons: none of those cloud solutions are authorized within my company.
Pros: as easy as it used to be with Box. 
_NUMBER_)Create an RPA to migrate files from shared to private OneDrive. 
=>Create PowerAutomate flow to replicate shared OneDrive/SharePoint files into his private OneDrive
use OneDrive Tableau connector to this private OneDrive. 
Cons : only the owner of the private OneDrive can take actions. 
Pros : OneDrive connector supports scheduled refresh. 
_NUMBER_)New logic: hosting extracts on an ETL solution server
=> Use Alteryx + Alteryx server. 
connect Alteryx to SharePoint using the standard input tool if configure permissions correctly on SharePoint and get the UNC path to SharePoint. 
Then in Alteryx generate a .hyper or .tde output. 
Alteryx server allows to schedule the load of data to Tableau. Upload the workflow to the Server gallery, then schedule it accordingly to requirements. 
Pros: Alteryx & Alteryx server are authorized within my company. 
Cons: requires more technical skills + license cost . 
_NUMBER_)New logic: use data virtualization thanks to Denodo -> no need of file storage. 
=> There is a Tableau connector dedicated to Denodo. 
But as with Alteryx there is a need to first register Denodo in Sharepoint. 
Pros: saves the issue of data storage. 
Cons: requires more technical skills + license cost . 
 
_NUMBER_)Other options:
=>Using Tableau Prep and create an RP. 
Same connector issues as Tableau . 
Automating this would require Tableau Conductor which is not authorized within my company.
Could automate it with an RPA , but that would still mean the data storage would be supported by a collection of .hyper or .csv files in a folder. This is not compliant with data strategy within my company. 
Many thanks in advance",Help,dataengineering,s978wv
I have use case for bigdata processing problem. I have an ETL that runs data deliveries. There are some tasks which generate derived tables over tables included in the delivery. Currently the script for the tasks is designed such that it requires loading all csv files in memory for processing. That often causes laptop to run out of memory. This Airflow DAG is currently hosted on local machine. Prime reason being frequent troubleshooting required during every DAG run. Would it be feasible deploying the dag on GCP cloud composer? because I need to troubleshoot it often. Make code changes to accommodate data delivery logic. How can I maximize processing and minimize time?,Help,dataengineering,shkth6
"I'm currently in healthcare and want to switch to DE . I have been conflicted in the best course of action because I do not know if I actually need a new _NUMBER_ year bachelor degree, masters or if people actually find profitable work with only a bootcamp under their belt. 
For transparency, I have two _NUMBER_ year diplomas in Research and Medical Laboratory Sciences so I am adept at the statistical side and fully ignorant of the computer side. I make $_NUMBER_ an hour in my current role. I am taking a quick little python course atm and absolutely loving it.",Help,dataengineering,sfbtdn
"In the past, I had just rolled my own, but curious what tools people use today to gather data from APIs  on an ongoing basis? I'm thinking: 
Scheduling - Prefect 
Scraping - Custom python script 
Persistence - PostgreSQL
For people that have dealt with API scraping on an ongoing basis, what are some challenges you faced & what other considerations are worth keeping in mind? 
One issue that comes to mind, sometimes with the pagination there are duplicate records. Is it worth just getting everything into some table incrementally with bulk uploads, and then having another process that would clean and deduplicate?  Or try to deduplicate in the scraping script before saving to the DB?",Help,dataengineering,s3h9yk
"I have a new Windows server, clean slate. Got a boot drive and a partition for installs. 
I have no working examples around me to refer to and I've never personally seen this done or worked on a team. That's the very brief context. 
___
My intention is to install python and use Git for version control. For now, I just want to write some scripts to pull data from a vendor's database and store it. Plain and simple. 
Do I need to install Git on the server along with Python and the repo, sync to a private Github and do all my work from VS Code on my local desktop? How far off am I?
I'll make this more sophisticated & work in validation / pipeline management tools as I learn, in case anyone's wondering. Trying to keep this bite size as I learn the ropes.",Help,dataengineering,s370dl
I've written a new project recently which focuses around these _NUMBER_ main things. I want to compare my project to that of one which utilises these tools. I'm particularly interested in the project structure so a github repo would be perfect. Does anyone have any great examples?,Help,dataengineering,s21wyj
"I am working in an indian IT company  and I have been assigned an Informatica Developer role. I wanted to know if there is a future for this technology, I dont see any coding here, only a bit of SQL for validating the data. 
I have seen some data engineering topics related to informatica on the internet. I want to know what other things i can do to make the most of this opportunity. 
I also love coding and pretty much like to switch jobs if this technology is not the future.
One more thing I am worried about if I want to switch to a coding job, whether they will consider my profile seeing my informatica experience since it didn't involve any coding. Idk I am just confused and worried. Any help appreciated. 
Thanks.",Help,dataengineering,s5yzn9
"Is it possible to store variables in a separate YML file in one of your models? For example, if I have a model calls ""test\_model\_1"", can I create a YML file called ""test\_yml\_1.yml"" with variables to be used in any sql file in that model? IF so, how do I call those variables from the sql file?
**\** I should have specified... I meant *separate from* the project.yml file. I don't want my project file being littered up with a bunch of variables that will only be utilized by one model.",Help,dataengineering,s3u58s
"**\*\*EDIT --> Size of data is _NUMBER_ TB\*\***
**I am writing a job In AWS Glue that should :**
* reads **_NUMBER_ TB across** **_NUMBER_ Million XML** files, each about _NUMBER_ KB. Historical data, won’t change or be added to)
* currently using Dynamic Frames, a call to resolve choice to deal with uncertain level of schema inconsistencies  among these files
* finally writes the data as parquet to an output bucket using some logical partitions based on fields in the data.
* The primary goal of this pipeline is to compact the data and provide useful partitions so that downstream research questions and ETL jobs will be significantly less costly to run.
* Further, we'd like to make as little assumptions as possible about the contents of the data at this stage and minimize data loss by, for example, imposing a particular schema on the data upon reading
* Additional context, I wrote this with PySpark, but am comfortable with Scala, so would be fine implementing in Scala if people think this is worth it.
The job runs fine on a sample of the data  to a test bucket, but the bottleneck running on the full raw data  is, unsurprisingly, in the initial steps of the job where the driver is forced to list all the files from the input source.
I’ve implemented some features that are supposed to be designed for this issue, like
* Job bookmark (_URL_ _URL_
* Bounded Execution (_URL_ _URL_
* Input File Grouping (_URL_ _URL_
* others considerations for memory management (_URL_ _URL_
But it seems there is no way to get around listing these files, and the associated pressure placed on the driver. Also complicating this is that the data has unhelpful partitioning. Basically, the data is severely over-partitioned in a pattern that follows:
* s3://bucket/**ENTITY-NUMBER**/
 * s3://bucket/ABCNEWS\*\*_NUMBER_\*\*/
 * each sub directory will contain about _NUMBER_-_NUMBER_ small XML files 
* There are likely hundreds of thousands if not one’s of millions of these subdirectories.
I’m sure others have faced this same issue, so here’s what I’ve gathered as potential solutions:
* Given a list of the distinct ENTITY values, selectively copy files from the original raw bucket into a staging bucket using a more helpful s3://bucket/ENTITY/ pattern partitioning, and then run the Glue job in batches, pointing at one ENTITY partition at a time
 * not sure if this will help as the performance deterioration on files seems to occur even at the _NUMBER_'s or _NUMBER_'s of thousands of 40KB files
 * I’ve tried using the AWS CLI sync command using AWS Cloud Shell but found that, as many have cited, there are issues here when moving tens of thousands of files
* Use AWS S3 Inventory to create a manifest of all the files, and then batch these into discrete Glue Jobs
 * Not sure how I would point Glue to a manifest of file names rather than an S3 bucket
I appreciate any feedback on these or any potential solutions here!
Thanks",Help,dataengineering,s9shwo
Has anybody implement data lineage project using Apache atlas ? Any link of git repo would be helpful?,Help,dataengineering,sdsccz
"Hi all,
I am working on Dataflow jobs developed in Apache Beam , but I think the same question applies to all MPP frameworks/engines.
From my understanding, when a step/task fails with an unhandled error, the input element is processed again and again until there is no error, which happens either when the error is temporary  or when the pipeline is updated to fix the error.
The pattern I am seeing is that when none of the two above cases happen, the input elements causing the errors are retried forever and overcharge the pipeline, causing autoscaling and a lot of wasted resources.
What is the way you handle such situations? Of course known errors are handled, but if a specific unhandled error happens, we have a problem. I also assume a big try/except is not the solution, since this would not raise temporary errors ",Help,dataengineering,s9cohw
"Hello,
I'm a BI analyst currently that recently accepted a DE position to work with the DS and ML people in our org, thanks to this sub and the Seattle guy on youtube. I started preparing around Spring last year. I officially start next month after my vacation.
The thing is, there's no other DE in this org. However, there are DS/ML people that did work on some DAGs but it's not their full-time job so they don't have a lot of time to debug and maintain their DAGs. We also have a full DE team on a different org but they're a little bit short on resources at the moment and too busy working on other things. They're all mostly pretty experienced and working on bigger things. Since I'm already in the same org with the DS/ML people, it was more manageable to get a position here as a DE since I've been here awhile. But I don't know if I am in over my head. I've basically been tasked with fixing two of the current DAGs with no timeline since they know I'm new and still need time to learn the current DAGs. But as a junior DE, I'm not sure what's the appropriate timeline to be able to finish it. I've also been tasked with building _NUMBER_ DAG next quarter and it seems like I have the whole quarter to do it. Does this seem reasonable?
I'm planning on spending the weekend to learn our libraries, connections and whatnot. I'm excited with the opportunity but I don't know if I'm in over my head since there is no senior DE to mentor me. The senior DS/ML people that worked on DAGs are nice enough to offer help but they are not DE. Alternatively, there are also tons of existing DAGs that the DE team has built and I'm able to take a look at, review and possibly learn but it will be all mostly on my own. Will I be ok? Are DAGs simple enough for a jr. DE to build/learn with the above resources and no senior DE?",Help,dataengineering,s47dtz
"So I am actually in finance but would like some insight on a report I built based on OLAP pivot. It was working great the first few runs then I started getting login credential authentication prompts when attempt to refresh the report. 
When I enter the password , it won’t take the password and eventually it will log me out of analytical services if I try to enter the credentials too many times. Initially I was thinking this was a problem of not having permission accessing the cube but when I tried to reconnect and set up the data table in a new file, I had no problem with logins.. could this due to how the permissions was set up? Only allow access to certain dimension?
And when I save the file and reopen again, it will keep asking me to put in the password again ughhh.
Any suggestions of what to do? I am trying to help the admin of the database to understand the problem but he seem to think this was a issue of entering the wrong passwordwhich is not the case.
I also share this user login with one of my other colleaguesnot sure if multiple people using the same login account will create a problem as well.
Thanks so much in advance!",Help,dataengineering,shjvxa
"I created a small project _URL_ on my local machine where I query an API, transform the data, and load it into different tables on an RDS Postgres database. Currently, this process uses a bash script to run activate the virtual environment and run each Python script in sequence, while writing errors to a log file. Which cloud product is most appropriate to host/schedule this process to be run daily? Ideally, I'd use AWS since I'm already hosting my database there, but happy to hear other options too. Thanks for any advice you can offer!",Help,dataengineering,se2tef
"We've trialled schemachange](_URL_ and [liquibase _URL_ which are change script based tools. We've ruled out a whole load of other tools that are either change script based tools or don't support Snowflake, including the following:
* Flyway
* SchemaChange
* Atlas
* Devart dbForge
* ApexSQL
* Datanamic
* DBArtisan
* DBComparer
* OpenDBDiff
* SQLDelta
* DB Deploy
Are there other state based change management tools we haven't considered for Snowflake? We started off with schemachange, but this is proving tricky to use as our customers keeps changing the priority on what we need to work on. 
By state based, I mean the tool can work out the delta of what objects are on an environment to work out what changes need to be applied.",Help,dataengineering,sfimjd
"Hi everyone,
I've recently been promoted to a Data Engineering position at work. That being said, my first project is helping migrate data from SAP ECC to SQL Server and solidify our data pipeline so my Analytics team can extract data in a more streamlined way for our dashboards and modeling. 
 
I don't have much guidance from technical leadership or access to technical expertise in this undertaking, and I wanted to see if there were any Sr. DE's that had common ""rookie"" mistakes they've seen in similar initiatives that I should look out for. 
Any insights are appreciated.",Help,dataengineering,schltg
"I'm working on a pet project and I've settled on using Cloud Composer to manage my ETL tasks. A main part of this is data scraping. I'm conflicted between two approaches:
_NUMBER_. Have the web scraping calls come from the pipelines themselves, to take advantage of the error handling and logging that comes with Cloud Composer/Apache Airflow.
_NUMBER_. Have standalone web scraping instances which listen to a message queue and scrape based on incoming messages. This way the system becomes more distributed and there's less risk for getting clogged by a surge in requests.
Have any of you set anything up like this? Any advice for which way I should go?",Help,dataengineering,s8oxj3
"Hi guys the data that I am working on is in a shitty state, and i have to transform it more consumable form. Is there a way to convert lat and long to city directly on BQ or I should write cron job in python to transform the data?",Help,dataengineering,s0eoiu
"Total noob question, hope this is an ok place for it.
Say I have multiple vendors. Some use Oracle, some use MS SQL, some use postgresql etc.
Can I stick to one flavor of SQL for my organization, or do they all need to be written depending on which DB I'm connecting to?
T-SQL for this one, PL/SQL for that one etc. Or can I write all the SQL for everything in T-SQL, for example?
The basis of my question is keeping things as simple as possible. Couldn't find my answer on Google, probably not asking it very well.",Help,dataengineering,sbyua0
"Hi folks. I’m very new to dbt and Fivetran. I have been doing data engineering for past _NUMBER_ years but most of my experience is in building data pipelines using legacy etl tools like Informatica and TSQL. 
Recently I completed basic tutorials on dbt. I was wondering if there are any public project that I can look into to get more hands on experience in dbt. Also, where and how do I start with Fivetran?
Appreciate if anyone can guide me in the right direction.
Happy Monday!",Help,dataengineering,sbpb8d
"Hi,
How do people get around the fact that fivetran cannot read data from BigQuery or other bigdata sources? Am I missing something?",Help,dataengineering,s1r3xs
"Hi! I am a Data Engineer & Analyst at a non profit organization. I'm pretty much the only person with an engineering/coding background and since I don't really have any guidance in my organization I was wondering what I could improve in the implementation I've built with our data warehouse and pipelines.
The setup is a BigQuery DW with some Cloud Functions running Python pipelines to feed data into it. We currently only have a few pipelines setup pulling data from a Postgresql DB, where we just pull the all the data we want from the Postgres DB table, truncate the previous version and insert all the data again
This was done because the Postgres tables had no reliable timestamps to handle only inserting deltas. Please advise if there is any other cleaner way to do this! 
We also have some more tables in there where data is inserted manually every quarter since we get a big file in a very dirty format, so I came up with a script to clean it but still manually upload it.
 We are also considering building another one to get more data from a partner's Redshift instance. 
PLEASE NOTE that no modeling was performed at all for this data warehouse, I just figured which tables and columns would be useful for our analytics and pull all these data into BigQuery to feed our Data Studio reports. 
There are also no tests being performed in the pipelines and even though I have a git repository for the pipelines, if I want to change the code for the Cloud Function I have to go in there and manually change it for each.
I am just looking for some general guidance on ways to improve this implementation since I feel a bit lost on what the best next steps would be. 
Thank you!",Help,dataengineering,sacaqc
"I need to extract data from Amazon Ads and DSP campaigns. The business has the reports in Amazon platform, but they would like to create more in detail reports and cross compare, so I need to take the underlying data of said reports or, at least, download and extract the data from the reports.
We are trying to load this into an Azure Data Lake, so the tech stack is Azure based .
Does anyone have any experience with this / these API?
Link to API doc below, I am reading it, but it's slow . Also, Google did not help - there are articles advertising 3rd party solutions of extracting data, so I know it is doable, at least.
APIs Documentation _URL_
le: added tech stack info",Help,dataengineering,s2tim7
"Hey guys,
I am trying to ingest data from eloqua uing rest api. Problem is rest api has a limit of _NUMBER_ million records per response and my pipeline is failing due to this.
Anyone has any suggestions or ideas to handle this scenario in adf?",Help,dataengineering,s8gg4f
We are migrating away from the old way we did data engineering to Databricks on Azure. We have various Databricks instances in our company that we need to get specific data from . What's the best way  to do this? Using ADF makes us groan because our ETL process is dynamic and we need to write code to handle these dynamic requests. Many thanks for any input provided.,Help,dataengineering,s3d7kp
"Background: So our data pipelines load data, update history, enrich it and then we come to transformations before actually generating the reporting table.
The data we have is revenue data coming from multiple sources mostly apis. In transformations, one transformation is to identify the type of revenue which requires manual input from another team.
So the manual process goes like this:
- takes a set of unidentified transaction rows .
- puts them in a separate table.
- this table is shared with other team who manually set the type column  of each unidentified row.
- then this table along with original table are joined together to generate reports making sure there are no duplicates and type of unidentified rows are now defined.
Ideally I know identification should be automated, which is already under development but not fully in place. So the manual process of identifying transaction cannot be replaced at the moment.
Question:
- Is it the most optimal way to do things?
- If you do something like this, do u have better suggestions?
- In business analytics, what search term should I use for this reading about this kind of business requirement.
Thanks for all the help.",Help,dataengineering,sc9iic
"I'm analyzing data extracted from an API in the form of JSON files in Jupyter Notebook.
What I'd like to do is dump all the JSON files the API can access into a data lake or warehouse and then write simpler SQL to access what I need when I need it.
Any recommendations on free/cheap data warehousing / data lake solutions that could do the job?
All I can think of is BigQuery, Databricks and maybe AWS?
Thnks and apologies for the newbie question.",Help,dataengineering,se180s
"I want to get familiar with Delta Lakes but everything I've seen on YouTube are high level overviews. There are no courses on Udemy or anything either.
I also have the book Learning Spark Lightning-Fast Data Analytics, but there isn't much on Delta Lakes in it.
I'm a person that enjoys learning with tutorials and books and I wonder if you have any recommendations or places to look for more learning resources.",Help,dataengineering,s734oe
"Hey I am wondering if anyone has done this before. 
At work we have to integrate data in azure blob storage with a 3rd party application that consultants are using. 
Originally I had thought of giving them specific access to the resources and operations  with a SAS . 
Since those can expire what is the best way to get them that code regularly in a secure manner? 
Or should I rethink the way to do this. 
I have thought about having them generate the SAS token on their end but you need the Account Key to do that and I know know if I want to do that. 
Any thoughts are good thanks!",Help,dataengineering,s1ib2j
"Hello!
I am a finance student in their last year of uni who has recently fell in love with the idea of data engineering.
In the last _NUMBER_-_NUMBER_ months I’ve started working on SQL , I’ve done some data visualizations with Tableau , I’ve done most of the easy SQL stuff on HackerRank and I just started with leetcode.
I’m at the point where I’d really like to start learning Python, but I keep feeling that my SQL skills just aren’t good enough to move on? Like I cannot justify moving forward because I feel like I haven’t gotten to a point where it’s justifiable for me to say “okay, next step”. Has anyone been in this position? 
Should I just move forward or are there any standalone SQL projects related to data engineering that when I would have completed would really help me solidify in my mind that I am ready to move forward and give me that confidence?
Any help is appreciated :)",Help,dataengineering,sf9yz2
"I have a few sources of patent data  that I would like to extract and process with sql and python. Is my best option to just use a notebook to do all of this? I would prefer to use GCP,  for the processing part, but I'm not really sure how to start. If anyone has any advice it would be greatly appreciated!",Help,dataengineering,s9pd25
"Wondering if those with more experience with Spark can help. Can pyspark parse a file that has multiple json objects each with its own schema? The JSON schemas are the same at the highest level  but then each individual object has nested arrays that differ in schema contained with in fields and tags. Is this structure something that can be parsed and flattened with pyspark? I having trouble getting anything to work due to the presence of differing schemas within a single file that is read into spark. Example structure below:
[ {
 ""fields"" : ,
 ""name"" : ""mem"",
 ""tags"" : ,
 ""timestamp"" : _NUMBER_
},
{
 ""fields"" : ,
 ""name"" : ""net"",
 ""tags"" : ,
 ""timestamp"" : _NUMBER_
}}",Help,dataengineering,s6bqyh
"Hello folks, looking to the seniors of this industry or anyone from a similar international background for some advice on my path.
From a SEA country, got my bachelor's in actuarial/Stats in the US, lost OPT job when COVID struck. Have since been working in my home country. I've held _NUMBER_ relevant roles: data analyst/consultant  and data scientist .
The data field as a whole is really new where I'm at, and both the positions have been at startups so I've been the only person on the team, so I've had the chance to wear the ""full stack"" data guy hat, so to speak. Engineering wise, I've helped build out some clients pipelines using GCP and services like Stitch, Fivetran. At my current role I built the full analytics pipeline  off their production NoSQL database.
Most of the work I've done is in python scripts and sql, with the orchestration being achieved via airflow/Cron jobs and the like.
I've really fallen in love with the engineering side of data over the heavy math and stats side of things, so right now I'd like to move overseas since the field isn't established here . I loved my time in the US and really felt at home there, so I would like to return there.
I know it's going to be tough, and I'm somewhat familiar with the difficulties of getting a H1B.
I guess my question is, what's the outlook in this particular field for people like me? Are companies considering international applicants? Non CS background seems to be a big hurdle in the US. What would be my best option to break into the field? Do I really need to invest in a Masters and hope to land something off of OPT?
Other international applicants/seniors familiar with overseas visa situations: how'd you guys do it? What can I do with my experience? What skills do companies look for as a bare minimum to consider foreign applicants?
TLDR: how can I break into the US/EU market as an applicant from a SEA country?",Help,dataengineering,s105u5
"Hey guys,
I'm a non tech guy preparing myself to switch into data engineering. I have gone through a few courses to educate myself and shortlisted one which I feel is good. I just wanted you guy's opinion who are already in the industry to advice whether the topics covered in the course are good in terms of current market demands? Would it be sufficient to grow in this field? If not any other course you would recommend ?
Would really appreciate any help here. I have provided the link below.
_URL_ _URL_",Help,dataengineering,sd3626
"After ""learning"" SQL and Python, what comes next in data engineering for someone who is completely new to the domain?
If say I want to build some sort of ETL pipeline to put on my portfolio, what are the tools that I would need and in what order is it best to learn these?
I keep hearing about Apache, redshift, docker, and so many more and I understand that it would be good to have all of these tools but where do I even start? Is there a certain order/workflow that these tools come into play? 
Ideally I would like to know what I would need as the fundamentals of an ETL pipeline and go from there.
Thanks so much",Help,dataengineering,sfzs2e
"I'm in the process of building a project to improve my DE skills, wanting to do something more complex/real-life scenario than I usually do.
Long story short, I'll be working with chess data. I'd like to ETL GM games details and build an analytics dashboard plus, if I've got time, a predictive model 
I'm fetching all data from a REST API serving JSON. Here is what I came up with. I'm working on-premises to limit costs.
_NUMBER_. Fetch some existing data from players' monthly archives. Parse them with Spark , store them into a Cassandra ""staging"" cluster
_NUMBER_. When this works, automate the retrieval of players' daily archive and add it to Cassandra's staging 
_NUMBER_. Schedule a spark job to daily treat new data and update my ""BI"" data
_NUMBER_. Build a ""BI"" dashboard with Tableau or Looker
_NUMBER_. Dockerise stuff
For step _NUMBER_, a lot of data will be fetched at the beginning . Would you store all data in an RDD  then push it to Cassandra? Or would it be more efficient to treat JSON lines one by one, parsing and storing them to DB?
I chose Cassandra because my queries would be pretty much the same every time. Hadoop feels overcomplicated here.
For steps _NUMBER_ & _NUMBER_, should I use Airflow to schedule a spark submit ? or Kafka ? or something else?
Step _NUMBER_, these _NUMBER_ tools seem to be the most used according to job offers. Good choice? Any other suggestions?
Something along these lines:
_URL_
 I'm pretty new and self-learning DE field. Is it somewhat correct? Or completely wrong? Any advice, optimization, DB archi must-do, tool recommendation, caveats ? Any constructive critiques are welcome, thanks for helping out :)",Help,dataengineering,sg5ttq
"I need to ingest highly sensitive data from file shares and a database using Azure Synapse or ADF and land it into Snowflake. I am not familiar with masking in Snowflake but believe that the masking/encryption needs to happen during the ingest process. That is, already masked data lands in Snowflake.
What is the best method to accomplish this? Is this the recommended method? Are there any tools/common design patterns? 
The data will need to be consumed from Snowflake by a team of Analysts/Data Scientists that have clearance to work with the data.",Help,dataengineering,sdpa0c
"I'm struggling a little with building a dimensional model around an accumulating snapshot fact table. I'm modeling a process for project timelines so the fact table would have start\_date, funding\_date, end\_date, etc. and the main dimension table would be project details, which would include information like title. The problem is a project only goes through this process one time so the fact and dimension table would always have a _NUMBER_:_NUMBER_ relationship which feels wrong. 
What would be best practice?",Help,dataengineering,s28kon
"Hi, beginner in data engineering here.
We're actually using a slow  and not very reliable  coded in Golang. Since a few weeks, we've been trying Apache Spark. It does work but this fixes only the ""transform"" part of our pipeline.
Here is an image of the total pipeline: _URL_ _URL_
As I said, this whole pipeline is in Go. We would like to use real data engineering/big data tools for this.
So, to recap, we need:
_NUMBER_. A tool to monitor multiple buckets and filter files on them 
_NUMBER_. Transform our files 
_NUMBER_. Run multiple aggregations pipeline thought MongoDB.
For example, if we decide to use only Apache Spark, then our pipeline will start from Go, then call Spark, then wait  for Spark jobs to finish, then call MongoDB pipeline from Go. This is not very practical and that does not follow any ""good practice"" in data engineering and management.
Thank you!
TL;DR: We are searching for the right tools for our pipeline , transform columns and rows, then insert all in MongoDB to run aggregations. Also, the most important, is how should we connect each part of our workflow?",Help,dataengineering,s9e3hg
"Hi all,
I work on a small startup that is based in Europe. Our only data infrastructure so far is our transactional database, but we already have big enough volumes of data to analyze and we need a proper data warehouse. I am more of a data analyst, but I need to set this one up myself; please consider this as a disclaimer, I'm not at all experienced in the field.
Our situation is as follows:
We have an S3 bucket where, for every transaction, a json file is stored with all the relevant information, which includes sensitive data such as the customer's name, phone number, email, nationality etc. For analytic purposes, some of the sensitive fields are not useful and can be removed, while others are useful but can be anonymized.
What is the best practice to do that?
I was thinking of an additional step in the process whereby a lambda function would be triggered everytime a json file is created in our bucket. The lambda would create a new json file with unnecessary fields removed and sensitive fields hashed, then it would store it in a new bucket. So all the processed data would live in a new location permanently while the original raw data would be erased periodically.
An alternative would be to perform the anonymization right on bigquery. But then, I would still need to remove the raw files from S3 , so I'd lose all this data.
What do you think is the best approach, and what kind of legal considerations should I have in mind?
Please help an overwhelmed non-engineer. Many thanks in advance.",Help,dataengineering,s8f21z
"I need to relate tables in my Postgres data warehouse for effective querying, but foreign key constraints inevitably make ETL a nightmare. What is your solution to this seemingly common problem?",Help,dataengineering,sao8io
"Is Airflow a good option for monitoring jobs  and tracking dependencies?
We have a bunch of different tools running ETL jobs and and no  way to track dependencies. Looking at the Airflow docs _URL_ it supports all  of our tools and it includes a SAMBA provider so I would guess you can just point to a file on a file share.",Help,dataengineering,sd1yli
"Hi Team,
I would like to run ML jobs in the following situation. In my home I have a powerful PC that has a GPU that satisfy my needs but sometimes I need to scale for training multiple models. I would like to setup an orchestration configuration that uses my machine but when needed ,on demand, it requested an ECS AWS machine , process my ML model and then brings the machine offline. I was hoping that something like Airflow could do it, but I can't find any resources how this would work. 
Does anyone have any tips on how to do this? Or I should use other tools? 
Thanks!",Help,dataengineering,sd61vz
"Hi,I am currently working in a WITCH company in DataStage Development support role. Although i am in the development team, Error fixing, incident handling is most of the Job, little to no chance of any development.
I have an offer from a mid level startup for Data Analyst role  with more than _NUMBER_% hike.
My aim is to be in a data engineer role and i am trying to learn and create projects on Spark, Kafka etc tools for building my skills.
Questions i require guidance on:
_NUMBER_. Current company has bond  and _NUMBER_ months notice period, with both of these, new company has no problems and will even look to buyout.Considering i have been rejected by multiple companies on _NUMBER_ months notice period constraint, should i switch and keep preparing my skills for internal switch to a data engineer position or to move out after a year to more relevant position.
_NUMBER_. Is my current role more relevant for a data engineer position or both the jobs will have no relevance for a data engineer position.
This is my first full time job after internship and i would not like to waste any company's resources and time.Any guidance will be highly helpful to me!!
Thanks",Help,dataengineering,s7pwum
"I have worked with BI in the past, but mostly in the analytics part: querying stuff from a data warehouse, generating analysis and such. So I know the very basics of databases and I know basically how PostgreSQL works.
Now I'm in a company that has zero infrastructure regarding a data warehouse. People run stuff on their own computer and store it in Excel and CSV on cloud.
I want to build a data warehouse from scratch, beginning with getting a VM to run procedures and run the database there, but **I know nothing about this part. In fact I know so little regarding the technical part here that I don't even know what to search for and can't even elaborate further on what resources I need you to point me to. Someone asked me if I wanted to run a cycle server or not and I don't even know if it's yes or no.**
**Can anyone point me to good resources to learn the technical part that goes around building a DW?**",Help,dataengineering,s85ix8
"I'd like to scrape a lot of data on comments from an API. The way the API works is a little funky and sometimes when paginating may encounter duplicates. What would be a good what to structure this kind of data pipeline? Each day there are anywhere from _NUMBER_-20k new comments. Here is my idea so far:
Scheduling: Prefect
Data API crawling: Custom python script to extract comments and topics from the API
Data storage: PostgeSQL
Each API call can only return a few hundred results at a time, so was planning to just do batched inserts from the python scripts directly to the PostgreSQL DB. I want to parallelize the scraping, so I guess I might just allow duplicates in an initial, raw table then worry about clean up in a secondary table. 
Questions:
* Have you done something similar? How did you approach it?
* Does going straight from API -> Postgres make sense? Should I store the raw results to S3 first? 
* Would dbt help? 
* Later on I would like to be able to do some NLP  and I think I'll need to use scikit learn / Spark. Is there another format of storage that would be better to support that? For me the NLP is an add-on that doesn't need to be ""live"". Otherwise, I am thinking I would simply schedule some kind of dump from Postgres -> S3 for Spark .
Thanks for your help!",Help,dataengineering,s3668b
"Hi,
I have a bunch of scripts that run benchmarks and pipe it into local text files. I would like to publish these text files with ngnix to prometheus . I installed nginx _NUMBER_ but I cant seem to be able to 
_NUMBER_. modify the port
_NUMBER_. Set the folder to the logs folder and make it scrape any logs.
The logs themselves will be changed every _NUMBER_ mins or so.
What do you think? Is there a project/guide I can use?",Help,dataengineering,s8p2o2
"Hello, it's my first day of internship and I was tasked to make a message bus infrastructure that can be used in connecting AWS applications. The problem is I'm new to all of this including AWS. I do not have any background at all. My managers didn't explain anything, they just told me to construct it.
I have researched the basic concepts of a message bus and what it does and read that it forms a service-oriented architecture. Could you give me some resources that can help me make the infrastructure? I checked Enterprise Integration Patterns but did not find what I was looking for.",Help,dataengineering,s65yea
"background: company moving from Kubeflow on kubernetes cluster on AWS to Azure and Synapse . 
We have data landing on S3/ADLS in parquet, it needs to be checked, processed, turned in useful data, and made accessible to users via python and SQL.
We work with the bronze - silver - gold principle. Raw data lands on the datalake, gets checked, put into silver, flat tables are made so they can be used for dashboarding and modeling in gold.
Data lands as parquet, silver and gold will be delta lake  to allow for schema changes over time.
Data will be landing in batch, daily, usually in the early morning. there's about _NUMBER_ distinct tables/files coming into RAW . 
These need to be checked one by one, and copied to silver  in delta lake format. The gold tables generally take multiple inputs before they can be made. Usually the order and timing of input tables is quite stable, but there have been many instances where it wasn't.
set-up:
we want to use Delta lake as storage, spark clusters as compute, synapse notebooks for code, and pyspark and sparkSQL as languages. 
We currently have much of our logic in .py files with functions, and .sql files that take arguments . 
we want to register tables in a sparkDB in synapse, but will perhaps use serverless sql pool to have data accessible via ODBC .
We'll develop code and pipelines in a LAB Synapse instance, and push to a Factory synapse instance via Azure DevOps.
the question:
What is a best practice to orchestrate all these pipelines, so they can run when their data is available? The first step should be easy, just create a file trigger to launch the pipeline  when a data file lands. 
Subsequent pipelines that take only one input can be triggered by the finishing of the previous pipeline, or we could do this as a second step in the same pipeline. I prefer to separate these pipelines to make it easier to correct things and rerun pipelines. 
This will probably mean that for _NUMBER_ tables and a load more finished tables in gold, there will be anywhere from _NUMBER_ to _NUMBER_ distinct pipelines. There will only be a few categories/kinds of pipelines , but each with distinct logic tailored to the data in it. 
Currently, we have general kubeflow pipelines that run distinct code based on arguments. Eg the compute-data pipeline is generic, and reads all new files in Bronze. it finds foo.parquet, so runs with the foo argument, reads the foo.py _URL_ logic file, and executes the foo.sql file. If it then finds the bar.parquet file, same thing but for bar. Rinse and repeat.
Is there a way to way to properly do this in synapse? 
Also, how can states be passed through pipelines? Say the table garply needs both foo and bar, but these run at different times. how does the compute-data pipeline know when both foo and bar have been updated to the latest snapshot? 
I'm currently thinking of writing logs to a DB, which some kind of meta-pipe can then read to check on states. Something like Neo4j which would also allow us to visualize the state of the DB throughout the day, and what pipelines are running and failing. 
But perhaps there's a better  way.
 
All input from people with experience on Synapse with lots of orchestration is welcome!",Help,dataengineering,s8gz41
"Dear all,
I would greatly appreciate any feedback on this, if you have the patience to read through the lenghtly post.
Context: I'm a DA with occasional DE duties, working for a startup. I have little DE knowledge/experience . In particular I have no experience with BQ or any other DW. Right now I want us to move away from doing BI on our transactional database  and instead copy our transactional data into BQ.
As I understand it , the most straightforward solution is to use tools like airflow or managed services like fivetran, in order to mirror  our transactional database into BQ.
But I have a dilema, as there may be an alternative solution in our case:
Our main product is, let's say, a ""reservation"". Much of our backend infrastructure is based on a queuing system . When a reservation occurs, a json file is produced in the queue that various of our microservices are using to do their job. This json document is very large and deep , includes arrays, and also contains every possible datum we would need for BI. We could say that this json file denormalizes every JOIN we would like to perform on our transactional db for that particular reservation. So, I thought that an alternative solution to mirroring our MySQL db could be to store all these json files in a datalake and batch-ingest them into BQ periodically.
I played around with ingesting a couple of thousand json files into BQ and although I find it a bit awkard as I'm not used to work with nested data, I can see that it could work. But I'm not sure how it would work at scale.
**So my main dilema is:** mirror our MySQL or batch-ingest denormalized json files? Can you detect any obvious problems with the json scenario? Do you see any benefits?
Thank you in advance.
\---
Additional information:
* On average _NUMBER_ json files produced daily. This will probably double next year.
* A %  of the json files will be updated at some point so they may need to be re-ingested in BQ. An educated guess would be at least _NUMBER_% of them. So the BQ dataset would need to be updated, possibly daily.",Help,dataengineering,sdy7g7
"As the title says, is there a library for Dask with similar features to Spark's GraphX for graph analytics?",Help,dataengineering,sftmkg
"I am trying to test the server status of my db with PyMongo and am getting an SSL error. 
I installed the root certificate to comply with MongoDB but it seems Python / OpenSSL can't access it.
Is there a turnaround to allow Python to access my installed root certificate so I can check the server status of my database?
Apologies if I've posted this question in the wrong place",Help,dataengineering,sf4fmh
"Hi, I've noticed that in Snowflake you can have unlimited number of Databases.
Let's say I would like to have different DB for each ""type"" of data .
Each Database would have almost identical schemas .
So, it would look like this:
PRODUCT\_DB: RAW, CLEAN, PUBLIC
CUSTOMER\_DB: RAW, CLEAN, PUBLIC
and so on... there would be a lot of Databases in the end.
What are some disadvantages of this approach? Is there something fundamentally wrong with this approach, or will it be fine even with a lot of Databases?
On the other hand, what are advantages of having RAW, CLEAN and PUBLIC DB, where we would have PRODUCT\_SCHEMA, and CUSTOMER\_SCHEMA?
Does it really matter at the end of the day? Thanks!",Help,dataengineering,s7vzl1
"I have worked with ADF with synapse, but my current project is ADF with snowflake.
I have done some meta data driven with synapse and adf. How can I do the same with snowflake?
Anyone tried the adf metadata driven data ingest with snowflake?",Help,dataengineering,s4x8yu
"Hi Gurus,
I hope you can help me. I have  is a simple conversion of a timestamp from UTC to AEST. However, when I view the timestamp in the source table , then I view the timestamp in the target table , the dates are exactly the same? Below is a screenshot of the code I've written to derive the column in Visual expression builder. I have the same issue with WhenModified.
Am I missing something?
screenshot from derived column - visual expression builder _URL_
Thank you in advance for your help, I really appreciate it.",Help,dataengineering,sfrva7
"I have a data pipeline where the following steps occur:
_NUMBER_. Call an index API endpoint, which provides a list of IDs in which to make other calls 
_NUMBER_. For each ID in the previous call, request another endpoint with that ID 
_NUMBER_. Extract attributes from ID call and insert them into SQL tables
When developing the pipeline, I would save the JSON responses to disk so I wouldn't eat up the API quota, but I'm wondering if it's normal to always save the raw response even in production.",Help,dataengineering,sets3p
"Hello, I was wondering about the best practice to notify users  about a successful/unsuccessful notebook run.
I read about job alerts, but is there a way to send notifications to slack?
Thanks in advance",Help,dataengineering,s1ce8y
"Hey folks!
Has anyone of you ever worked with Manta (getmanta.com]) for data lineage management? Or maybe Collibra ?
Asking for a mid-large size company that needs to get clarity on its lineage and report on it . The company is not using a unifying layer like Snowflake. Just heterogenous systems. 
Keen to hear back!
ContinentalCake",Help,dataengineering,scbp3x
"Hi All, Recently I have changed my company and they offered me ""senior Data Engineer"" role. I was okay since my expertise is in Data Engineer. But after joining the new organisation, they have added me to a project where the scope is for ""data analyst"". I know data Engineer should have data analytics skill to implement data engineering solutions. But, assigning data engineer to a project where the tasks are only related to data analyst??! Is this the same case in all organisation? I am confused whether to continue the job or change the organisation.Please share your thoughts.",Help,dataengineering,sc6wi0
"My company have a running system with RabbitMQ to assign tasks for workers. In my understanding, RabbitMQ help me schedule the order and assign which task should be run at the moment, and monitor the worker's status.
Then there is Airflow, which seems similar to what RabbitMQ does, the introduction I found says it ""schedule, monitor and assign tasks for execution"", and it also needs RabbitMQ queue as messages. What's the meaning of using Airflow if I have RabbitMQ already? 
More specific, what's the difference of function for Airflow and RabbitMQ?
Thank you",Help,dataengineering,s7ebsb
"All of my questions are in the title - just trying to get clarity around what tools / contexts I might hear the words ""in memory"", what it means, what alternatives are, why it is done one way vs. another, etc. . Thanks so much in advance.",Help,dataengineering,sbsb0e
"Hi,
I currently want to export data from two ERP systems  to Snowflake, but the number could increase to _NUMBER_.
Unfortunately the databases do not have database logs enabled and CDC/log based replication is not possible, but the tables have timestamp/rowversion columns.
I tried Airbyte and I like it because it handles schema changes well, but to do a full load of a database after Snowflake it is rather suboptimal. It is slow and if you select another table, all tables have to be synchronised again, even if nothing has changed. With many tables, this is simply unnecessary  data transfer.
Do you have a cost-effective alternative that performs well with full loads?",Help,dataengineering,scn3gu
"Hello all,
Hope you are well. 
Thanks to AstraZeneca, I was able to open source some of my work for the past _NUMBER_ year. 
The project is named, magnus and relates to data science/engineering pipelines.
The project is available here: _URL_
And the docs at: _URL_
Could you please share some feedback?
I am open to contributions and can use your help in making it better/useful.
We are releasing magnus-extensions, an extensions package which makes it cloud ready, soon.
Cheers,
Team magnus",Help,dataengineering,scbkbb
"Hi folks, I have created one ETL pipeline. I want to learn more about
ETL development. Can you suggest me any good books or some website where I can get more knowledge about ETL?
I have used airflow, gcp, python, terraform, SQL, some of AWS services in my first ETL.",Help,dataengineering,semga6
"I am just starting to use the delta format and I am finding it hard to find info on a topic. 
When appending to a new deltatable, does it automatically remove duplicated rows  or is this something that I will have to do in code. Here I mean rows that do not already exist in the deltatable before the append.
What is the ""expert"" way of handling this since delta tables can grow to huge tables.",Help,dataengineering,s7ltj3
"**I am writing a job In AWS Glue that should :**
* reads _NUMBER_ GB across _NUMBER_ Million XML files, each about _NUMBER_ KB. Historical data, won’t change or be added to)
* currently using Dynamic Frames, a call to resolve choice to deal with uncertain level of schema inconsistencies  among these files
* finally writes the data as parquet to an output bucket using some logical partitions based on fields in the data.
* The primary goal of this pipeline is to compact the data and provide useful partitions so that downstream research questions and ETL jobs will be significantly less costly to run. 
* Further, we'd like to make as little assumptions as possible about the contents of the data at this stage and minimize data loss by, for example, imposing a particular schema on the data upon reading
* Additional context, I wrote this with PySpark, but am comfortable with Scala, so would be fine implementing in Scala if people think this is worth it.
The job runs fine on a sample of the data  to a test bucket, but the bottleneck running on the full raw data  is, unsurprisingly, in the initial steps of the job where the driver is forced to list all the files from the input source. 
I’ve implemented some features that are supposed to be designed for this issue, like
* Job bookmark (_URL_ _URL_
* Bounded Execution (_URL_ _URL_
* Input File Grouping (_URL_ _URL_
* others considerations for memory management (_URL_ _URL_
But it seems there is no way to get around listing these files, and the associated pressure placed on the driver. Also complicating this is that the data has unhelpful partitioning. Basically, the data is severely over-partitioned in a pattern that follows:
* s3://bucket/**ENTITY-NUMBER**/
 * s3://bucket/ABCNEWS\*\*_NUMBER_\*\*/
* There are likely hundreds of thousands if not one’s of millions of these subdirectories.
I’m sure others have faced this same issue, so here’s what I’ve gathered as potential solutions:
* Given a list of the distinct ENTITY values, selectively copy files from the original raw bucket into a staging bucket using a more helpful s3://bucket/ENTITY/ pattern partitioning, and then run the Glue job in batches, pointing at one ENTITY partition at a time
 * not sure if this will help as the performance deterioration on files seems to occur even at the _NUMBER_'s or _NUMBER_'s of thousands of 40KB files
 * I’ve tried using the AWS CLI sync command using AWS Cloud Shell but found that, as many have cited, there are issues here when moving tens of thousands of files
* Use AWS S3 Inventory to create a manifest of all the files, and then batch these into discrete Glue Jobs
 * Not sure how I would point Glue to a manifest of file names rather than an S3 bucket
I appreciate any feedback on these or any potential solutions here!
Thanks",Help,dataengineering,s9shwo
"My team has a pretty large pipeline setup. Running the entire pipeline with production data  takes like _NUMBER_ hours. We have some lower-tier environments with smaller datasets where things take like _NUMBER_ minutes, but at the end of the day, the issue with these datasets is that they might be ok for building the happy path but bugs are not always apparent until we hip preprod and run an _NUMBER_ hour execution.
Having _NUMBER_ hours of downtime minimum per user story is forcing my team to take on larger end-to-end stories. The reason for this is that breaking a story into _NUMBER_ or _NUMBER_ pieces would mean running the whole pipeline a lot more. On the other hand, user stories feel too large at the moment so I am thinking of ways of mitigating the issue.
Right now one option I see is to reengineer the pipeline so that it runs in stages and each stage has its storage. so if we run for _NUMBER_ hours and then find a bug in stage _NUMBER_ of _NUMBER_ we would only have to fix the bug and rerun the last _NUMBER_ stages.
Another option could be making better datasets for the lower tier environments but I am not sure how to do that.
The goal is for the team to be able to decide on how to tackle user stories optimally, Ideally tackling issues in multiple, smaller, safer steps. Not having our pipeline forcing us to clump everything together.
Edit:
More details. 
We have unit tests for everything, not as many as we would want but we are working on it.
Regarding CI/CD: 
Right now these big user stories require us to use feature branches that have lifetimes of _NUMBER_ day to _NUMBER_ weeks. This brings me to another question. In software engineering, I see that the tendency is to go for Trunk based development + feature toggles. I could envision that working really well with something like an API, but I struggle to imagine that working out well in a large big data pipeline.
How are other data engineers with large pipelines doing CI/CD where everything lives in one branch?
Any Ideas? If anyone can point me to any books or talks that might point me in the right direction that would be great.",Help,dataengineering,se0to8
"Tools like DBT have made it easy to version/source control data in databases, but my team is struggling to do the same with managing security  Has anyone else run into a similar issue? What solutions did you find?
EDIT: TIL that terraform is open source and not an AWS-specirfic tool. looking into it now.",Help,dataengineering,s2aisk
"Hello, 
I am new here and I am in the process of setting up my LinkedIn profile. I need your help in finding an appropriate banner that communicates my specialty as a Data Engineer.",Help,dataengineering,s5znhw
"I'll start by stating this is really an Analytics Engineering question, but hope someone here can help me out a little or at least point me in the right direction.
The EL part is done. Unfortunately, it's not currently done incrementally but is instead executing a raw ingestion of all data from the source system. Regardless, I have access to all my source tables. Next comes the T, which right now is a series of sequential SQL files that execute in a specified order. Not using dbt or Airflow, this is actually external client data flowing into our SaaS product, so execution of the files are controlled via the backend. 
We are currently truncating or outright dropping and recreating the data for all time even though only a small fraction of source data changes each time we run the EL process. Right now, we allow the transformation process to write data again for all time. Highly inefficient process, but it got the job done fast and cheap. The primary key where all of those files write to is a UUID. 
Hopefully that paints the picture well. The issue is that we're finally at the point that truncating the data in the final output table no longer works for us from a scalability perspective . I know in Postgres INSERT...ON CONFLICT UPDATE is how to do an UPSERT, but that's not really my issue. My problem is that the PK of this table is the UUID, which each time we execute the process, it creates new UUIDs. 
So if I don't truncate the output table at the start of the process and simply change my INSERT INTO to INSERT...ON CONFLICT UPDATE, how do I define the conflict when the output of the transformation process will have a distinct UUID from the last time the process ran.
Am I overthinking this and the reality is I need to segment my process so that it only executes for the new subset of data ?
I'm new to the role and am trying to understand how to approach the problem correctly. All ideas welcome. Thanks",Help,dataengineering,secen2
"I know Spark is supposed to be fault tolerant etc. etc. 
My question is -- for those of you who run mission-critical data pipelines with Spark, do you guys actually rely on this fault tolerance? i.e. take out spot instances and pray that everything goes fine if one of them gets pre-empted? Or do you guys still use on-demand instances and raise errors when an instance goes down?",Help,dataengineering,s7dull
"Folks, I have attached a reference architecture diagram here that depicts a typical old-school architecture using a NoSql database. Can someone help me understand the arrow pointing from the *SQL database* to the *API/Web Tier* titled ""analytical queries""?
What sort of analytical queries are we talking about here?
Is it referring to a Reverse ETL Process?
_URL_",Help,dataengineering,s6cce5
"Good afternoon, I have a problem regarding accessing data from within a pipeline.
I need to access some data from within a pipeline, but I DO NOT want to pass that data as a variable to my PTransforms. . I also don’t want to hard code this data into the script that will be ran in the pipeline, because that’s sensitive information. I have tried two things that didn’t work:
- I have tried getting this data from the OS environment and dynamically changing the variables that belong to another python script before the code goes into the pipeline itself. The plan was to have my other script which is the one that runs in the pipeline to import that first script and use its variables. But when I tried running it, all the variables were still None.
- I have also tried creating an object before going into the pipeline, with the credentials, pickling it and saving it to a temporary file. Then, in my script in the pipeline, I would open that file, and get the credentials. However, when I tried doing that, I got an error log on GCP saying that the file didn’t exist, even though it did exist on my machine. 
Can anyone give me any other suggestion? Thank you.",Help,dataengineering,s75tbw
"I'm planning to play with aws a little meaning there's alot of uncertainties and a lot of unexpected charges that might occur, as far as my research goes aws budget could only alert you when the threshold for a specific service has been breached but it's not really going to do much more. 
I'm planning to use a debit card with a very low amount of balance so when things go wrong they couldn't charge me anymore as it doesn't contain enough money, i've tried searching but i haven't seen anyone who's done the same - now i'm getting paranoid whether what's wrong with my idea, can anyone point it out?",Help,dataengineering,s19rv3
"Hi there,
I'm facing this problem at my org. I'm working as a data eng in a team of a dozen analysts.
Analysts were tasked with creating tables in BQ to support their dashboard . They would also schedule their queries in Airflow. The reason they were doing this is because of scarcity of data engineering 
We ended up with many datasets and tables parts of different data pipelines. A lot of the SQL queries in these pipelines were building the same transformations and sometimes same aggregations .
In some cases it led to having different ways in SQL to calculate the same metric. This is obviously a problem because an end user consuming the same metric from different data products could spot the difference and be confused
So my question is:
How do you go about building a process which makes it easy for analysts to find what has been built already and that they can reuse without spending too much time searching/browsing through lots of sql files in gitlab for example?
Thanks!",Help,dataengineering,s8ypeu
"I have a question -
In current project ,we want a way for users to upload files.
Once files are uploaded to S3 folder,lambda takes care of rest processing.
In which way users can quickly upload files to S3 without having any access to aws. Users are not at all tech savvy.
Creating a flask website is long term solution ,but I just want quick and easy way as interim solution.",Help,dataengineering,s35m72
"Hi all,
apologies for what may seem an uninformed question but I've been trying to think of an architecture to build off of an API. We use a SaaS service to collect machinery time series data . The data is fed into the proprietary SaaS through a ""historian"". The SaaS then offers a public facing API.
However when we tried to fetch a large amount of data the API just can't handle it . I would like to build an ETL and/or pipeline to get all this data on our own cloud  and be able to run machine learning tests etc. on the data. 
Am I thinking in the right direction? Do you have any technical stack to recommend?
Thanks.",Help,dataengineering,shd811
"We are standing up a Snowflake DWH newly. Now the question is should I opt to go with snowflake datalake for file storage or Azure datalake storage. Considering the fact all my current files are already stored in Azure datalake.
_NUMBER_)Need help on understanding pros and cons of using snowflake datalake vs Azure datalake.
_NUMBER_)Does snowflake datalake has GUI kind of interface to archive the processed files for future usage, regulatory usages?",Help,dataengineering,sdajbp
"Title says it all ..... Searched the internet alot, couldn't find anything other than few half baked stuffs.... Any leads would be extremely helpful!",Help,dataengineering,sdyzwb
"Has anyone here appeared for the technical round at 5x for the data engineer position? They've told me on call that it would be a _NUMBER_ hr technical round, which would involve taking a test simulating day to day work of a 5x DE.
Has anyone appeared for it, or has any tips for it ?
Thanks!",Help,dataengineering,s1evh0
Took a new job and I've been asked to choose a new machine. I was disappointed to see that Apple doesn't sell Intel chips at all anymore because I would've clone my _NUMBER_ MBP. I know when M1 came out there were a number of compatibility issues with data science related packages. Have those been resolved? Do they not really impact your day to day?,Help,dataengineering,s077i4
"I'm  a DS where I work, wearing different hats as necessary, but I'm having to become less ignorant about DE because we don't have one and it's clear my company is not handling data well.
Right now I'm working with longitudinal medical assessments of our users: questionnaires assessing their health before and after using our product. Some of the assessments are in an internal MongoDB database , others are in an external forms website . The ones in our internal database have a user ID, but there are several versions of our questionnaires on the forms website without user ID, so I have to match names by Levenshtein algorithm to an enormous list of users. I have Python scripts collecting questionnaires from all these sources , matching them to users, interpreting what they are and ordering them by datetime, deciding if the timeline is acceptable for analysis, etc. so the data can be assembled and explored for a causal effect. Each time I want to tell my boss who needs an assessment , I have to run a script that collects data from all of these resources, matches names again, interprets the metadata, etc. in a very inefficient way.
Clearly, it'd be better if all of these assessments were put into a central repository I could query from for faster insights and even for dashboards monitoring what % of each user base has assessments completed, who is due for them, etc. It seems to me a data lake would be most efficient, because there are different variations of forms with slightly differing questions, with pipelines scripts running on a schedule that extract from each resource and match names when needed . Am I correct in thinking a data lake, hosted somewhere like AWS, and a few pipeline scripts, could solve this problem? Thanks for your help.",Help,dataengineering,savhoq
"Hello, you mystical beasts.
I am currently employed and hold a masters degree in an unrelated field. I want to earn a bachelors and/or masters in the field of data engineering. My hope is to find scholarships and grants that can help make my dream a reality. 
Sorting through the internet for scholarships has been a daunting task. I want to find real opportunities to apply for grants or scholarships specific to the field for women.
Please help with any information you may have. If you were awarded any scholarships, I'd love to know which! Also, any websites or collective of where to best find and apply would be greatly appreciated.
Thank you!",Help,dataengineering,shgvr7
"The main problem is, I have some table's from Postgres and We build a batch pipeline to load these data into BigQuery, there are some columns from these tables that are updated each hour, day, or week.
I'll give an example, customer status, this status could change from ""active"" to ""inactive"", ""blocked"", ""pending"" somethings like that. 
 On Postgres, it works fine because it handles well with transactional data operations, but BigQuery doesn't handle well with that transactional data operation, So our solution was to create a view with all deduplicated data for each table.
The thing is, it won't scale, because now we have large tables and our BigQuery's bill is too expensive.
There is some move this data from Postgres to BigQuery without creating duplicated rows when some data is updated?",Help,dataengineering,seybpa
"Recently I have completed _NUMBER_ courses on Udemy. 
i.e. Azure Databricks & Spark Core For Data Engineers]",Help,dataengineering,sdyec5
"Hey all, 
We’ve recently had a databricks demo session and now are scoping out the cost of using it in our organisation. 
We are currently building a data warehouse and etl pipelines in Azure, however we are interested in using databricks for our advanced analytics and machine learning in the future. 
Cost is a significant factor, and we want to compare the cost of databricks to the cost of other azure tools in this space. 
Can any of you give me some direction on how to begin getting $ figures for thIs comparison? All help is appreciated as I am just a junior who is trying to show some initiative. 
Thanks!",Help,dataengineering,s2v7qi
"Hello everyone,
I've written a script in Scala that reads data from MongoDB collections and store their data as parquet files on HDFS using Spark.
The problem is, in one of these collections, when I read their data, there is a column that sometimes its data type is NullType and sometimes is DoubleType!
I used to convert all NullTypes to StringType, but in this specific situation I can not do such a thing.
I really stuck on this problem and I don't know how to handle it. And there are some nested columns that makes the situation even more complex.
I really appreciate your help on this if could give me a hint how to solve it.
Thank you in advance.",Help,dataengineering,s2zn63
"Hi everyone, first of all thanks everyone for building this great community! You have helped me tons so far.
I am a jr data architect and trying to get up to speed as quickly as possible with data management in general. After reading DAMADBOK I want to get some more experience with data warehousing and hands-on modeling. Following your advice I have started The Data Warehouse Toolkit by Kimball. 
Do you have any tips on a real-life test project I could work on while reading the book? I have access to an AWS Sandbox account, so something on there  would be great.",Help,dataengineering,sftpkt
"Hi All,
I am new to Dagster and wanted to see if there are any online resources/videos/courses available for training?
I couldn’t find much online. Any help would be greatly appreciated.
Thanks!",Help,dataengineering,scg8bn
"I'm starting to get into ""big data"" and the lakehouse architecture . 
Do you know of any resources that document/ explain the different patterns how data can be aggregated into the gold layer tables ? 
Example, assume I have an immutable transaction log, are there patterns that I can adopt to summarize or aggregate these transaction log entries into a single entry ? OR any patterns that describe how such a table can be joined with other tables to create a de-normalized view?",Help,dataengineering,saop3p
"I'm a newbie to Redshift, have worked with Oracle, MySQL before.
There you can do a desc.
What is an equivalent command in redshift?
Also the way I access table is like abc.xyz .
I did some googling and found pg_table_def, svv_columns but they don't seem to work in my case.
I believe I'm missing an escape character or something like that.",Help,dataengineering,sblxng
"Hi all,
I work on a startup as their only data analyst and, by necessity, I deal with data engineering tasks on occasion, at least until we have the luxury of hiring a real data engineer.
Our main data are our transactions  and google analytics. We also have data from various services we use , that stakeholders analyze via the tools offered from these services, such as online dashboards. I want to improve the situation by setting up a basic data infrastructure with bigquery as our DW where, as a first step, I will be storing and analyzing our transactional data.
Now, while I'm in the process of setting up our DW, as a company we have almost decided buying the services of a team of BI consultants that have built their own BI platform. Their backend is based on bigquery and the frontend on power BI. They will be ingesting our data  into their bigquery, then they will be merging them with various domain data they generate from independent sources, and then they will be making customized dashboards for some of our teams and provide insights.
My main concern is that they insisted that they will not be using our own bigquery for their platform and thus we will start building separate data pipelines for the same data.. Both google analytics and our transactions will be ingested in both DWs. I am afraid this will introduce double the complexity, maintaining two pipelines instead of one.
Am I right to be concerned about that? Is it common practice that external data consultants would maintain their own independent data infrastructure? I have little knowledge when it comes to Data Engineering nor I have been exposed to this kind of situation before, so any advice would be very appreciated.",Help,dataengineering,sbk5o6
"I'm somewhat new in the DE world , and I really want to increase our certainty that our ETLs are doing what they're supposed to do.
We have both our lake and warehouse in different Redshift clusters  and we have some very standard spark jobs for those ETLs. What I want to have is some ""validation"" step that the result is what we expect:
- Number of rows in source and destination is the same 
- Test columns against source 
- Foreign keys point to the right entities 
- Some more, maybe 
We started looking at Great Expectations and invested some time on it but it doesn't seem to support checks upstream vs downstream, at least not easily or that I can see.
How are people doing this?",Help,dataengineering,saqti9
"Hello guys,
I'm building a database on Bigquery for a project written in Python using the client libraries which is basically a recommendation system but rather than for ads, it's for better-tailored search results. 
Should I make it so every new user automatically creates a new dataset? or just a new table for that specific user .
I also believe I'll need a table of users and their information. Any recommendations? 
Thanks in advance!",Help,dataengineering,s5nuxw
"I frequently use ast_node_interactivity _URL_ setting in Jupyter to get multiple outputs from a single cell. It does not seem to work in Databricks and I cannot find any mention of it online. 
Any way to make it work in Databricks?",Help,dataengineering,s8hv69
"Hi folks, 
I am a master's student looking for an MLOps internship. I would like you to give me your valuable feedback/suggestions so that I can improve my resume. Thanks!
_URL_",Help,dataengineering,sclz30
"Hi! I am trying to build a project for my uni's FYP and decided that I will go all-in so that it can double up as my personal portfolio as well. My idea is to build a modern batch end-to-end data pipeline with containers orchestrated with Airflow, from ingestion to visualization.
Originally I was hoping to collaborate with a company for the data source , but as time goes on I began to feel that it is incredibly difficult for companies to share their data to a student. Therefore, I began to look into the possibility of using open source data or Live API.
The problem is, I am facing difficulties of choosing the most appropriate data sources. I hoped my project could accomplish and demonstrate these characteristics below:
\- Integration from multiple diverse data sources
\- Batch import every X time 
\- Data sources rich enough for a few visualizations 
Can anybody advise me what data source that I can potentially use to accomplish this? Thankss!",Help,dataengineering,s2sdrb
"TL:DR: On read, a column in spark is an Intiger, during convertion to Pandas its a Long, write fails because in the silver table its a Double  - Have any of you encountered this before?
I have a process where I use spark to read in data from a delta lake using a sql query. I am finding the closes Lat Long pairs between a every day updated file with a static file. I am doing this Raw-to-bronze, bronze-to-silver, silver-to-gold. In the bronze-to-silver I read in the data using spark, I convert that to a pandas dataframe so that I can finish adding the columns with the closest lat lon pairs and convert back to spark to write to the delta lake. A few columns, that during read are Int are converted to Long in pandas, then I create a spark to write to delta lake but then I cant because in the silver table that column is Double. I have tried many times and overwritten the schema, but always the same error comes up. Has anyone ever fixed something like this before?",Help,dataengineering,sgv5fo
"Hey everyone,
Trying to get a grasp over our event traits being sent to Segment and soon RudderStack. I've been auditing our event data and it seems like our tracking plan basically mashes as much data into traits that don't really have anything to do with the event. Here's an example:
 track;
Our product person claims they need the data but we already have video author information in MixPanel. It seems to me like MixPanel should be able to derive the videoAuthorUserType no? Is the above bad practice? Any tips on best practices for shaping event data?
It seems to be some of this data is being flattened together to compensate for a lack of a feature on certain destinations but that should probably be fixed with a transformation to the destination no?
Appreciate any advice or tips.",Help,dataengineering,s2ocuj
"My thinking is that: you could get ksqlDB or Materialize or another streaming analytics offering to perform analytics in  real-time after joining multiple tables of streaming data from both  your OLTP database and  streaming event data from IoT sensors that are written to Kafka topics.
Since this is the case, why would you ever use something like HVR for CDC to have  real time into your OLAP CDWH ? 
* Is there any need to actually have that information move into your CDWH real-time? For me, I see two use cases of that data once it's in Snowflake
 * _NUMBER_) Analytics: when you are doing analytics on data in Snowflake, are you able to have those updated in real-time if you are querying Snowflake?
 * _NUMBER_) Reverse ETL / Operational Analytics: when you are syncing modeled data from Snowflake into your SaaS apps like Intercom or Salesforce...
 * Do the underlying data models in Snowflake get updated as soon as new data comes in ?
 * Regardless of the answer to the above, is data synced to your SaaS apps  updated *in real time / as soon as the modeled data changes*, or is it updated in batch?
So, in a nutshell, I'm wondering why there is even a use case for a company like HVR when you could just analyze that streaming data with a different streaming data analytics tool and no need to put it in your CDWH. 
Thanks in advance!",Help,dataengineering,sbuge9
"I find it a PIA to use the wiki, to put it frankly. For example, if you go to the FAQs and take a look at the ressources you have to juggle _NUMBER_ scroll bars at once to search a page that is only viewed on not even half of the screen. The interactive graph on the right should not be permanently visible but only optional at max. Furthermore, I find the whole concept with showing multiple pages over each other rather disturbing and quite the opposite of a clean and clear design of markdown/HTML files.",Help,dataengineering,s4lljg
"Does anyone have a good ""how to get started-"" guide on building infrastructure for a DWH in the cloud for a private account? 
I'm interviewing for jobs and have heard from colleagues that some companies have task' included in the hiring process containing the above request. So was wondering if anyone have a guide on how to get started, similar to all the available guides on how to get started with respective programming languages.",Help,dataengineering,s785uk
"Mods - I'm sorry. I know this is a stupid post. I recently posted a thread about where Kafka fits into the modern data stack and someone offered to speak with me. I accidentally ignored the chat and I am upset because I was excited and this is a rare opportunity for me.
I will take down this garbage post as soon as they see this and message me. I know this is silly but please let me keep this up. Thanks.",Help,dataengineering,rzuz4i
"I feel like when someone is explaining their data stack, it is a word salad of brand names. I have a hard time lining up a particular technology  with its function within the enterprise.
As a bonus, I would love some primer that explains what all the functional areas are and how they work together. Not sure that exists though!
Much appreciated.",Help,dataengineering,s2cyhg
"In order to work with Spark, I would like to learn Scala. Until now I only have Python knowledge. I have browsed through Udemy and Coursera but there doesn't seem to be plethora of Courses like for Python. Can anyone recommend good learning sources like courses or books based on their experience? Thanks a lot!",Help,dataengineering,s7nexo
"I noticed that Data Scientists and Analysts at my organization prefer to use Jupyter Notebooks to create data processing code. 
It is often the case that they create complex pipelines consisted of multiple interconnected Jupyter Notebooks where one Jupyter Notebook is consuming the output of another Jupyter Notebook. 
At the moment they don't have a way to schedule this ""notebook pipeline"" for regular execution. 
I wonder if there a tool that they could use to define a dependency graph between notebooks and schedule that graph for daily execution. 
Personally I use Apache Airflow for workflow orchestration, but Airflow is too complex for Data Scientists to use. 
Is there a tool that people without deep SWE knowledge could use to achieve a similar result?",Help,dataengineering,s3qb5g
"Hey guys!
I'm currently in the process of studying for DP-_NUMBER_, the Azure Data Engineer Certification. However, the MS resources  are too detailed and overwhelming since I have no prior technical experience at all.. 
Can anyone recommend sources to study from? Any courses out there for beginners that provide a simple explanation to things? 
Any resources would be much appreciated _EMOJI_️",Help,dataengineering,s1i5no
"Setting up config and infrastructure is a nightmare in Windows. Even with terraform and docker and what not.
Windows WSL is a bandaid to most stuff. I find myself constantly jumping between powershell and WSL even though I have just only started learning DE. There is a unique problem in every step of the way of when setting up infrastructure in a Windows machine and eventually when you do find yourself ultimately landing in the WSL github issues page you know there is no fix for that particular issue.
Now I thought about using linux exclusively but I really need to use Excel from time to time. 
So, I wonder who even uses Windows as their main machine? Why and how?",Help,dataengineering,sco2zg
"I have a table in snowflake which contains around 15M rows. Now for few columns I need to convert it's type from varchar to numeric and datetime. Currently snowflake doesn't support this. So, I have created a new table and moving data from old table to the new one using python.
However, after _NUMBER_ hours of running the code stopped due to session error and only half of the data loaded in new table.
Now it's hard to see which rows are missing because there is no any primary key or unique column.
So, Is there any other efficient way I can use to accomplish the task?
#snowflake #dataloading",Help,dataengineering,s4cuwk
"The thing is, I am not a good writer and I do not know how to sell myself, so I really would appreciate some help :) 
WORK EXPERIENCE 
xxxxxx , Remote 
Data Engineer, _NUMBER_/_NUMBER_ – Present 
- Implement a data lake and data warehouse, using Airflow and AWS cloud, allowing non-tech 
members of the company to have access to business data in a security and localized place. 
- Built a data framework that improved overall performance for building a single data pipeline in 
_NUMBER_%. 
- Integrated data from multiple third party APIs. 
xxxx , Remote 
Data Engineer, _NUMBER_/_NUMBER_ – _NUMBER_/_NUMBER_ 
Implemented real-time data ingestion pipeline for non-sql data with Apache Nifi, AWS 
SNS and SQS and Kafka to store data in S3. 
Design and maintained a data ETL pipeline, in AWS, improving performance and 
reducing operation costs. 
Maintained StepFunctions and Airflow pipelines, adding new features and improving 
overall performance. 
xxxx , Sao Paulo, SP 
Machine Learning Engineer, _NUMBER_/_NUMBER_ – _NUMBER_/_NUMBER_ 
Worked with a multidisciplinary team to develop an artificial intelligence computer vision 
solution in Python . 
Design and implement an AI Pipeline in Google Cloud Platform  environment for faster 
model training and deployment, reducing the overall time of _NUMBER_ weeks to _NUMBER_ days. 
Create data ingestion pipelines in GCP BigQuery improving the data science team productivity. 
Implemented Data Version Control  framework into company data culture. 
EDUCATION 
Universidade Federal xxxx, Brazil, _NUMBER_ - _NUMBER_ 
Computer Engineer 
SKILLS 
AWS  
Spark, Kafka, Airflow 
Python, Pyspark and SQL",Help,dataengineering,s8n92n
"I need to trigger my ADF pipeline when three files arrives in paths : container/folder1/file1.parquet container/folder2/file2.parquet container/folder3/file3.parquet
Only when these _NUMBER_ subfolders gets new files should the ADF pipeline trigger.
How can we achieve this?",Help,dataengineering,s24qyo
Can anyone help me with some good interview questions on azure data engineering tech stack? Any design related questions are also welcome..,Help,dataengineering,s700yf
"**So a little context about me:** 
Undergrad: Double Major in CS and DS 
Masters: Business Analytics 
Experience:
* Two Internships as a Data Engineer .
 * I learned how to build database schemas from an ETL process to production.
 * Became familiar with Azure Synapse and Data bricks.
**Context about my situation:** 
I was recently hired as the first ""Data Scientist"" at a quickly growing startup . My first task is to move their on-premises database to a cloud-based data architecture. Our department is low on funding and I'm currently the only person working on this project. . I want to build the best architecture that I can given my experience .
After building this data architecture I'm hoping to move into more analytical role where I would like to leverage data to make business decisions.
**The Current Data Model:** As of right now, the database is built in PowerBI using many DAX and PowerQuery statements. . Most of the data is populated out of salesforce.
**Objective and Advice:** Although I have experience in Azure Synapse and Databricks, I am open to learning any data architecture platform. I hear many good things about snowflake and I'm wondering if the company should pursue this tool. Before I start anything, I am focused on learning how the current database was built and I think I want to reverse engineer the process and build it from the ground up.
I have a lot of questions:
* What should I do first?
* Data Lake vs Data Warehouse?
* Which platform should I be using? 
* For those experienced in the field, what is one thing you wish you had known sooner?
 * What platform is the best in your opinion?
**TLDR:** I am a fresh grad tasked to build cloud data architecture, looking for any and all advice. 
Excited to hear your input :)",Help,dataengineering,s3bb7b
"if any one have experience on ADF and Airflow what is it look like on working , which is better?",Help,dataengineering,s76yg7
"Hi all! I am studying a MS in Big Data and this year I have to do my final project and I would like to know the opinion of the community. My main objective is to use this project to help me to get a junior job as a Data Engineer . After some research, I came to the conclusion that I mainly need a project to show my skills in Python, SQL and some Big Data technologies, and preferably using real data instead of a static dataset.
Considering this, I have decided to use the **Twitter API** to read tweets with the #nowplaying hashtag and get song information from **Spotify API**. The technologies that I plan to use are **Airflow**, **Spark**, **Cassandra** and **Metabase** or, if I have enough time, build some frontend with **Flask and Bootstrap**. Also, I would like to use **Docker** to run the project in a container and make easier to reproduce it. Additionally, my tutor is a researcher in the Data Science field and we will probably add some machine learning when I talk to with him about my choice, so this may vary.
Any thoughts or opinions? Would you change anything in this project considering my objective? I am new to technologies like Docker, Flask and Bootstrap, so that is why this part is more like a ""possible next step"" than an actual phase. I also have a question related to Docker: if I develop my project and then I decide to give a try to Docker, can I just migrate my full project to Docker, creating a container with all the ETL flow and the visualization part? Would it be difficult?
Thank you in advance! _EMOJI_
_URL_",Help,dataengineering,sgptiz
"My company is sponsoring the courses on Coursera for the next _NUMBER_ year. is there any course on data engineering/data engineering tools on there worth taking? The IBM professional data engineer course seems comprehensive but have read lot of negative reviews regarding it here. Any suggestions?
Edit: my company currently makes use of Airflow, AWS S3, splunk, Qtest, Redshift and Informatica Data catalog for data lineage and governance.
There is news floating around about migrating from redshift to snowflake.",Help,dataengineering,s9ba7p
"I understand that bad things can happen when SQL queries are executed based off user input but I'm having trouble figuring out what situations are acceptable and if they're not, how I can fix them.
I'm using dbt and I have an automated ETL that runs my dbt models on a daily schedule. I've recently added a variable/parameter to one of my models. My pipeline runs in Github workflows and just calls dbt run through the command line with the --vars option. I guess if someone was able to hack into my GH, they could change what's passed with --vars and add something sketchy to the SQL? Idk... is this something to worry about? On one hand it seems like this is exactly what people talk about with SQL injection but on the other hand, I'm confused why they make variables an option if it wasn't safe!",Help,dataengineering,sasfso
"Hi,
I am trying to work on a solution that ingests medical records. Due to compliance, it is mandatory to encrypt the data in transit and at rest.
I am planning to read the medical records using AWS textract OCR and update them in DB. The idea is to connect this to AWS Quicksight and show the visualizations.
My questions
_NUMBER_. What are the better alternatives for OCR software to read and ingest medical records? I am using AWS textract and Comprehend Medical. I have tried Python tesseract lib but AWS textract is neat w.r.t identifying forms/tables/blocks in the documents better. 
_NUMBER_. What DB performs better with encryption. Any leads? I am not good at identifying whether to use SQL OR NoSQL DB here, mostly I tend to use what I know, which might not be the best choice. Right now I am planning to use MySQL but I am not sure if it's the best choice.
Thanks, appreciate your time and help.",Help,dataengineering,s3qppg
"`pipebase` _URL_ is a low code data integration framework.
In general, the framework allow developer customize data pipeline through `manifest`](_URL_ definition and wire a variety of system through [`pipeware` _URL_ plugins to sync/transform data.
Here is a `Tutorial` _URL_ as quick start, build your first hello world app  with CLI
And here is a list of `Example` _URL_ demonstrate how to compose manifest  and wire external system ex: `kafka`, `rabbitmq`, `mysql`, `cassandra`, `rocksdb`, `aws-s3`, `mqtt` etc.
Have fun !",Meme,dataengineering,s0ooxy
"Virtual community peer-to-peer DataOps sessions: **_URL_ _URL_
* Building multi-cloud platforms with tight regulations
* Strategic data team composition with talent shortages and constantly increasing priorities/objectives
* Open source cloud native data lakehouses
* Building vs. buying considerations
* Operational analytics loops on the modern data stack
* Streamlining notifications/comms/jobs with Slack integrations
* Delayed releases
* Data warehouse performance tuning
* Orchestration in modern data platforms
* Modernizing workloads with EMR
* Moving from batch to near-real-time analytics and visibility
* End-to-end orchestration of ML models 
* Building resilient systems
Starts at _NUMBER_ AM PST on Wednesday - appreciate the support as we all learn and grow together!",Career,dataengineering,shezw2
"The peer-to-peer DataOps community agenda is live for the free, virtual DataOps/pipeline sessions at _URL_ Feb _NUMBER_ _EMOJI_️ Both live and on-demand. 
Each session will be practitioner-to-practitioner focused so we can learn from each other and includes perspective on:
* Building a multi-cloud data platform in a tightly regulated industry at Babylon Health
* How the team at Slack streamlined their DataOps stack
* End-to-end orchestration of ML models at Volta
* Multi-cloud architecture at Arcus payments
* Open tech cloud native data lakehouses at IBM
* Modern data platform orchestration at Dutchie
* Moving from batch to near real-time analytics at Akamai",Career,dataengineering,sahaj5
"I just got an offer for a DE position at Meta and was interested in knowing if others could shed some light on the role to help in my decision making process and check a few assumptions I've made. 
Briefly, what I have managed to understand is:
_NUMBER_) Meta DE work is, mostly, closer to the role of an analytics engineer involving a lot less development and more SQL/dashboard building 
_NUMBER_) Work life balance can be hard to achieve 
For the first point I'm less worried. I do enjoy that kindof work and am comfortable in that kind of role. My main concern here is work life balance. Reviews on Blind and Glassdoor are kind of all over the place with regards to this but many mention long hours and high pressure. I get that this can vary a lot by team, but I'd appreciate any insights any of you might have.
Is work like balance as hard to achieve as others claim? What's the culture around overtime/after hours work?
Edit: clarifying questions",Career,dataengineering,s0xr0d
"Currently I’m working as a consultant, with the latest cloud and data tech . 
I enjoy the stack but I came to dread consulting and am looking for a way to transition to a more pre-sales oriented role which I cannot do at my current company and have not much experience in. 
I have the following offers on the table:
A) switch to other consulting company as a principal consultant. Tech portfolio is wider, not focused on “being cutting edge”. Pre-sales is part of the role, as is delivery, project org etc. Probably least jump in TC. 
B) become a trainer for cutting edge big data tech, e. e. spark, Kafka, Hadoop, at renowned boutique consulting firm “experts among the experts” stuff. Keeps the knife sharp, no delivery anymore, customer facing, opening doors but not pre-sales per se. Acceptable jump in TC. 
C) _NUMBER_% pre sales architect/consulting role at a  vendor of a solution for data estate building automation, enabling smaller companies to join the data game etc. Tech wise this feels like a step back, the role however is closest to what I’ve been looking for. Biggest jump in TC.
now my question: if the longer term goal is to become a pre sales guy or even transition completely over to sales in the big data tech I use today - is it wiser to stay closer technology wise even if it’s still more on the delivery side? Is it a good move to take a detour to training to get more exposure towards customers? Or is it ultimately the pre-sales game that I should start leveling up and transition to the same role, different tech later?
I know there’s not a straight forward answer, would just appreciate to get other opinions on it, as I have been discussing with myself a lot already. 
Any insight from people who have done a similar transition already is especially appreciated :-)",Career,dataengineering,sfghem
"I finally got a really cool Junior level job in London, and want to thank the community for keeping me sane in the process. I applied to over _NUMBER_ jobs and the whole process took _NUMBER_ months. 
They said I'll be primarily using AWS Glue for the first couple of months, which I have no experience with. If any of you know any tutorials I'd love to take a look.
Thanks!",Career,dataengineering,s5cldd
Hi all! Hope my post is relevant to the group. I am thinking of applying for a position ay Spotify and would like to ask if anyone has any experience of what working there is like. Keep in mind that I am referring for the emea region and I am really familiar with the Dutch working condions. Every input will be highly appreciated! And a happy & healthy new year!,Career,dataengineering,s2defm
"Hi all,
I am interested in contributing to building technologies that facilitate data engineering. Example technolgies like spark, kafka, Airflow etc. Basically I'm interested in building data engineering tools more than using them. 
Should I then target DE roles or SDE roles?
Please let me know if you'd like to me to add more context to the question.",Career,dataengineering,s59snh
"Taking in count that usually Data Architect are very senior Data Engineers, also Enterprise Data Architects and Data Architects being almost the same job. 
Are the salaries of Data Engineer Staff/Lead, Data Architect and Enterprise Data Architect close?
Thank you",Career,dataengineering,sdtvny
"_URL_
We at DataTalks.Club _URL_ are running a free data engineering course.
We'll cover:
* GCP, Terraform, Docker, SQL
* Data pipelines orchestration 
* Data warehousing 
* Analytics engineering 
* Batch processing 
* Streaming 
More details here: _URL_ _URL_
See you tomorrow!",Career,dataengineering,s59bpv
"Hi,
I've just started a new job as a junior DE and I will work with Ab initio, among others. I can choose how much of my time will be spent with Ab initio and I'm wondering whether it's a helpful technology for my future career or is it becoming obsolete.
Thanks",Career,dataengineering,s745m0
"TLDR;
No Bachelor's degree, and not much time, money or motivation left to get one. Former musician employed in BI through right time, right place and hard work. Should I look away from the data space for an opportunity to build things with code? 
I started my journey into tech/code ~_NUMBER_ during what would be a _NUMBER_ year battle with an adverse reaction from a common antibiotic. Prior to this, I had successfully made a career for myself as a professional musician and director at an entertainment park just before landing the role of my dreams as a guitarist in the United States Air Force band. Once I realized my health wouldn't allow me to get through the compulsory basic training, I decided to make a pivot into something I could do if I had to keep walking to a minimum. I went through Al Sweigart's entire Automate The Boring Stuff With Python which somehow led to a now seemingly misguided entry into Lambda School's  Data Science program.
In retrospect, I think I may have been better off in the web development track, but for someone with only an associate's degree in music and zero quantitative skills, I did surprisingly well in most areas and finished the program. After over _NUMBER_ applications and exactly zero interviews, I started to feel a bit hopeless until an old pianist friend saw one of my projects on social media and decided to send a small machine learning POC my way. This led to a contract-to-hire role at his company, a small BI consultancy almost entirely fueled by Power BI.
I love the people I work with and intend to stay for at least another year . Still, I certainly felt a bit off track once I understood it would be a rare occasion when I got to code anything. I've learned a lot of valuable business, time/project management and customer service lessons at my job. I'm even being given the opportunity to help build data engineering into the business which has really encouraged me to stay, but will likely still be mostly tied up learning how to manage expectations with Power BI deliveries for the foreseeable future.
The thing I'm most discouraged about: it seems that everywhere I look in the data space, my lack of a bachelor's degree seems to be the biggest barrier to a future in this field. I can and will learn more data structures and algorithms, new programming languages, constructs, frameworks, design concepts and tools, but at this point in my life, I don't feel that it's practical to spend my limited time and money in pursuit of a bachelor's degree to fill what feels like an imaginary gap. I feel a welling frustration just typing this out _EMOJI_. 
What advice do you all have? Should I start thinking about pivoting to a space with a lower barrier to entry? Should I sink the time and money into that bachelor's? Staying motivated without knowing that hard work will lead to bigger and better things is turning out to be pretty difficult. Thanks for anything this community's got!",Career,dataengineering,s2zue2
"I'm trying to decide if the pivot from AppDev to Data Engineering is in my future and would like to get a sense of what it involves in the real world. I'm hoping for a broad-brush breakdown of responsibilities and what languages/technologies/systems are the most important to get your job done.
Bonus points if you're willing to give a rough-swag of your experience and compensation.
Thanks people, I appreciate you taking the time to share your expertise.",Career,dataengineering,s1nb6u
"Hi guys,
I'm interested in data engineering but I don't want to end up as simply another SQL monkey. Are there any jobs out there that have you working with container orchestration, IaaC, Cloud, big data tech like spark/databricks and deploying machine learning models all together or would it be better to pick just one of DevOps/Data/ML engineering and just stick with that?",Career,dataengineering,scdesz
"so I’ve been a web dev for about year and might be moving into this cloud data engineer position that pays a lot more than I’m making. The talk with the hiring manager seemed nice and she liked I have a cert in aws and was glad I was asking questions and wanted me to schedule a meeting with the tec lead. This was awesome and I was really excited until I realized I legit have zero experience in anything data. I’m only _NUMBER_ and didn’t really do much sql and data wasn’t offered in college so I have pretty much zero knowledge of snowflake, databricks, lake houses, data warehouses etc. 
The manager did tell me the first year for the team was rough since everyone is brand new to all the new technology which made me feel better but I’m nervous I’m going to be hired with high expectations with data and python knowledge when my only professional experience is web dev technologies and languages. 
I love the idea of the huge pay raise but don’t want to screw over the team with the lack of knowledge and catch up time I’ll probably need. Am I overthinking this and just move forward or should I wait to get more experience",Career,dataengineering,sf2zkc
"I have been teaching myself to become a data engineer for a few months now, but I wanted to know if I am wasting my time. Should I go for something like data analysis instead, or could I actually land a job without a degree in CS if I can prove I know what I'm doing? Any input is appreciated.",Career,dataengineering,sei1dj
"I see a lot of post on how realistic it is to become a data engineer from X field and I think to many of them are worried about what they're doing and not how they're doing it. I went to school for criminal justice and was a phone rep before picking up reporting that consisted of copy and paste cognos reports. Unintentionally I've developed a SQL skill set and picked up some python to move data for reporting efforts. 
If you're using the tools and can speak to it clearly you have a good chance of breaking in. More effort should be placed on this and how you share those achievements on your resume you are ahead of the game.
I'm sure people will disagree, I just want people to not stress out about not being to break in because of lack of opportunity. Make your own opportunity who cares if you over engineer a solution.",Career,dataengineering,s5ndm2
"Are there people here who do freelancing as data engineers around here? How is the freelancing spectrum for data engineers, how is the pay, how do you find clients, what pros and cons do you see to it?",Career,dataengineering,sf4g0y
"Just saw it on their website as I was looking for the job postings:
_URL_ _URL_",Career,dataengineering,sdxt1n
"Any fellow women in Data Engineering?
How are you enjoying the free women's toilets at tech conferences?
How are you doing in your career progression?
I often feel like I have to work 3x as hard as a male Data Engineers to even get noticed in the right ways.
I recently found out I'm being underpaid by quite a bit compared to my colleagues  who add a similar amount of value to the company. I try not compare myself to others too much, but knowing I'm on roughly the right track is useful.
Also I have to deal with a lot of assumptions from my colleagues that I know less than they do. Each time I join a new company, my colleagues are ""surprised"" at my abilities, and it takes them time to realise I'm not a complete idiot and I actually know what I'm doing.
I've been discriminated against job applications. I once applied for a job to a company only to be ghosted. A few months later, my ex colleague got the very job I applied for. We graduated at the same time, same degree and grade but my university is more prestigious. I also graduated with a masters, he graduated with a bachelors. We worked at the same two companies, quite literally joining the same day at both companies. Same job titles and similar experience in both companies. Yet, I didn't even make the interview stage.",Career,dataengineering,s6ac6p
"Hi All! 
I have a question about what should i do with my career, but firstly, let me give you some background. 
I'm a data engineer in a small/medium consulting company. We are building modern data warehousing solutions . Right now I'm stuck in a project where we are using Azure Data Factory for ETL, Azure SQL and Azure Data Lake. In this project we are focusing exclusively on no-code, low-code solutions. Unfortunately there is no way for me to change the project. 
Here is the question.
I started having doubts if staying in no-code / low-code stack is a good idea. Whenever i look at the job postings there is always Python / Scala / Java required . Because of that, I started to think that if anything bad happens it would be hard for me to find another job without proper experience in code-based solutions.
Should I look for another job where I would get more code experience, what would you guys do?",Career,dataengineering,saoqks
"I've been working in the data space for _NUMBER_ years. Predominantly designing, building and supporting data warehouses. I transitioned to python _NUMBER_ years ago and currently using python and neo4j in the clinical data space. 
When looking at job specs, my most obvious gaps are cloud and ML.
I'm curious about, cloud, machine learning, blockchain and kubernetes, but I'm well aware that if I don't use what I learn, I'll forget it. 
The other aspect I need to consider is the accreditation. It's possible to learn all the skills using free educational material online, but that doesn't come accredited. 
Courses can be remote or based in the UK. 
Is there a highly regarded course provider in the UK that holds training bootcamps or similar? 
Any help appreciated.",Career,dataengineering,s8g2am
"Following a CS conversion masters, I was hired as a technical account manager a few months ago at a startup. I started automating some processes for them, which my boss took an interest in, and after a few conversations I'm now in charge of building ETL pipelines with Python for internal data  into SQL databases, analysing the data, and developing PowerBI dashboards. It's early days but, if I make some good progress with it over the next couple of months, I'll likely switch to being an actual full time data engineer at the company and not a TAM with some engineering responsibilities.
This is obviously a great learning opportunity, but as the company has no formal data engineers working there and the dev team are all product-focused I have nobody directly above me for guidance on this specific project. While it's exciting to be managing this project end to end at such an early point in my career, but I want to make sure I'm setting myself up for success later down the line.
So I figured I'd ask here:
* What are some technical skills/technologies that I should aim to build up while working on this project?  
* What are some organisational skills that would be worth practising? 
* If you work with data engineers and analysts, what do you wish they knew/did differently? 
Thank you!",Career,dataengineering,s1q190
"Hey Everyone 
I am learning Data Engineering, have couple of roadmaps that I am following, question that I have is that I wanted to be a bit ahead of the curve and update my resume and LinkedIn profile, is it ok if I add python and sql etc into my profile early on? I am only few weeks in. I am coming from Transportation and Hospitality industries, so besides what I am learning not much relevant experience 
Just like everyone else new to this, I want to get my foot in a door as soon as possible, haha
Thanks in advance Everyone",Career,dataengineering,seqfas
"So I'm currently working as a Data Engineer on GCP/Bigquery, but I also have courses on Databricks and Snowflake that I could study but not sure if it's really worth it? Is it better to stick to one stack and really build experience and expertise on it, or is it better to learn multiple stacks even if you aren't going to work on any of the other ones for the foreseeable future? Is there even a benefit to doing so? No clue what the right way to progress in my career is and hoping for some insights on this from the folks here",Career,dataengineering,s9vz5f
"_URL_
""Hi guys!
I have recently got a job as a reporting specialist at a Law Firm. My responsibilities however are different from what had been promised at interview and I actually cannot develop my SQL skill so I'm currently looking for a job as a Junior Data Engineer.
Could you please have a look at my CV and give me some feedback on what can be improved?
I would be very grateful for your help.""",Career,dataengineering,sbojex
"Planning a training for entry level data engineering positions for _NUMBER_-_NUMBER_ months covering the below topics. 
**Part _NUMBER_** 
_NUMBER_.Core Concepts
_NUMBER_.Hadoop
_NUMBER_. Hive
_NUMBER_. Sqoop
**Part _NUMBER_**
_NUMBER_. Spark Data Processing 
_NUMBER_. NoSQL Databases
_NUMBER_. Cassandra 
_NUMBER_. Hbase
_NUMBER_. Airflow/Nifi
**Part _NUMBER_. Streaming**
_NUMBER_. Spark Streaming 
_NUMBER_. Kafka Streaming 
**Part _NUMBER_. Cloud**
_NUMBER_. AWS
_NUMBER_. S3, Athena, Glue
_NUMBER_. EMR
_NUMBER_. Step execution
**Part _NUMBER_ Projects**
_NUMBER_. Project _NUMBER_ : Sqoop to hive import
_NUMBER_. Project _NUMBER_ : Spark Batch Job data load to Data Warehouse via orcheration tool
_NUMBER_. Project _NUMBER_ : Apache Kafka realtime sata load via spark to Cassandra
Anything more I should add to make it more appealing ??",Career,dataengineering,sewhgf
"Hello,
I was first BI hire in company . 
Because of type of the business, usage and ""agile"" thinking I chose the database of choice PostgreSQL, on-premise. Database is scaling amazingly well , there are literally _NUMBER_ problems, small to none maintenance needed after properly setting the config. 
Now, because of this boom in hiring, I decided to try job market and I horrifying discovered that _NUMBER_% job positions are requiring some cloud experience. 
What are my options for keeping up to date with cloud, given that I am keeping same position? 
I still fail to see how for exactly our type of business would db and ETL process on cloud be advantage (very predictable and stable usage and processes, hosted on computer with specs far higher than needed, far less unplanned shortages as AWS's us-east1  
Only thing I can think of is to create some bullshit ML side-project, where we need cloud for burst computing power. 
Any suggestions? I have theoretically full power as I am in charge of everything BI related.
Tl;dr: I love my on-premise Postgres, job market wants me to switch to cloud, how to get best of both? 
Also, is it hard to lie about your cloud experience? I mean everybody is saying that its maintenance free and things just work, so in theory it should be super easy to learn",Career,dataengineering,s1spyc
"Sorry if this may not be the right sub Reddit for this question but I recently received an offer for a data engineer position at 75K, is that reasonable? I have _NUMBER_ yoe working as a DE, where I’ve been getting paid severely below market value already  so this is a huge jump for me, but just want to make sure I’m not selling myself short again. I currently reside in Texas",Career,dataengineering,sgapx2
"Hey guys, I joined as a data engineer intern about a week ago. This is my first job straight out of college, and I have no prior experience in data engineering whatsoever. 
I was advised to learn Python and PostgreSQL by my leader, and for now, I'll just be watching others work. Everything is new and overwhelming, and it's hard to understand the architecture. I asked all of my questions, but I still don't seem to understand it, and most of the senior leaders are always busy due to remote work.
Some of the technologies I'll be working on are S3, Lambda Functions, Elastic Search, Kibana, Kineis, and Glue. I have a conceptual idea of what they do, but overall, it's still blurry. 
If anybody could point me out to the right resources or path, it would be of great help to me.",Career,dataengineering,sbvbau
"Hi everyone! I've been offered 100k for a DE position in the LA area. I will be moving there and will get a 4k relocation bonus. Wanted to know if this will be a good salary to have a normal/bit above-average life. Currently live in Phoenix, AZ on a 60k salary. As far as I've researched I will be able to maintain my way of life and maybe even increase it a little bit, but after checking LA rents I'm not _NUMBER_% sure, pleaset let me know your thoughts!
Edit: Wanted to add that I have 2YOE total with only _NUMBER_ months in Data Engineering.",Career,dataengineering,s36eqp
"Hey folks. I’ve been working as a data scientist for the past _NUMBER_ years in the Canadian fintech industry, on top of my regular, or traditional if you may, data science/analysis or machine learning projects, I got more and more involved with building automated ETL pipelines and cloud infrastructure and ML API development. I started a few weeks ago to apply for some data/cloud engineer roles kinda just wanted to see what’s out there.... Now surprisingly and with a bit luck I guess, I received an offer for data engineer with a consulting firm... I want to pick your brains to see do people usually move from data science to data engineering or the other way around?
A bit background about me: I only have a BS degree in engineering with a minor in statistics  not trying to show off or anything and prob not important but the reason I am bringing this up is the majority of my data scientist colleagues have a master or Phd in stats/engineering/physics, I used to feel that I was just lucky to be working among them. Not saying data engineers are anything less, but I expect data engineers are more engineers and not necessary need a Phd for that matter which is why I never did a master or even interested in going back for one... honestly I was once been told if something is not working blame data engineer not the data scientist.... Lol imaging it prob would be a lot more stressful in that role...
So just want to know has anyone done this switch or the other way around and do you think you’ve made the correct move? What would you consider before making any decisions?
Thanks in advance :)",Career,dataengineering,sdoguq
"Interviewing at a company  that has a young and small DS team  looking for a DE. The team is comprised of _NUMBER_ DS, _NUMBER_ Developer, and _NUMBER_ Data Analyst. This role is geared towards serving DS exclusively and bringing their work to scale. A DE team also exists, but they serve the company as a whole. In terms of maturity and tech stack, they are in the process of migrating data from on-prem to the cloud . 
For myself, I've been an analyst within the scope of BI at my current organization and I've touched everything from data modeling, developing ETL/ELT processes, stakeholder management, and building dashboards. I'm looking to jump into an engineering role since that's what I enjoy the most.
Has anyone been in a position of building new infrastructure from the ground up? Do you think it was more helpful in your learning and career progression, as opposed to a company that already had a mature and up-to-date data stack? For example, a number of other companies I'm interviewing with are already fully on the cloud and are using tools like dbt. The benefits I personally see are being a ""pioneer"", setting up/introducing the new infrastructure, and understanding the data lineage better.",Career,dataengineering,scx9kd
"About me: 
- _NUMBER_ years in academic-style roles in economics. Experience using R and Python to build datasets for econometrics. 
- Enrolled in Masters of Data Science degree part-time. Most of the classes I’ve taken are DE courses .
- Looking to move into more more DE-esque roles but worried that I don’t have the correct “formal” training. 
About the Role
- Mid-size firm specializing in financial analysis for small financial firms.role is on a small team of _NUMBER_-_NUMBER_ data analysts.
- Job title is “data analyst” but tasks mainly involve moving data submitted by clients into their on-prem data warehouse. “Side-projects” to automate this system and eventually move everything to cloud in a year or two. 
- job would be to bring “technical skills” to the team and help them make this transition.
My thoughts/ concerns
- I don’t have any “formal” DE experience, what I do have is mostly self-taught. How should I think about continuing this self-education in a role like this? 
- Is it more important to get “hands-on” experience, even though it’s outside of a modern DE solution? Or, should I focus more on trying to get into a role with more established De practices?
- Pay seems mediocre for this type of task .",Career,dataengineering,sc5f7d
"This will be long so I'll TL;DR at the bottom. 
I am trying to get some outside perspective on my career from the sub. I have been feeling the pressure of my position and it has been triggering a flight or fight response from me. Half of me says to dig in and learn, and another part of me says to get out asap and find a new path. 
For starters I work for the fed. I began my career in mid _NUMBER_, shortly after graduating college. I was hired into a rotational management program that lasted _NUMBER_ months. After completing the program I took my first official position within marketing but in a reporting role. This was my first exposure to SQL. The position had great potential for me to learn how to manipulate data for reporting purposes under a great boss, but that never happened. A reorg later and I was basically a report jockey. I would run some basic stuff and generate some pre made reports and then send out emails. It was extremely easy, but I was bored and wanted to get into a job with real skills... 
That led to me getting a job for the data warehousing org. I was not qualified by any means, and they knew that, but they were willing to bring me on. Initially they started me in one of the data marts. They told me they wanted me to work on a few things but that the primary goal was to get trained and learn development. Now there was essentially no onboarding, no training, no real guidance at all. I sat at my desk for weeks, months with nothing. I was trying to get a grasp but the world of development has so many facets and with the layer of government red tape over every process I was at a loss. After a few months and another reorg later I land on a different team within the same org. Now I am handling audits and agile PM work, no development. 
After the better part of a year of doing that I end up in a temporary assignment as a developer on a different team. Now this is related to the first position I had but very different in terms of what tools they used and the processes they used to promote the code changes. However I have a manager who knows my situation and was great with working with me and giving me projects I could do early on. I wasn't treated as a full on dev since the position was temporary but I was promoting code and learning. 
Well that lasted a handful of months and fell off to not having much work. Well what do you know, another reorg, a big one. Our whole organization was wiped and rebranded, a full on top down reorg. I was not selected early on to lateral into the equivalent position in the new org. 
From March _NUMBER_ - Sept _NUMBER_ I was unassigned. I still had to sign on, join some meetings, but I had no work, and nothing to do. The manager of my team was also in the same position so he was checked out. Everything was leading to me being laid off in October. At this point I haven't really had much contact with my previous team in a while. I wasn't technically on their team, and they had new managers, I was on an island trying to find a job. 
In the last round of interviews before being laid off, I got several interviews. Before I could even attend them all, I get a call from the new head of the new version of our organization. No interview, just says you got the job if you want it, but I need to know within a few hours. So right then I have to decide to stay in the role, but take the new position which ended up being a promotion for me, or risk it on the interview I had just done and the _NUMBER_ more that week. I ended up accepting the position because I found it interesting and didn't want to change AGAIN. 
I have now been on this team since Oct _NUMBER_. I have roughly _NUMBER_ months of actual dev work and I am in a full on data engineer role. I have inherited _NUMBER_ applications that originated in the early 2000s and have also been through reorgs and conversions and I am on a _NUMBER_/_NUMBER_ on call rotation every month or so. There have been several major changes through out the reorg process to how code is promoted and there is very poor/ little onboarding or training material. The manager I was working with previously did not get rehired into the same role, a new manager with no experience in what we do is now the manager. 
This is so long but typing this has been cathartic. 
TL;DR - I am not qualified for the role I am in. I have essentially no support from team members or my manager any more and I don't have much means of training at work. I feel like I am just waiting for something to fail or a project to come to me and blow up in my face. I find this field interesting, but I just don't know if I can get there. Part of me says to dig in and try but another part of me says I'm too far behind and I can't ever be great at this job. I don't even know how to begin to be good at the job. 
For those that are interested in what tools I am using... 
Ab Initio, UNIX  , SQL  some other things too but that is the primary ETL stuff.",Career,dataengineering,s1he6t
"I'm a data engineer with _NUMBER_ years of experience in public sector primarily working with Microsoft stack . Been looking for jobs and got these two offers
_NUMBER_) Senior Data Engineer
 In a greenfield startup, that has been hiring a lot and had a number of acquisitions over the past few years. Job sounds very technical and in a small team which i feel would be a great challenge. 
_NUMBER_) Enterprise Data Architect 
In a large established organization, building data model through low code from CRM solutions but involves a lot of comms with loads of stakeholders, and setting up the general data strategy for organization. I felt it would be a great opportunity to learn the other side of things sort of related to data engineering. 
Main question I guess how to evaluate which offer to pick considering in future Id want to have a bit more of leadership role but still be involved with creating technical solutions :/",Career,dataengineering,se0mk7
"Hi everyone,
I've been at my current company  for around _NUMBER_ months and it's pretty fun - although it's more analytical engineering than core data engineering I'm still learning a lot and gaining domain knowledge. 
However, because I worked at an early adopter of the MS Azure stack I get daily job offers in the region of _NUMBER_-90k - this is substantially higher than what I'm on, for reference _NUMBER_ years ago I was working as an ops analyst earning £37k.
I wanted to ask has anyone jumped shipped due to the crazy market demand and taken on the salary increase.",Career,dataengineering,s24yoe
"I have started my career as a Data Engineer and currently working on Informatica Powercenter ETL tool and a lot of SQL.
I got aware that this ETL tool has no future and now i want to shift to other roles in the DATA domain.
By surfing the net ,i found out that Data Engineers mostly work on Bigdata stuff like Spark, Hadoop, Kafka etc. or they work on cloud stuff like Azure , AWS etc.
Experts in this community please help me to chose between Bigdata or cloud based on its future, number of opportunities etc. and please lay out a roadmap to follow from scratch.",Career,dataengineering,s5kc29
"Hi,
I have around _NUMBER_ yrs of experience into software developer + data engineering. I am pretty interested in blockchain and would like to transition to that domain. How easy or difficult it is to transition at this time? Are there any work involved of using blockchains in big data?",Career,dataengineering,s1yy9j
"Hi, I m an experienced BI Data Analyst with around _NUMBER_ years of experience in Reporting and writing SQL queries. I have recently completed my masters in CS in Europe with majors in  as well. But Lately, I m interested in Cloud DevOps Engineering and even cleared AWS SA certification. Is it a good move to shift to DevOps/cloud ? or should I concentrate on Data Engineering? my only concern for Data Engineering even while applying for jobs is half of DE jobs are based on tools like ADF, Informatica,SSIS and remaining have good job openings like SQL,Spark, Python, Airflow, etc. Despite all that DEVOPS ENGINEER were paid more than Bigdata Engineers and has a high volume of openings too, To add on that DevOps engineers have the same tech stack everywhere unlike Data Engineers.
Is this a good choice moving from BI data analyst to Devops cloud Engineer ? Does hiring manager will even consider some one like me for Devops positions ?",Career,dataengineering,sbuzdq
"My current job is offering me a boot camp to either learn ab initio or azure  in for the next months. Do you have any opinion in what is the best option in terms of carreer development? 
PD: ATM I don't have any prior experience in either tool and I can only select one option",Career,dataengineering,s3k2np
"Hi, I'm a Sr. Data Engineer with ~_NUMBER_ years of experience looking to transition into the realm of Data Architecture. I've found passion in building scalable systems and cloud architectural components that support data engineers, data scientists and MLEs perform their duties better. 
How do I make an active transition into Architecture driven roles? Data architect roles are sparse, with not many options for less than _NUMBER_ YOE. What kind of jobs do I look for and how do I let recruiters reaching out to me for DE roles that I'm more interested in architecture? 
Would love to hear the community's advice!",Career,dataengineering,s0x928
"company I work for does all of their work manually. for instance if a price of one item changes, then every item that is related to this one item also needs to change. it is basically a large network of products and prices and orders change constantly. they have a person dedicated to typing in and updating new prices into spreadsheets manually and calculating the new price.
What I was thinking of doing was creating a simple Microsoft Access database that will have connect all of these different price listings in such a way that once the price of one item changes, the prices for all other items change accordingly and automatically.
Since this isn't part of my work responsibility I would have to do this additionally once Im done with my work and Im wondering about how to charge for this.",Career,dataengineering,s5hzs2
"Hey Everyone 
What is the best Job board for data engineering?",Career,dataengineering,sa9m13
"Personally, I am finding myself less interested in the business and more interested in the technology and data. Is that a similar motivation or perspective for those who switched out of DA/BI?",Career,dataengineering,s04c4i
"I am joining a company as an “associate data engineer”. Would it be wrong for me to put “data engineer” for the job title on LinkedIn? Which title would you put and why?
View Poll _URL_",Career,dataengineering,s56arg
"I'm a DE/DA with <1YoE. Although I'm on a team, all our roles are very independent and responsibilities change week-to-week . My work involves automating current processes, making reports/dashboards, implementing/improving ETL. Most of it is ad-hoc requests or fixing bugs.
Manager has been satisfied, and only suggestions for growth have been quite generic: ""take ownership of more projects"", ""think about where you can help"". I like not being micromanaged, but sometimes think I could use a bit more direction.
_NUMBER_. Any tips for growing at a small company? Is it possible?
_NUMBER_. What skills should a DE aim to develop, or what projects should one have experience with, within their first year?",Career,dataengineering,s6iyeh
"I recently got an interview with one of the Big _NUMBER_  for a data engineer job. Just curious if anyone worked for them before?
What was the work like and how was your interview? 
Thanks in advance.",Career,dataengineering,s51d6t
"After being confused for multiple years, I've finally decided that a career in data science & engineering is right for me. I just wanted to know which tech companies have good data engineering teams so I can align my job search accordingly.",Career,dataengineering,s7n0bw
"Reaching out to my lovely community in Ontario and Canada to understand how much comp adjustment should one expect and in particular how much raise should one expect for _NUMBER_-_NUMBER_?
As an architect in the data engineering, what should be the expected salary range for a mid senior level position.
Any information any existing resources would be helpful.",Career,dataengineering,sa6ucd
"Hi all,
 
I hope this is appropriate to ask. I'm transitioning over from a wholly different field and have been approached by recruiters to interview for META , but have so far been hesitant, as I have heard once you work for them it can be hard to find work again, and from what I know of FBs practices, any gig with them would be basically a quick cash grab for a year or so. 
 
Is it the case that working for META can ruin the odds of future employment in DE?",Career,dataengineering,sakz79
"Hey guys, I joined as a data engineer intern about a week ago. This is my first job straight out of college, and I have no prior experience in data engineering whatsoever. 
I was advised to learn Python and Postgres by my leader, and for now, I'll just be watching others work. Everything is new and overwhelming, and it's hard to understand the architecture. I asked all of my questions, but I still don't seem to understand it, and most of the senior leaders are always busy due to remote work.
Some of the technologies I'll be working on are S3, Lambda Functions, Elastic Search, Kibana, Kineis, and Glue. I have a conceptual idea of what they do, but overall, it's still blurry. 
If anybody could point me out to the right resources or path, it would be of great help to me.",Career,dataengineering,sbvbau
"After being confused for multiple years, I've finally decided that a career in data science & engineering is right for me. I just wanted to know which tech companies have good data engineering teams so I can align my job search accordingly. 
Edit: I'm not worried about the experience and skillset part, I've got that covered. I'm interested in knowing which companies are considered as good places to work by the data engineer community",Career,dataengineering,s7n0bw
"Can you guys drop some names for me to research? If you have opinions or experience to share, even better.
Edit: to clarify, I don't necessarily mean that they're actually ""good"" at DE , but that they specialize in it/are active in that space. Trying to learn what the DE/BI consultant landscape looks like.",Career,dataengineering,s4rtcq
What are all the primary/fundamental skills needed for a Cloud Data Engineer?,Career,dataengineering,s7tmqx
_URL_ _URL_,Career,dataengineering,safl90
"Hello Team, recently I was rejected at a retail company mentioning that I did not write optimal code that they expect from senior software engineer using OOP's concepts. Since then I have been learning OOP's and now I am not sure how do i proceed from here.
_NUMBER_. Where do i get use cases to write OOP's code.
_NUMBER_. How would i know that the code I have written is optimal
_NUMBER_. Are there any sample codes or books that gives me an example on how do i start after finishing OOPs concepts?
Also, can someone point me to some code base or book that shows how Data pipelines are build using spark? I am interested to learn how raw data gets transformed before it is put into Curated layer of Data Lake",Career,dataengineering,sbfh57
"I am currently working as a junior machine learning engineer. But the uncertainty in outcome and the requirements of learning a vast variety of skills and the long hours of time consuming training makes me want to transition into a data engineer.
I have always had some attraction to cloud  and bigdata. I just want to know if data engineer skills are concrete and is it good for me to transition in terms of career growth and salary. Please provide me some insights on this.",Career,dataengineering,sbdycg
Was hoping for this to be my first big boy job in data engineering but alas :/. Any advice?,Career,dataengineering,schsvv
"I'm on the job hunt and would love to be working with sports data. I had a look on LinkedIn and some of the top sports teams in the UK and Europe  don't have any data engineeri employees.
Do these companies go direct to the likes of AWS and Microsoft and work with their best engineers to build their solutions, or do they do work with specific consultancies?
Any tips for trying to get into the sports industry would be greatly appreciated.",Career,dataengineering,s27npk
"Hey everyone! I started my data engineering adventure for about two years now, when I got the offer to join this new department into the same company I was working in a position of a database administrator. During all this time I researched about what data engineer role is assuming, but I feel that I cannot find by my own a good learning path to become better in this field. The company is moving slow in this direction, and the department is not involved into many projects that can help me develop my skills.
Is anyone of you interested to share with me his/her experience as a data engineer, develop new projects to learn and get new skills together?",Career,dataengineering,s6yhkn
"Hey there,
I work at a smallish company in the ecom business. I use azure synapse and power bi for all of my reporting needs.
Originally, I had used dataflows in power bi to report on data loaded from a traditional sql database, but as our datasets have grown we've quickly outscaled that solution as any report over 1gb failed to refresh in Microsoft's BI service appropriately. Scaling was a nightmare and I quickly asked sales to cool it on this feature. 
Today, I load data from various sources, then generate KPIs and dimensions in synapse to load into power bi. This makes the refresh times much quicker as I am refreshing data already prepared and aggregated. This also means I can scale my solution out to multiple clients with relative ease.
One thing to note is i do most of my work through the synapse UI. I create all of our dataflows and pipelines through the synapse UI. It is very 'low code' and I'm not sure if this is optimal or even how to make it a better solution. 
When I look at posts on here, everyone is honing their craft with Python notebooks or using some programming solution to get it done. 
My questions to my fellow redditors:
Is it even possible to get a data engineering job with some SQL, BI and azure synapse knowledge? I've built our data warehouse from scratch, but I worry because I'm learning to float and I'm being asked to swim. I don't really have a mentor and I'm kinda making it up as I go.
What are some good resources to help up my game and give me the best opportunity possible to build cool things in this space? As I said, I am building everything from scratch myself. I am being asked to lead this front and I have no clue what I'm doing. I'm desperate for resources to learn from.
Thanks in advance",Career,dataengineering,s2oev1
"Hey Everyone! I’m interested in pursuing a career in data engineering and am currently in a dual degree master's program for healthcare and information science. I’m starting to take some courses in the Info science degree and was wondering where my focus should be. 
I am currently enrolled in a database systems course and will likely be taking a sequence of _NUMBER_ classes in databases systems that will cover design, implementation, and SQL. 
I will also be taking ~_NUMBER_ courses in python programming throughout the program . 
Other than those two conceptual areas, what other things should I focus on to maximize my educational opportunities and foundation to shift my focus into data engineering?",Career,dataengineering,s30v79
"I have just been asked by the lead of the DE team I just joined to start learning Bamboo. I love learning new tech, but would like to inquire with the sub if this skill will be useful in future DE job hunts.",Career,dataengineering,sbn6n4
"I moved from a product analyst role to a data engineering role. At first I had tons of work and was so busy. Now that most of the pipelines have been built I feel like there isn’t much for me to do. I am a bit worried that I am no longer providing value to the company, but because I have less than one year in my current role I don’t think I can change jobs easily, nor do I want to. I feel like I have to figure out what to build next, but the business doesn’t seem to have the demand for it. Has anyone else experienced this?",Career,dataengineering,sdhqna
"Hi, 
I'm looking to move to AU and I was wondering how is the data engineering landscape in Australia? In term of opportunities and salary. 
I am a medior DE with _NUMBER_ year of experience, have been working mainly with Spark, Airflow, DBT, Containers on GCP and Azure . 
How hard is to find a company to sponsor your visa? 
Thanks",Career,dataengineering,sdh63a
"I was wondering if some can explain the difference between the roles to give me a better idea about both fields. Also, which one would be better career wise. Thank you!",Career,dataengineering,sdh98x
"Edit: Amsterdam 
Hi all, have an interview coming up and was wondering what sort of range I should mention if they ask for salary. I don’t particularly care because I just want a foot in the door, but it’s probably not wise to say ‘I don’t care what you pay me I just want to work in DE’. So what range is normal for fresh graduate junior DE positions in western/Central Europe in a big city. I don’t expect much but it’s probably good to go in with an idea.",Career,dataengineering,sar8nq
"context: although as a data analyst I have requirements but often it’s show us insights etc which is creative... For lack of a better word, really 
Do you find your de job interesting? I’m really debating btw de or ds, unsure. I enjoy coding . I don’t love reading research papers. Any and all comments appreciated.",Career,dataengineering,schh5e
"Hi everyone. Just looking for some advice about the current situation I’m in. 
I’m _NUMBER_ and graduated uni with a BS in CS this past May. I’ve been working in IT straight out of college and been wanting to get a job in data, so I applied for a junior ETL developer at a startup. I got a call from the CEO two days later telling me he wants to hire me on the spot without any formal interviews. I thought this was great since I have no experience in ETL development or anything in this field at all, and thought this would be a great start. 
The current job I’m at now pays me well. At $_NUMBER_/hr, it’s pretty good for a consultant . I do get the usual health/dental/vision/401k benefits but I don’t get holidays or PTO. If I don’t work, I just don’t get money. Also, I’m on-site _NUMBER_ days a week with no remote option. The big downside is that I do not see myself growing in this field at all. I don’t do anything data related where I can perform SQL queries or code some Python or R. 
With this ETL position, it’s gonna drop me down to $_NUMBER_/hr but the CEO was adamant that I will be learning good real world tools, software, and programming if I want to get into data engineering later . It’s remote but the only benefits it looks like I’m getting is PTO. 
This is where I’m having this weird feeling in my gut. I asked him that the job board listed $_NUMBER_-$_NUMBER_/hr and I would like to meet in the middle - so around the same pay I’m getting at $_NUMBER_-ish/hr. He started growing frustrated  when I asked for that pay. He said he’ll have to see how my performance is and he’ll bump my pay up, and proceeds to say his company is the only company that will give me this kind of experience and further my skills for any future careers I might get in data engineering
He also proceeds to say “people negotiating their pay with me turns me off when I have people in India, or people without experience at all taking that offer in a heartbeat.” Afterwards, he says “this job is the real sh*t and the sh*t we do will give you the experience you need. As long as you prove to me that you can put in the _NUMBER_+ hours work, you’ll get the pay bump”. 
Are all these red flags, or am I just being too greedy with the pay?
Thank you in advance! I’d love to hear your thoughts on this.
EDIT: I also want to note that I am currently going through a Data Engineering bootcamp and am willing to put in time and learn in this field. I just don't know if leaving my job for this will be worth it, and I'm worried I'll be taken advantage of. I do really want experience but don't know if I should start with a company and CEO like this.",Career,dataengineering,s9q178
"Hey, I'm a junior data engineer. 4ish months in the job  and I've been asked if I want to learn Ab Initio as one of the tech leads handles it almost single handedly. I initially said yeah, sure. Never heard of it and thought it'd be a good personal development opportunity to add a string to my bow alongside using scala and spark.
However, after doing a bit of research online I'm not so sure anymore. Is it a dead tool everyone is trying to get off of? The company I worked for have renewed it as they aren't ready to get off it after trying but the financial cost of it is something they would rather do without.
I know I'm starting to rant a bit here but should I try and back out of this as it isn't of much use to my career or is it worth learning alongside other stuff for a broader perspective?",Career,dataengineering,sfttyh
"Hey everyone, need your help on evaluating my resume.
I recently switched to a job which is more of ops than data engineering. But I want go back to being a DE. Have experience in designing and developing batch and streaming apps 
But since I made a few short switches recently, I guess my resume looks bad  and my applications are getting rejected even through referrals.
Though I am skeptical on this because I have seen many people moving to faang with more frequent switches. 
So I believe there is something wrong in my resume, please take a look and share your suggestions on what needs to be added/removed to make it better for job searches. Would be very helpful and I'd greatly appreciate it.
_NUMBER_. I had intentionally removed most of my previous orgs' experience because it would make my resume longer .
_NUMBER_. And I removed my education details and redacted the recent org I work with.
Thanks everyone.
resume link _URL_
P.S. I have a bachelor’s degree in computer science. Seems I have removed that information instead of redacting it.",Career,dataengineering,s1gezp
"DataOps community virtual talks _URL_ _URL_
Practitioners from Zillow, Google, Slack, Babylon, Squarespace, Volta, Unravel, Akamai, EasyPost, Baker Hughes, IBM, Cisco, Wheels Up, DBS, Capital One, Wistia, AWS, J&J all doing sessions. 
Would appreciate the support of this peer-driven community.",Career,dataengineering,sfoqkg
"Hi, I'm 32M and currently live in the UK  and it's about _NUMBER_ months I'm looking for job, I'm new immigrant here but have no visa problem to _NUMBER_, I was a .Net back-end freelancer developer for about _NUMBER_-_NUMBER_ years design systems and databases, built web APIs, Windows applications, and I'm familiar with python, wide ranges of databases such as SqlServer, MySql, PostgreSql, SQLite and also noSqls, recently  I've started to learn about Data Engineering and started to learn about ETL and cloud base databases. 
so here is the problem, I started to apply for paid internships or junior roles on LinkedIn, Reed, glassdoor almost everywhere, but the companies reject my application sometimes in under _NUMBER_ minutes. 
could this community give me some advice about what should I do?",Career,dataengineering,s0n04a
"I'm starting my first DE job next month, just wanted to know what your day to day was like? How do you get work, what do you use to get your job done and any advice you might have, thanks!",Career,dataengineering,s1vuip
"Hi guys,
In around a week or two I'm gonna have a technical interview for Junior DWH Engineer. I'd like to ask you which topics I should have covered by then from **The Data Warehouse Toolkit** by Kimball?
* the job is for a junior 
* I have a job of a Data Analyst right now
Thanks for your help.",Interview,dataengineering,sdu1bx
"I've been asked this question in almost every interview i've attended. I'm a Software Engineer looking to transition to a DE. I understand the concepts of Spark and Hadoop and have done several projects using them, but lack practical exposure on how to actually deploy spark jobs/applications in production. I can only explain how I've got it running on my system using containers, but they expected me to explain how I'd do it on a cluster.
Can anyone share some insight on how you deploy to production or are there any resources that can explain how i can implement or learn this.",Interview,dataengineering,s9cjww
"Soon I have an interview at one of the Faang companies, I have been preparing at leetcode and feel pretty confident however since I have a friend with whom I have worked previously and knows me , was thinking if I should let my recruiter know, will this help??",Interview,dataengineering,s4dbuz
Why can't analytics be done on top of backend database?,Interview,dataengineering,s6wazj
Failed coding portion at a faang interview. Although I knew how to code the problem given I froze during the interview.. Are all interviews like this?,Interview,dataengineering,s91atx
"I had an amazing conversation with Vinoth about data lake and lakehouse technologies. He gave one of the clearest explanations of what a lakehouse is. Vinoth is an amazing guy and has crazy experience in large scale data systems and he was kind enough to share part of this experience with me.
You can listen to the conversation here: Data warehouses, data lakes, lakehouses and large scale data systems with Vinoth from Apache Hudi. _URL_",Interview,dataengineering,s2e9ct
"I have a SQL, Python, PySpark test on Devskiller next week. Has anyone have had experience giving test on Devskiller?
I'm nervous.",Interview,dataengineering,s9fhiw
"So I was given a take home assignment for an interview for a new job that is to create a Python library that basically takes a pretty simple csv, runs some basic tests on it, transforms it a little including separating it into different tables, then loads it into a sql db. Included should be tests, explanations on how to run, and pseudo deployment code. It’s taking me a little longer than I expected for a take home assignment and part of that is just distraction etc but I guess I’m just curious as to how long it would take others to do something similar.",Interview,dataengineering,sgny62
"Sorry if this has been asked before multiple times but I wanted to include the job specifications in my question. I'm applying for a DE internship with a sports team and I'm preparing for the second interview that will include live coding.
The main project during the length of the internship involves taking JSON files, doing transformations to the file, and sending it to the database, and it would be done in with a Python data pipeline project.
I reached out to a member of the team and in his words 
> I plan on it being one more traditional “blackboard” question and then one higher level pseudocode type question with database concepts that I’m just more curious on your thought process on. While I don’t want to give away too many specifics, the overarching goal of that part of the interview is just to make sure you have experience to do the technical part of the job.
Skills required from the job listing _URL_
Any help would be greatly appreciated!",Interview,dataengineering,sgmrrg
"Interesting episode I thought I’d share that discusses big data, and why so many business are hoarding their information. 
_URL_
Description copy and pasted below:
Big data is a big deal! Today, I was glad to welcome Viktor Mayer-Schonbergeroday on the show to discuss how impactful data information and security is, along with how our mental frames change the world.
Bio: Viktor Mayer-Schonberger is the Professor of Internet Governance and Regulation at Oxford. His research focuses on the role of information in a networked economy. Earlier he spent ten years on the faculty of Harvard's Kennedy School of Government.
He has published eleven books, including the international bestseller ""Big Data"", ""Learning with Big Data"", and the awards-winning ""Delete: The Virtue of Forgetting in the Digital Age"" with Princeton University Press. He is the author of over a hundred articles and book chapters on the economics and governance of information.
In _NUMBER_ he founded Ikarus Software, a company focusing on data security and developed the Virus Utilities, which became the best-selling Austrian software product. He was voted Top-_NUMBER_ Software Entrepreneur in Austria in _NUMBER_ and Person of the Year for the State of Salzburg in _NUMBER_. He has chaired the Rueschlikon Conference on Information Policy in the New Economy and in _NUMBER_ he received a World Technology Award in the law category for his work.",Interview,dataengineering,sd6g5k
Has anyone interviewed for RedHat Data Engineering Intern role? Could you share your experience and how to prepare? Thank you!,Interview,dataengineering,s20gtf
"so..we use git @ my current place of employment and i am starting to look for a new job. 
how should i go about listing it on my resume? 
does ""utilized git for ci/cd"" sound too generic? i wasnt the one who implemented it, i just use it so i cant say that it was me who decided to put this in place.",Interview,dataengineering,s3j1kv
"I had a question from a data hiring manager today who asked “ What is single source of truth ? How do you maintain it”?
I am curious to how you would approach the question. I know they use dbt. Does that change anything ? I had difficulty answering because I work for a company and we are about _NUMBER_ people so it’s easier to maintain. The tech company I was interviewing for was size of _NUMBER_-_NUMBER_.",Interview,dataengineering,s6ool2
"I'm in the midwest, and not moving. >_NUMBER_ years of experience, and currently work remote for an F50 company. Recently, I've been able to get multiple offers remote from big and small companies, but they're not the famous FAANGs that have onsites and tough leetcode-style technicals. 
I've been upfront with all my recruiters and HMs about my need to be remote. I bring it up in my first conversation. No need to waste anyone's time. Is this the right approach for FAANG? or should I go through the whole process, pass all the cycles, then make that request? I ask because I understand these companies often interview people before they're even considered for a specific role/team.
I also know that Facebook is the most remote friendly. Amazon has people working here, but I assume that's because of their infrastructure. Ohio is one of the AWS Regions",Interview,dataengineering,sadjin
"Hi Data Engineering Community,
I am writing this to seek guidance on how I should go about preparing from leetcode for DS/Algo rounds for DE Interviews. I am currently working as a data engineer but planning to switch in next _NUMBER_-_NUMBER_ months . In my day to day job I write a lot of sql and spark pipelines in pyspark , but DS/Algo is something that I am not it touch with. 
l want to understand 
What topics in DS/Algo should I practice from leetcode and how many questions should be good enough? 
There are just so many questions on leetcode , I want to prepare a collection of problems for practice.Can some one who has prepared for DE interviews from leetcode , please guide me",Interview,dataengineering,s7lffn
From my research it seems like DE interviews are some of the hardest out there because you are just required to know everything-- your standard leetcode style questions plus SQL questions plus system design questions. Is this correct? Can you give me some examples of interviews you've had  and what questions they asked for a DE role?,Interview,dataengineering,sdg1rr
"What should I expect? I was told it is going to be a SQL, some python scripting and Datawarehousing logic questions. .
How many questions will I get and how many should I get right? Thanks",Interview,dataengineering,s15n8b
"Currently interviewing for an AE role and made the final round.
Prior to this stage, I completed a take-home challenge that was basically two components:
* Design a data warehouse  that answers business requirements. I also had to make a write-up that justified my design choices and detailed the relationships. There was no code involved in this step. This took up the most time of the challenge for me.
* Based on the same dataset, create an ETL that fulfills another business requirement. Then, query from it to calculate some KPIs and answer some business questions.
What can I expect the final round to be like? I will be presenting my take-home challenge to a couple of engineers, which will be followed by another meeting with a PM and hiring manager . 
Any tips, or things that I should watch out/prep for? I would appreciate anyone who can offer some insights and help!",Interview,dataengineering,s9iqw5
"I am looking for a Microsoft solution to the following requirements
* Orchestrate R scripts that perform ETL processes
* Fully Cloud based
* Small scale, low budget
* Needs to manage different R versions and package versions
Our data pipeline using R scripts currently extracts data from our Azure SQL database, and other cloud based applications using API's. Transforms the data then places it back in our SQL databases.
I have had a look at databricks, however i don't think this is appropriate for us, as we'd need to rewrite our scripts using rSpark. I would like to maintain our scripts, and just upload and version manage in the cloud.
Is Data Factory an appropriate solution? Can you manage R versions using Jupiter notebooks? Can you execute these within Data Factory? This may not make sense - i am still trying to understand the environments.",Discussion,dataengineering,sgolt0
I installed the gratis Wolfram Engine to play with because I watch the Wolfram channel a lot. I don't remember it getting listed in most of the usage statistics. Is it a really niche thing?,Discussion,dataengineering,sexv8n
"Hoping to learn how to make Airflow ""self-service"" for Data Scientists or ML Engineers.
Does anyone have examples they can share on using a framework for the DS/ML teams to create/submit models for production?
Also, would be interested to hear if people are using Airflow and something else like MLflow, Kubeflow, etc to fit DS/ML Ops.",Discussion,dataengineering,s3vzkg
"I created a docker image to pick some data from an API and then store it in our data lake, but as an intermediate step, it stores and processes the data ""locally"" before uploading. In this particular case the data is not really sensitive, but it still made me think - **is the data stored in a docker container secure?** is it immediately destroyed after the container is done? can it be accessed by the service I'm running it on, Azure Container Registry in this case? Or persist after the container is done?
In case of processing PII, are there any considerations when processing and storing the data even temporarily in a container?",Discussion,dataengineering,s2dw46
"I'm studying for Data Engineering role, and trying to setup pipelines in my own job for experience.
One of my jobs is to take **time writing data**  and **invoice data**  and use them to generate a new report.
Here's the basic steps:
_NUMBER_. Finance team send me these reports each month, or every couple of months. 
_NUMBER_. I manually restructure them as they aren't always in a consistent format. This doesn't take long, as there's only _NUMBER_ or _NUMBER_ columns in each report that are relevant to me. But it's still manual work.
_NUMBER_. I run a python pandas script to join the reports, run some fancy calculations, and output a new report. This new report lets finance know how much money to charge each project in the business
_NUMBER_. Send the reports back to finance.
_NUMBER_. I do this for each month.
I guess for the time writing data, I could find out where these are coming from, and setup something that would extract the reports each month to a repository of my own.
For the invoice data, I'm not so sure. As far as I know, Global IT upload these to a private Sharepoint, and then the Finance team lift the relevant information, before sending to me.
I'm just wondering how a real data engineer would actually approach this, and whether it's worth me trying to fully automate it, or just stick to the way I'm doing it.",Discussion,dataengineering,scaifd
"Let me preface this by first making clear I have no connection whatsoever to Dagster or Elementl or anyone involved in the project. In fact, I hadn't even heard of it until very recently. But this shit is unreal, so far it seems like one of the most well thought out tools I've ever had the pleasure of working with. It seems like every feature I think ""that would be pretty cool to have"", they've done it, and beautifully. It makes Airflow feel like this clunky monster that you have to work around, instead of working with. And I think Airflow is pretty cool. I just feel so less limited by Dagster's model. Not to mention their huge focus on testability of pipelines is a complete shift from frameworks where testing is often a  afterthought. I get that adoption is and probably will be slow with Airflow as competition , but Dagster honestly feels like something that if adopted could fundamentally change how people think about building data pipelines. If Dagster was selling stock , I'd be buying.
All this high praise should come with the acknowledgement that the project is young, the community is small, and documentation, while good on the surface, gets pretty scant when you start customizing a deployment. These things come with time. And I'm hopeful their eventual managed service will solve some of the infrastructure complexities. 
This brings me to a question - do others have any experience running Dagster in production? Any issues at scale? Are there any specific shortcomings or issues you've had that would be good to be aware of before diving in?",Discussion,dataengineering,s71ao7
"I created a data streaming pipeline using AWS Kinesis. I write data to a Kinesis data stream which triggers a Lambda every _NUMBER_ minute or after _NUMBER_ records - whichever comes first. The Lambda processes the semi-structured data and inserts it into a MySQL RDS database. I also have a Kinesis Firehose that inserts the raw  data into an S3 bucket. I also created a real-time data analytics application to analyze the streaming data 
Link to Medium article  _URL_
Link to GitHub repo with the code is included in the article.
I am however curious whether that approach is the best way to go. So, would like to discuss
_NUMBER_. The best tools available for real-time streaming of data from IoT devices 
_NUMBER_. Which tech stack are you guys using for data streaming
_NUMBER_. Review and suggest improvements
Thanks",Discussion,dataengineering,s5gh8n
"I am working on building a ETL Data-pipeline and wanted to ask what is the best practice when it comes to extracting and loading data. 
Scenario - both sql servers are on premise practically next to each other. 
Is it better to 
Read from sql—> Write to csv —> Load to sql 
Or 
Read from sql using pandas —> directly load to sql. No csv generated. 
Both work for me. Wanted to see what is the best practice ?",Discussion,dataengineering,shiifd
"Has anyone ever used Azure Databricks  for their ETL? What was your experience in terms of performance?
Context:
We use ADF to bring data into Delta lake, then run numerous transformations using spark SQL, scala, and Python UDF's. 
Goal is NRT data  for our clients.
Fastest we've managed to do this so far is _NUMBER_ mins.",Discussion,dataengineering,s69nfz
"Hi all,
I'm a pretty seasoned data engineer  but I have a new use case. I am working at a company thats product will be exporting large swaths of data to clients externally. Traditionally I've built DWs that plug into BI tools or can serve simple exports, but I'm wondering about scaling of exports. If I need to serve 100GB a day to external clients, what is the ultimate tool to do that with?
Env:
We are using AWS, our raw and stg layers are in parquet files, and our DW will be in some sort of DB 
Do I use spark to build exports via parquet files? Do I put in DW and query and send out from there? What are thoughts?
I know using something like Aurora or Snowflake to export will be very expensive as it will be cloud egress but idk how that holds up to cost of just running exports off data lake.
Thoughts?",Discussion,dataengineering,s0omwg
"Folks, if you are using databricks, can you please describe:
_NUMBER_. Stack - what are you using for orchestration, managing dependencies, storage, transformations, business intelligence, data science. 
_NUMBER_. What are the major use cases you are using this stack for?
_NUMBER_. Are you using anything for managing the metadata?
_NUMBER_. Do you have a data warehouse as well in addition to the databricks stack?",Discussion,dataengineering,sdpz1y
"So a lot of Kimball's principles like strict normalisation and star schemas/snowflake are being replaced by wide tables, which take advantage of cheap storage/columnar querying and are faster to query compared to multiple joins. However, over time wouldn't wide tables be a disaster if certain dimensions  get crammed into the the wide tables only for them to later have to change? How do you guys decide between the efficiency and speed of wide tables vs the strict data validation and robustness of a star schema setup? Or are there cases where you build ur base data architecture as a star schema setup and then create views/materialized views which are in the form of a wide table at the presentation layer?",Discussion,dataengineering,s1668j
"How do you handle data quality issues in data engineering pipelines.
For example in our use case we run spark scheduled jobs every _NUMBER_ minutes from S3 to Hudi  tables and jobs that load data from Hudi tables to Redshift fact tables . we do basic data checks before landing to fact tables. 
But our plan is to implement a frame work that can check data quality and take action based on pre configured rules. 
_NUMBER_) what are general design pattern for data quality in such cases 
_NUMBER_) how does it impact on the data latencies . How to minimise. We can also check and correct data after the data is loaded as a secondary batch process.
_NUMBER_) are there opensource data quality tools/ framework that can work in these scenarios ?",Discussion,dataengineering,s594ms
"Suppose we have a very big table that is being pushed to S3 . It is greater than _NUMBER_ TB, now changes to the table are being captured using CDC. 
Now we want to read the actual state of data using the read query, which will require merging the big table with the CDC. 
We can't have a job every _NUMBER_ mins that will be merging the whole data. This would be very expensive. 
Also, we can't merge the data for every read query.
What should be the solution in this case?",Discussion,dataengineering,s7nc34
"Background: So our data pipelines load data, update history, enrich it and then we come to transformations before actually generating the reporting table.
The data we have is revenue data coming from multiple sources mostly apis. In transformations, one transformation is to identify the type of revenue which requires manual input from another team.
So the manual process goes like this:
- takes a set of unidentified transaction rows .
- puts them in a separate table.
- this table is shared with other team who manually set the type column  of each unidentified row.
- then this table along with original table are joined together to generate reports making sure there are no duplicates and type of unidentified rows are now defined.
Ideally I know identification should be automated, which is already under development but not fully in place. So the manual process of identifying transaction cannot be replaced at the moment.
Question:
- Is it the most optimal way to do things?
- If you do something like this, do u have better suggestions?
- In business analytics, what search term should I use for this reading about this kind of business requirement.
Thanks for all the help.",Discussion,dataengineering,sbr4in
"At Airbyte, we just raised a $150M series-B. We also openly shared our investor deck and pitch.
_URL_
In our deck, we share how being open-source and community-powered is key for our growth. We want to solve the long tail of integrations problem with our participative model, remain a non-opinionated ELT tool to address everyone's needs, and provide a fair compute-based pricing in Airbyte Cloud.
We are curious to know what the data engineering community thinks about our strategy moving forward? What are you most excited about? What are we missing?",Discussion,dataengineering,s2bgsp
"I believe that companies are waking up to the fact that the time of Data Engineers is better spent on creating assets and building pipelines, not maintaining a dimensional model or optimizing a SQL query. There are many cloud products  that have optimization algorithms that can outperform any human. I think that dimensional models like star schemas, are going the way of the cube. Soon to be relegated to outdated technology. Cloud compute has gotten so cheap, and cloud storage has gotten so cheap, that the cost/benefit analysis is now incredibly obvious: spend more on good engineers, spend more on cloud compute, and get more value per dollar.
I explain more about how Google is implementing this strategy effectively if you want to read more, but the bulk of my thoughts are above \^ Learn from Google's Data Engineers: Don't Optimize Your SQL _URL_
What are your thoughts? I know this is kind of controversial, because so many people are proud of their learnings in optimizing queries.",Discussion,dataengineering,se0mz6
"Hi All,
What can be good ways to earn some decent money with data engineering skills as part of second source of Income.",Discussion,dataengineering,s55ilu
"We're trying to pick the right tools to build a near real time analytics data pipeline. We have about 60TB of data stored with around _NUMBER_ million new records a day. We're on DB2 and the queries take hours to run . We have a private cloud with open source options available if we want to customize. We're also looking into Azure and Snowflake as alternative in the public cloud. Curious what you think a good stack of tools looks like to deliver this kind of volume in near real time. We think delivery will be either through Tableau, Power BI, and/or APIs.",Discussion,dataengineering,s6xm81
"Hi experts,
Assuming I have a bunch of sensors sending out JSON data to Kafka, and on the other side there is a Vertica database.
Now that I'd like to make sure data coming in are of good format, and those don't fit get sent to a data lake dump for further investigation. Further assuming that I know the right schema beforehand.
I think maybe I can setup a schema registry, put up a validation layer between Kafka and Vertica, and route the rejected ones to a separate data lake, say, just a S3 bucket, and convert the good ones to avro format and load into database.
Now the thing is how would you propose to implement it? Data coming in pretty fast , and owner needs real-time streaming for monitoring purpose. I think I can pick say _NUMBER_ of them and write some tests, but I'm not sure which tool I should use for the validation. 
For example, I can definitely choose Apache Spark, read from S3 and validate one by one, and do the conversion to avro, and send to another Kafka server, which stream them to Vertica. So the pipeline looks like this: S3 -> First Kafka -> Spark apps -> _NUMBER_) bad ones to another S3, and _NUMBER_) convert good ones to avro -> Second Kafka -> Vertica.
What's your experience about this kind of application? Thanks in advance.",Discussion,dataengineering,s6dtvj
Datacamp seems to be having a neat promo now but I was hoping to get insights from anyone who has taken the course before and thinks it’s worth the time and/or money or not. There seems to be limited data engineering resources  and I can’t seem to find one that covers the most ground and puts me into consideration with interviewers. Thank you in advance!,Discussion,dataengineering,s8qkd3
"We use Stitch at my work to get data from various SaaS tools into our data warehouse, and airflow for cases where a stitch integration doesn’t exist or we want to do something more custom. 
The airflow DAGs I write follow a general pattern of 
 _NUMBER_. Create some db tables
 _NUMBER_. Make some API requests and store them in s3
_NUMBER_. Parse the requests and upsert the data into the tables in step _NUMBER_.
I just read the readme of the Singer getting started repo _URL_ and am excited to write my first tap! I’m thinking instead of writing a new Airflow DAG whenever I want to pipe API data into our data warehouse I could write a singer tap and use Stitch instead. Is that a stupid idea?
For anybody who has gone down this path, would you recommend it? Any advice?",Discussion,dataengineering,sgd6cu
"Hi Everyone, 
There is lots of discussion around internal data movement, but I'm curious what everyone is doing when it comes to external data exchange? Does anyone else have automated processes that connect partners? Either through APIs or batch files? 
I'm interested in building something in this space, and wondering what everyone does today, and what sucks about it.
Thanks!",Discussion,dataengineering,s3ukpe
"I started to wonder if it's okay to have different data models in a single database . 
For example, a database has tables. Some are built as a star schema dimensional model, whereas the rest are created as a 3NF model. 
I understand that this may not be a good practice in a business context. But I wonder if this is feasible and may be helpful in some cases.
Thank you!",Discussion,dataengineering,s4bdit
"Snowflake is promoting Streams and Tasks a lot but DBT already provides Incremental Model and Scheduling capability .
I am curious if anyone using DBT + Snowflake stack found use case for Streams and Tasks.",Discussion,dataengineering,s0jyj1
"We have a client SDK that sends logs to Elasticsearch.
Sometimes, the SDK might send a batch of millions of logs to Elastic.
I want to enrich those logs.
I want to do this after inserting and asynchronously to not block the insert.
As a solution, I thought about sending the logs also to a messaging broker.
I want other microservice to pull from the broker, enrich the logs and update Elastic.
I want the pull to be of a batch of thousands of logs to perform the enrichment faster.
I thought about using Postgres as a broker because we already use PG, it's performant, and the learning curve is zero.
We use the same PG to store all of our data.
I thought about implementing a table named \`logs\` that the SDK will insert into, and the microservice will pull a batch from it every X minutes.
I am concerned about the performance deterioration that'll happen to PG as a whole because of the large volume of reads and inserts that'll be in the \`logs\` table.
_NUMBER_. **What do you think about the idea of using Postgres, and my concern?**
_NUMBER_. **Which alternative will suit my use case better: Kafka, Redis streams, RabbitMQ, or another one?**",Discussion,dataengineering,s1hsmy
"Hi everyone, 
wanted to get your opinion on the subject on how to best manage ETLs at my current job. 
Basically we have not so bad architecture of dozens of micro services, all deployed to AWS, infrastructure managed by terraform. 
But when it comes to the ETLs I think it is very cumbersome what we basically do:
_NUMBER_. all our micro services have GraphQL API exposed internally
_NUMBER_. in order to run an ETL we fetch data from different sources 
_NUMBER_. than we enrich data with lots of different data fetched from sometimes dozens of our internal APIs, the practice here is that usually we use DataFrames for that 
Most of the time this is basically fetching and joining data - which could be easily done in any DWH or even database  but we have everything spread between dozens of postgres databases. Most of the jobs are batch ETLs. Than with that data we do something . 
So now there are two things: 
_NUMBER_. The only advantage of having this architecture  is that the schema of the API and logic behind is decoupled. 
_NUMBER_. On the other hand if we would bring the data into single DWH or something extracting data in the ETL would take few lines of SQL . 
Let me know what do you think is the way to go here.",Discussion,dataengineering,s1ies3
"Is it typical to use a DW to reduce complexity of integrations across what are essentially source applications? I.e. use cases where applications need data to function, not just a reporting use case?
E.g. Oracle and ServiceNow both consume/produce different elements of employees, departments, budget accounts, building locations etc. Currently we have a spiderweb of ad hoc integrations between many systems. The pro-DW group of folks would like to simplify this by having applications put/pull from the DW instead of many connections between source systems directly.
Some folks don't seem convinced this is a good idea and are saying it's adding complexity not removing it since we're adding more systems to the mix. But if you draw out the proposed end state of a DW based integration architecture, it would seem to be a lot simpler and easier to govern than our current approach of a spiderweb of many many vertices and edges. Am I on the wrong track?
What do you all do at your organizations? Just tons of JDBC/REST calls, or put stuff in a central model and have everyone try and use that?
Thanks!",Discussion,dataengineering,secb4r
"How do you manage your data schemas? I'm curious to find out if anyone stores their data schemas in a central version controlled repository as opposed to something like the confluent schema registry. I like the idea of a central 'data dictionary' and not having another critical service to manage and monitor and it feels like it would make it easier to maintain standards across an organisation. 
But, I have a worry that having a single dependency across our organisation, which in turn may be used for code-gen across many projects, could be problematic. The main issue I see is losing release independence; as soon as a schema change is committed it needs to be released quickly otherwise you block the whole organisation. But, with backwards compatibility rules in places and a fast release pipeline it seems workable.
Does anyone have any experience of something similar or have simply ruled it out as a crazy idea?",Discussion,dataengineering,s3trjg
"I recently joined a team that's using Scality for storage. What's the difference between Scality and AWS, and what's the advantage of using one over the other?",Discussion,dataengineering,s2ddl0
"My customers are online retailers and the business model is a subscription service where my customers pay a monthly fee for me to maintain the data analytics .
I currently see it working in AWS, where I hook up to their ERP system and bring it into my own database  on a schedule. From there I can automate analytics PDFs in Lambda and connect to Power BI to maintain their dashboards.
Is handling this many different customers  viable or is this completely impractical?",Discussion,dataengineering,s0gdc7
"For people out there running Spark on _NUMBER_ c5.2xlarge instances on EMR or Databricks -- why not just use _NUMBER_ c5.8xlarge instance? Everything will just be so much faster. Is there some benefit of using many small instances?
Edit: I see a lot of comments about finer-grain scaling benefits. This is very true. Are there any other benefits?
Wow a lot of comments! I want to give a tldr for people who don't have time to scroll through them all. Seems to me that for data science workloads where you care about getting results quickly and no fault-tolerance, autoscaling etc., go for one big instance! And for data engineering workloads where autoscaling and fault tolerance required, go for many smaller instances. 
I also made a point that on AWS  bandwidth scales with cores -- actually I did an experiment and that is not the case, at least when downloading from s3. So there is a point to be made that more small instances == higher S3 bandwidth.",Discussion,dataengineering,sdqfib
"Hello, we are researching options to introduce data lineage into systems at my company . I see many projects and I don't really know in which terms should we compare them. We have our own custom scheduler and we are mostly running Spark  and Flink  workloads, reading from Kafka, sinking to Cassandra and S3. I was looking into DataHub, Amundsen, Marquez and Apache Atlas. From the high level perspective, all these solutions seem pretty similar to each other - of course integration options vary a bit. How have you made the decision to go for one over the other?",Discussion,dataengineering,s5ibx3
"I was looking for some declarative ELT tool for creating my analytics solutions, and DBT was the closest I've found. I liked its concept, but I came across quite a few limitations when I wanted to use it. I couldn't specify and create basic things like data types, indexes, primary/foreign keys, etc. In the end, I decided to implement my own - more straightforward and more flexible. I've published the result - dbd on GitHub _URL_ Perhaps, you can find it helpful. Your feedback is greatly appreciated!",Discussion,dataengineering,rzvqj5
"Hey everyone,
This can be an odd question. I am in my late 20s. I have _NUMBER_-_NUMBER_ years of Big _NUMBER_ and banking experience and another _NUMBER_ years of audit background in maritime / logistics sector. Besides, I have Economics degree.
In my latests job, I had to work with OLAP cubes to create insights for upper management. This led me to dig a bit deeper to learn SQL. For last _NUMBER_ months, I've pretty much automated my boring tasks on SSRS and I told my manager about it to get database access to create and automate more tasks. Since then I am working with BI Developers and Analysts to create dashboards from these reports but tech stack they use is pretty much outdated.
I realized I enjoyed technical part of the jobs much more than 'finance' part of it. Since I have no background on CS. I've decided to start CS50. It was tough but I learned so much from it. I've stumbled upon Seattle Data Guy on Youtube. Almost watched his all videos he posted in last year.
My question is: is it being too unrealistic to be a Data Engineer? I think switching to Data Analyst is much more doable but that is not what I want to do.
PS: English is not my first language, please ignore any sort of mistakes.",Discussion,dataengineering,s5aoyz
"I could not get any response to this _URL_ question. I did more research and work and realized I could phrase the problem better and may be can get better response.
I want to implement SCD
A chunk of rows in data mart table can have SCD in two ways:
- api data is updated.
- manual changes that are conveyed via google sheets by another team.
Is there a way in DBT, to cater for two sources for snapshot.
Thanks.",Discussion,dataengineering,sd3gco
"Currently, there are new Reverse ETL products. However, I wanted to understand why there are new Reverse ETL products and why not the cloud ETL/ELT players launched Reverse ETL as a feature only.",Discussion,dataengineering,sgw37e
"Dynamic programming, Tree, Recursion etc ....How common are these concepts employed at real time data eng projects? Should I dedicate lots of time learning these concepts or move to core Data/ETL/Data operations related concepts.",Discussion,dataengineering,s9xkw5
"Given a greenfield project with a customer with no data warehouse, and no reporting across its separate on-prem systems 
How would you build a solution that enables analytics, and enables the use of data from the various systems in some dashboards, and apps?
The customer has a small IT department, with only one data analyst . There’s no devs, and little room for a big project. I just started reading up on the topic of data engineering and data plattforms and currently this is what I’ve come up with. 
### Part _NUMBER_: Put everything in blobstore
_NUMBER_. Extract data from the systems using Matillion.  Matillion seems like a simple tool to use for non-devs.
_NUMBER_. Place all data in blob store, with each system in a separate folder.
_NUMBER_. Put the relationaldata as csv files in the blobstore
_NUMBER_. Put the timeseries data also in the blobstore 
### Part _NUMBER_: Transform it to a common model
_NUMBER_. Make a common language and define one big model that spans the entire company
_NUMBER_. Use snowflake to transform and build that model using a virtual warehouse 
_NUMBER_. Use snowflake to further transform into a star model
### Part _NUMBER_: Use the data
_NUMBER_. Connect Power BI directly to snowflake start model
_NUMBER_. Build Apis that connect directly to snowflake, and builds apps on top of that 
I would be really glad if you could let me know if I’m on the right track, or if I’m doing something really stupid, and maybe guide me into better solutions.
Thanks",Discussion,dataengineering,sby36j
"Hi everyone, 
Currently looking into what the best data quality tool would be . We use Airflow for ETL , and are on Google Cloud for context. 
We are looking at:
_NUMBER_. Monte Carlo _URL_ and it seems good in that you don't have a lot of set up and it will be a complete package. However, since it is closed source it would be an expensive option.
_NUMBER_. Great Expectations _URL_ We like the open source nature of it but have a hard time getting it to work with the rest of our set up. Also, it seems we need to implement it per project/ team where Monte Carlo would be a solution for the whole company.
_NUMBER_. Soda _URL_ This one is the newest and we like the SQL structure of it which goes well with our infrastructure. 
However, really curious if anyone here has experience with one of these tools and what the experiences are! Also, if there are recommended tools that we are missing this would also be very helpful.",Discussion,dataengineering,s7n0qc
"Hi everyone,
Just wanted to know if anyone had experience building an events pipeline in GCP.
Mainly, our clients will make calls to our APIs and we will publish the resulting events to PubSub and then Bigquery.
Any tips or pitfalls ? Is Dataflow the right tool for moving the raw events from PubSub to BQ ? 
Appreciate any tips of if someone has experience in this area.
Thank you !",Discussion,dataengineering,sezbcp
"Hello folks,
We need some customized metric tables and views built out for a new business team that's growing fast. The data sources will be a combination of external SaaS/ DBs and some internal data, and it's what you'd expect with a ELT/ ETL data stack with transformations done in a DW.
A question that's come up is, could we have these pipelines built and managed for us? If you're familiar with the Managed Service Provider  model, we're looking for a similar solution, where we can define the requirements  and have the 3rd party provider take care of the data engineering aspects for us.
Our CEO does not want to bring in _NUMBER_-time contractors who set things up and leave, and we don't want to hire someone who would be dedicated to this, so this model seems to be the best one for us.
Thoughts? Have you seen such engagement models at your company? Is it even common in the data engineering space?
Also, been searching for a few weeks, so would really appreciate any recommended vendors.
Thanks!",Discussion,dataengineering,s31sjh
"Recently, I've been more involved with data architecture at my job . I am mostly designing things based on my experience but I haven't really been following any people or blogs about data architecture yet. I wonder if you could suggest some valuable resources to me.",Discussion,dataengineering,s0pu76
"e.g. Top Medium authors, blogs, newsletters, etc.",Discussion,dataengineering,s3x6ab
"I currently govern an environment that houses data in a few different tools: MySQL, Redshift, and S3. I'm trying to generate a data dictionary that will make navigating and documenting these systems easier. This tool would ideally allow me to define objects and columns, and link objects where appropriate. 
What tools are folks using to house data dictionaries? My fallback is creating Confluence pages with database diagrams, but would ideally like something more robust _EMOJI_",Discussion,dataengineering,sd8kuf
Any ideas on how to approach while designing data systems? Are there any good resources which talk about data pipeline or framework designs?,Discussion,dataengineering,sbzk4x
"I'm working on a monolithic PySpark project and have now an opportunity to refactor and redesign things a bit so was wondering what's your approach for code organization and composition, do you introduce any additional abstractions for your data transformers?
It seems like this topic in the context of pure data-oriented projects is quite poor... There is a ton of books and talks around software design but it often seems not applicable when building regular data transformers. 
I started to dig into some potential functional programming approaches and I came across **polylith** but also no success with data context examples there.
Do you prefer an OOP/class-based approach or FP?",Discussion,dataengineering,s2i4ho
"Modern Analytics platform covers both Business Intelligence  and Predictive Analytics . The Data Architect of yesteryears are not knowledgeable on AI/ML stuff, whereas Data Engineering being a new and more of a hands-on role, it is capable of serving both of these deliverables.
If we agree that the head of an Analytics platform should be from Data Engineering background, than what should be the title of that role: Senior or Lead or Principal Data Engineer?",Discussion,dataengineering,s9t66m
"My company is in the process of moving their Consumer Data Platform from Microsoft Azure to Snowflake. The CDP when I was hired a little bit less than a year ago was basically utilized for one off excel pulls with very limited comprehension of the data for the business. I was hired on as an analyst to do what analyst do - run analysis on the data, showcase use cases for the data and help the business utilize the data. Much of my job is pulling together ad hoc insights on our customers and making it digestible for our business partners. The tool of choice in the company is Spotfire which I don't care much for considering the tool is slow and as an analyst with several years experience in developing Power BI dashboards from the ground up - I have to basically ask IT to make any minor change and then wait for them to publish and then if it's not correct I have to come back and repeat this cycle. I am the only analyst in the company who was able to get their hands on Power BI license and this took a lot of political leverage from my manager and leadership. When I first joined the company I had a lofty task of rewriting majority of the queries that were initially shared with me because of very slow execute time of these outputs  and because the data wasn't in a format that was necessarily beneficial to my analysis. 
With the Power BI Pro license and the fact that the CDP as it stands right now all being MSFT products - the usability from an analyst perspective is a dream, I'm able to set up direct connection from my PBI reports to the tables of interest in the format I need it in . 
I have a pretty good relationship with one of the lead data engineers and he has told me that I'll be able to get the data in the same format in Snowflake as my hard coded SSMS tables but with a view in snowflake. I am incredibly weary to go back to using views as I'm afraid I'm going to hit walls with load execute times. I feel very left in the dark with what the hell this data engineering team is doing in snowflake and how it'll compare to the azure platform as there are different data connection pipelines being laid down. 
To make things even more complicated - I have a background in data engineering from a previous role where I was writing SSIS ETL scripts as well as doing pretty intensive DQ checks. Right now I'm sitting under the marketing org and I worry that this IT team looks at me like an incompetent dumbass . From a data engineering perspective how can I best communicate my concerns with this team and ask for more insight into their workstreams without looking like a jackass? This team is incredibly understaffed and dealing with a ton of tech debt - so I'm trying my best to be empathetic and not a bother but I'm worried the work their doing is going to cause me and my team a huge headache. 
Side note - this role has been vacant for over _NUMBER_ years prior to me joining. Prior to myself joining the role was occupied by a data scientist who ended up leaving the company because he was finding himself doing more data quality work than actual modeling. My manager is NOT technical and has no technical background (apart from some Tableau report building some years back and some data modeling but with very small amounts of data -  ). 
I have a lot of hope and a vision for what this CDP can mean to the company so I want to make sure I'm airing my concerns/desires in a way that doesn't come off as a wagging my finger.",Discussion,dataengineering,se19h9
"“Data lineage is like a family tree but for data”
_URL_
Data lineage is a technology that retraces the relationships between data assets. In the data world, you start by collecting raw data from various sources and refine this data by applying various transformations.
Building a table for a particular use case, or a “child table” requires using other data tables called “parent tables”. The data lineage helps you retrace which parent tables have been used to build child tables. It basically helps you rebuild the family tree of your data.
In reality this is what data lineage looks like 
_URL_
New interfaces for data lineage tend to be simpler and address one use case per visualization in order to make it easier to understand for business experts. ‍Here’s an example of data lineage in Castor _URL_.
_URL_
Data lineage is a technology, not a product. It is one of the underlying technologies behind a lot of data products .
The reason data lineage is so popular is that there are a lot of new use-cases, both for business, engineering, leadership, and legal department.
 
The most common use cases for data lineage are the following: 
\- Data Troubleshooting 
\- Impact analysis 
\- Discovery and Trust 
\- Definition Propagation‍ 
\- Data Privacy Regulation  
\- Data assets clean up or technology migration
More on each use-case here _URL_",Discussion,dataengineering,s34bvk
"So I'm studying for the GCP PDE exam and don't have a huge amount of hands on DE experience. I finished the _NUMBER_ part Coursera course that's mostly aimed at preparing a DE for working within the GCP ecosystem. It's been helpful but I've been going through question banks/practice tests and realized I'm missing a fair bit of the hands on knowledge about migrating from legacy setups and how to properly troubleshoot things when standing up a new pipeline/tool.
I know a lot of this comes down to hours spent on the job but do any of you have recommendations on resources that were helpful for things like going from Hadoop to Spark workflows, updating legacy SQL code, when to push or pull new data or use a publish/subscribe model etc. Basically looking for resources on data architecture and common troubleshooting.
I'm currently a few chapters into Designing Data Intensive Applications and it's been great at helping me build a mental model of how some of these tools work but so far it seems a bit general. Also this sub has been great too for exposing me to new topics and y'all are great!
**TLDR:** Looking for resources on data architecture, troubleshooting and hands on stuff. GCP specific would be nice but AWS or open source works too. Halp plz!",Discussion,dataengineering,s7zsi0
"this seems a topic wich gets discussed the least yet it is a big issue in our company right now. We already have an warehouse solution SAP BW on HANA and have a BI tool Tableau. We are now discussing how to create info cubes.
Now the IT wants precise requirements but we know SAP mostly from the UI and know little or can only guess the underlying data structure. We want to cluster and specify our requirements to be able to discuss this topic with the IT.
Now to my question. Are there any methods to collect/cluster requirements for data solutions? How do you deal with requirements? Do you use any tools? Is there any literature to this topic?
I’ve been going through some books and most just mention that requirements have to be collected and business context understood but don’t mention any way how to tackle this task.",Discussion,dataengineering,shc7jn
"Hey, I'm getting into the data/AI space from a history predominantly in Linux and writing application software. I'm really looking to create a new project, and I want that to be in the data/AI space and runnable in kubernetes. I've been looking through the Linux Foundation data/AI landscape and I'm not sure where there is a gap in tooling. 
My instinct is something around data ingest, even moreso around consuming social media data or something. However I don't have a network of contacts yet to know if that's useful, or not. I wouldn't mind of the tool was in data storage, or processing, or filtering, I just want it to have a need and a community. 
Not looking to make any money - I'm a FOSS advocate and enjoy writing Golang on Kubernetes. Purely looking for a project to build an interesting tool in my spare time.",Discussion,dataengineering,sde1mm
"I’m in a team where our pipelines are reading data from Athena. Potentially lots of records. Are there any concerns of performance of reading from Athena vs RDBMS? I’ve never personally seen a use case of Athena for reading from it. Only used as a way to query the data and run some analytics against it. 
I’ve seen some limitations on Athena as well which can pose as a issue. Is there anything else?",Discussion,dataengineering,s79d7f
"Hello Peeps,
I am trained as a data engineer in my company. I have been an offered a role of the snowflake developer for a project should I take a decision to work in that or wait for another one?
**Trained On:**
Azure, ETL basics, Scala, Python, Data bricks, Spark, SQL, Linux and currently training on AWS.
FYI: Its not based on cloud.
Thanks for answering_EMOJI_",Discussion,dataengineering,s0p65j
"What are some competencies you would expect of a mid-level Data Engineer, and what are some good stretch goals?",Discussion,dataengineering,sbwsyf
"Hi all
I'm curious as to how other companies collaborate and maintain that collaboration organically. I know there are Community of Practices and I have heard of guilds, but I've mostly experienced these from a high level / business value perspective, rather than less informal, more engaged experience that is driven from the bottom-up.
Just curious as to whether there are ways of bridging barriers across business areas that I haven't seen or thought of!
I did find Bloomberg's article on their internal guilds very interesting: _URL_ _URL_",Discussion,dataengineering,s6tmax
"Recently started working in a computer vision/robotics company as ML ops. We're building some in house tracking tools -- dvc has become a bottleneck and wandb is unreasonably expensive. 
Part of the plan is to have a django API servicing dataset retrieval/creation, model-reuse, pipeline introspection, etc. This was actually going well, until I started drafting up the.... models.
My Django models now reference my research teams models, which in turn depend on various other data models , and its starting to drive me nuts.
Someone on my team casually suggested namespacing ML stuff as ""AIModel"" or ""MLMode"" but I still hate how this looks:
 class AIModel:
 model_name = CharField
...
Anyone have any tips on naming stuff in this day and age?",Discussion,dataengineering,s0wpv9
"I spend a lot of my time reading log files, and I’m realizing that there is probably a lot of room for creativity. It’s the primary UI for my python jobs. Does anyone have a favorite resource for how to pick what formats and information should be included in your logging statements?
One of my favorite patterns was to always end a job with either of the _NUMBER_ lines below:
$timestamp|END:SUCCESS
or
$timestamp|END:FAILED",Discussion,dataengineering,s9ky4n
"Any chance we could use a different format for the salary discussion threads? Maybe like a Google Form with results presented in Google Sheets so that we can filter on location?
I'm based in the UK and find I'm scrolling quite a bit before finding posts from fellow UK engineers.",Discussion,dataengineering,s80dd7
"Modeling question from a n00b:
My org has customers and often the business questions are around already-aggregated facts about each customer. For example: ""How many customers have _NUMBER_-_NUMBER_ transactions, _NUMBER_-_NUMBER_ transactions, etc"". I think to get these answers I create factless fact tables, but for the purpose of data exploration  I'd like to roll up a count of those transactions by customer and store it as an attribute of the customer. 
I'm sure there is a different way to think about this so any direction is appreciated!",Discussion,dataengineering,se3e16
"I have a couple of databrick clusters which I want to assign only read access to specific users, so they can just get read the event and driver logs. 
When I look through the permissions, the lowest permission is ""can attach to"" which still allows the user to create notebooks. 
Is there a solution to give user just read access?",Discussion,dataengineering,s7uheh
"We have built 180Protocol, an open-source toolkit for data sharing. It targets enterprise use cases and improves the value and mobility of sensitive business data.
Our alpha release is live on GitHub _URL_ Developers can quickly build distributed applications that allow data providers and consumers to securely aggregate and exchange confidential data. Developers can easily utilize confidential computing  to compute data aggregations from providers. Input/Output data structures can also be easily configured. When sharing data, providers get rewarded fairly for their contributions and consumers get unique data outputs.
Read more on our Wiki _URL_",Discussion,dataengineering,s9d1q2
I am a ds and I see the differences mentioned everywhere when it comes to data management. I am having trouble understanding why this is the case if anyone care sharing some insights. Thank you!,Discussion,dataengineering,sfme7l
"Curious which you would choose and why.
View Poll _URL_",Discussion,dataengineering,s9ghhk
"I'm curious how various companies measure ROI for their data platforms. You could attribute indirect value from each new use case enabled through the data products, but there's additional costs outside of DL/DW that would need to be accounted for . Do you assign an arbitrary % to each use case enabled or ignore the additional costs from those downstream teams that use the data? Costs are easier to track but I'm struggling with how to measure the business value across the current user base.",Discussion,dataengineering,s704lo
Trying go from Postgres to snowflake environment. Not a historical load. It will be incremental daily. I wilL NOT have access to download raw Postgres files. I will only have access to move the postgres data using the tables there. Unsure size of data as of now. Best approach for this?,Discussion,dataengineering,s7riow
"As a data engineer, I'm pretty happy with my salary and technology used in my current position. I probably make slightly less than software engineers at my level in my company. I enjoy using python and SQL, and also being familiar with a wide range of AWS services. I'm pretty interested in learning devops skills too, and I feel like data engineering has been great teaching me this.
I noticed that most senior engineering management/CTO's/tech entrepreneurs have software engineering backgrounds, as well as maybe an MBA. Is this a result of data engineering being such a new field, where very senior data engineers have yet to progress to those types of roles?
How do you imagine data engineering career progression to be like?",Discussion,dataengineering,sa5ugk
"So I had a interview recently with Afterpay which I totally bombed. I was given a transaction table of a buy now pay later company . The loan process can go like : approved, authorized, charged, refund, etc :
Example of schema :
merchant , 
Customer , 
amount, 
loan_status, 
date. 
 
 
 
I am curious to how you would design such a table for olap systems ? Some questions that need to be answered :
How many customers rejected , approved ? 
How quickly a loan is paid? 
How many checkouts were initiated with Afterpay? 
Monthly cash flow of given loans? 
I am a total noob so I went with : just join all the tables . Wondering if there was better response ? How would you do data modeling ? The company uses ELT with cloud warehouse.",Discussion,dataengineering,sgup4d
"Hello everyone, as a Data Engineering consultant I don't always have the luxury that a Data Engineering environment is setup already, so I have to do that first. I find it sometimes quite a process to make a choice where and how to deploy each tool in the best way. 
Some cloud providers are making this choice a bit easier by providing those tools out of the box, but this is paired with somewhat lower customization and higher usage costs. So it is still a tradeoff. 
A trend that I am seeing is that a lot of tools can nowadays be deployed on Kubernetes, which is a container orchestrator and supports automatically scaling resources up and down based on application load. I see the benefit of using it for data engineering applications, since a lot of my data pipelines run for a relatively short time , and thus it is not required to keep a machine running the rest of the day.
That's why I started digging deeper into deployment of tools such as Airflow and Spark on Kubernetes. Learning everything about Kubernetes can be a bit daunting, especially if you're not used to working in operations it the past. So I was glad that there are ways to make the deployment to Kubernetes easier with the use of 'Helm', which helps define, install, and upgrade Kubernetes applications. 
There were already a number of Helm Charts available to deploy Airflow on Kubernetes, such as the Bitnami one](_URL_ and a (_URL_ These Helm Charts are definitions of Kubernetes resources which can be used to easily install applications on the Kubernetes cluster. Recently Apache Airflow released [an own 'official' Helm Chart _URL_ as well, with lots of functionality that can be enabled with minimal configuration. With this introduction it feels like deploying the tool on Kubernetes is a great option that has a great future ahead.
What are your thoughts? 
If you worked with Airflow on Kubernetes already, what is your experience with it? 
Would you prefer it over other ways of deployment?
 
PS. I also definitely see the potential of hosting Spark on Kubernetes for many of the same reasons. So I thought it would be interesting to share this upcoming virtual meetup with you . Two engineers from one of the largest retail companies in the Netherlands, HEMA, will explain how they setup Airflow and Spark on Kubernetes. They tried tools like DBT and Lambda functions for data processing, but decided to make the move to an easily scalable, low-cost PaaS EKS environment. Might be nice to have discussion with them about it in the virtual meeting room. 
February _NUMBER_ _NUMBER_:_NUMBER_ CET / _NUMBER_:00am EST  
_URL_ _URL_",Discussion,dataengineering,se3yul
"So I've noticed that AutoML is reducing the need for Data Scientists that build models. Shoving data into models can be done by anywho who knows SQL  and AutoML is MASSIVELY out-performing teams of data scientists. 
Do you think data science is overhyped right now? Do you think there's a need to build a train models when AutoML can do it better and cheaper?
See: _URL_ _URL_",Discussion,dataengineering,s7aj4r
"Hello :) 
I would be interested to learn more about the history of distributed computing/data engineering. 
I've seen that reading/watching about how people did things before helps me better understand why we do things today in a certain manner today. 
So, any good blogs/videos/books about the history of data engineering/distributed computing/databases/etc.? 
Thanks in advance!",Discussion,dataengineering,s8sxg9
"Hi, what is best way to store and manage data to identify outliers such as security breaches, network outages or machine failures in real time. In financial services, companies can respond to potential fraudulent sign-in attempts or credit card transactions by joining a real-time activity stream with historic account usage data in real-time.",Discussion,dataengineering,sd54mi
"Hi! How do you organize access to multiple csv, xml, xlsx files via SQL? Is there an integration solution that provides an automated scan of files on shared endpoints such as files on Google Doc or Dropbox, scan them and add them into a relational database? E.g. postgres. So I could work with the data later on via a single access point and the data remains in sync all the time?",Discussion,dataengineering,sc7zfo
"It seems like to do so, you would need a really expensive development/test environment. Maybe it is just extra hard for us because we have protected health data, and you can't just replicate that in non prod systems?
I just want to know does anyone do this? If so I have some other questions. Thanks.",Discussion,dataengineering,scjfnb
Hello there. Any news you have heard if Azure will add Airflow as paas or managed service? Or opinions why they won’t...,Discussion,dataengineering,sbrc7a
I'm curious what tools are used to create canned reports  that can be run in batch mode  on a set schedule . Maybe cognos? Any others?,Discussion,dataengineering,s7925h
"This is a DBT question, but possibly has broader application. I have one main DBT tag that refreshes _NUMBER_ or so tables every 24h. Those tables are used downstream for a lot of use cases. Now I have a request to refresh _NUMBER_ of those tables every 1h for a new application. I think I have the following options to support this and I'm trying to understand which one makes the most sense:
_NUMBER_. Overlap both schedules: This means I'd be refreshing every 24h for all _NUMBER_ tables, but also every 1h for _NUMBER_ of those tables. That way I don't have to replicate data, but some tables will be ahead of others and have fresher data. I'm not sure how that will affect the analytics downstream.
_NUMBER_. Replicate the tables and create a new tag: This would create two separate flows, one that refreshes _NUMBER_ tables every 24h and another that refreshes _NUMBER_ tables every 1h. This is cleaner from a scheduling standpoint, but it leads to replication. I'm not concerned about costs but more with the codebase complexity. And the more schedules I need to support the more table copies I'll have to create.
Is that something others have faced before? Please, help me think through what option makes the most sense.
Thanks.",Discussion,dataengineering,s9bfnu
Curious!,Discussion,dataengineering,shmkaq
"I'm a BI developer that turned Data Engineer. I'm now in a Senior Data Engineer role and I know a big blocker in my career is that I do not know Python. Six months ago, I left an organization after being there for five years where we didn't use Python at all in the shop. It was a large retail company and it was all corporate IT .
My new company is a startup that is moving out of the startup phase, they're in super growth mode, and I'm helping build their data ecosystem from the ground up. I can see that we can possibly benefit from Python in our stack and also I think for going forward in my career, it's a critical skill to have in my toolkit.
What I'm wondering is if you all took any DE-focused Python classes or had any recommendations outside of just the normal suggestions for learning Python that are over on /r/Python. For me, I learn best when there's a practical application to the course.",Discussion,dataengineering,sh4u69
When someone asks you what you do for work what is your response? I feel like most people don’t know what a data engineer is.,Discussion,dataengineering,s85nc8
"Hello everyone!
As you know, to achive exactly once processing in Apache Kafka we have to create two topics: one to store producer's system information like change logs/offsets and another one for data. In order to write a piece of data you have to open transaction and make two writes
 kafka_transaction
When a producer restarts it must re-read system topic to recover state to continue data write. 
This way leads to x2 writes, x2 topics and tons of complexity during producer start-up. If you have distributed systems and _NUMBER_+ producers and _NUMBER_+ topics, the Apache Kafka usage turns into pain.
Is there any messaging system that natively supports exactly once sematics like part of API ? The solution with two tropics looks unnatural and ugly. 
For example, system log/information can be placed into write api:
 write
aslo during producer's initialization the log item can be returned in producer's API
 log = init_producer
There is no two topics, no need to re-read system log. Simple and easy",Discussion,dataengineering,sgf04p
"Will Snowflake Scripting reduce the use of writing javascript store procedures?
_URL_ _URL_",Discussion,dataengineering,s93fki
"Hi guys, 
Looking for your advice / opinion: 
With so many ML/DS YouTube channel , what thing you feel is missing ? or What kind of YouTube channel in this niche you will like to see . 
Thanks for your answers .",Discussion,dataengineering,s32tqp
"I’m exploring data solutions for my workplace. I wonder what people here use.
Currently I’m using
SQL Server for database
ADF for data pipelines and orchestration
Metabase for data visualisation and reporting
These are put in place before I joined. I’m not too happy with ADF. I think it’s inflexible and solutions we have in place are mostly quick hacks. We’re also lacking a proper solution for data lineage, data testing,...",Discussion,dataengineering,s4cdew
"I've used Informatica, Control-M and Wherescape for different requirements. What is your preferred or frequently used data orchestration tool and why?
I prefer Wherescape as it has various pros and some cons which I have learned to adpapt. Best pros is metadata versioning, automated code generation and deployment agility. Also, seen some new promising tools such as dbt, Astera DW Builder and others.",Discussion,dataengineering,sdnhws
"I specialize in data migrations using SQL Server to Salesforce and customers are more frequently needing data warehousing strategies then integrating it within Salesforce afterward. 
I have no experience with cloud computing and how it works, but I’d love to learn about it. 
Where is the best place to get started in learning how I can use AWS as it relates to data warehousing and the like?
Sorry if this seems like too loaded of a question, but I truly don’t know where to start.",Discussion,dataengineering,sajl94
"Hi All,
Let's say I am having a data of _NUMBER_ TB residing in AWS S3. I have do a simple transformation and aggregation. I am very new to EMR/ Spark. I am having difficulty choosing EMR cluster size and Spark Configuration. 
I know it is very vague and depends upon use case. But atleast to start off, what would be the best cluster size and spark config so as to not incur more cost or get OOM issues.
I am trying this config. Please let me know if this is correct
_NUMBER_ master node instance - m5.2x large _NUMBER_ VCPU and _NUMBER_ GB and _NUMBER_ GB EBS 
_NUMBER_ core nodes instance - m5.2x large _NUMBER_ VCPU and _NUMBER_ GB RAM _NUMBER_ GB Storage
""spark.dynamicAllocation.enabled"":""false"",
""spark.driver.memory"":""10g"", ""spark.executor.memory"":""20g"", ""spark.executor.cores"":""_NUMBER_"", ""spark.driver.cores"":""_NUMBER_"", ""spark.executor.instances"":""_NUMBER_"", ""spark.yarn.executor.memoryOverhead"":""4g"", ""spark.default.parallelism"":""_NUMBER_""",Discussion,dataengineering,sb1vqc
"On episode _NUMBER_ of the Data Engineering Podcast](_URL_ they talk about [Anomalo _URL_ which looks a very interesting commercial tool to test data using statistical anomaly detection - they have a short and sweet video demo on the landing page.
Think getting an alert when in the new batch of data loaded in the warehouse there is an abnormal amount of Null values.
Has anybody here used this? What about comparable solutions? Anything open source?",Discussion,dataengineering,s74re4
"I recently created an automated ETL pipeline on Amazon Web Services  with SNS email notifications using S3, Glue crawler and ETL job, Lambda and EventBridge. Here is a link to the Medium article
 link _URL_
When a file is inserted into the S3 bucket, it triggers a Lambda functions that starts the Glue crawler. When a crawler is done, it creates an event in the EventsBridge that triggers another Lambda that starts the Glue ETL job. When the ETL job is done, it creates another event in the events bridge that triggers another Lambda that activates the SNS to send out an email notification
I have a concern
_NUMBER_. Is there a way to send the bucket name and folder details to the crawler using Lambda so that it does not go through the entire bucket but instead crawls that specific folder only
Thanks",Discussion,dataengineering,s3j3h4
Hello! I am in a part time CS program and I am really interested in data stack and upstream data management. But after five courses into my master of CS program I still find much of data engineering concept and task incomprehensible. Is there a particular class like database or Hadoop that are most useful for my purpose?,Discussion,dataengineering,sfmamr
"Edit: I'm sober now and thought about deleting this but the comments have been interesting so I'll keep it live for a while.
Databricks poses as the ""open source"" solution to the big data platform challenges of modern data-driven businesses. It certainly has contributed A LOT to OSS. MLFlow, DeltaLake, Spark, Koalas.
But the sham of the supposed 'open' lakehouse architecture is quite subtle. Does it operate on an open source format? Yes...kind of... the format is parquet, but the benefits of the 'lakehouse' really come with the delta log. Now is Delta open source? Yes....kind of.... It certainly is open source, but is most useful, and only reasonably used in production when coupled with Spark. ""But I can read parquet with dozens of clients that aren't spark"" I hear you say. Yep, you can. But reading the parquet files from delta lake won't do you any good beyond what you could have done just with vanilla object storage. The entire lakehouse promise is void in that scenario.
The above highlights some of the inconveniences, but isn't really an indictment of the lakehouse. Where the company's messaging/branding and capabilities really fail is they've tried to put on the bold face of being the ""Data + AI"" company. A company that no longer wants to be just the ""Spark people"".
So how does the lakehouse paradigm support AI in an open and extensible way? Well... it doesn't. Say you have a delta table in your s3 bucket that was written there by a databricks cluster as part of an ETL process. Maybe its done some overwrites and has run multiple times. Say you're looking over a state of the art research paper that mentions they trained their network on two A100 Nvidia GPUs. You go and spin up a EC2 instance with that same backing. You fire up your vscode.
Okay now you need the data....wait but how do you get it?...Well its in delta lake...open source format...hmm but you need a delta compatible reader... Wait but you can just read the parquet? But then why did you bother with a lakehouse then? Okay well...what if you just used a local pyspark session to get the data? Great! But you'll need spark, and hadoop, and any additional jars to be able to connect with the storage. Ugh. Okay, well you schlepp your way through all of that. Now finally, time to get the data. What...java heap out of memory? Well time to go twiddle with some spark configuration settings, always fun. Hmm, looks like it works for a small subset of data, but is this scalable? Is this going to work for large training jobs ? Ick. Maybe...maybe if you feed it enough RAM. But standalone spark single spark will never reallyyy scale. What about the other delta readers, delta-rs and the like? Well...same problem for scalability .... Hmm so how can you get the data out of delta lake and into your EC2 instance to train your AI model? Aha! Just ignore vscode and your EC2 instance and use the databricks notebook, with the databricks runtime, in the databricks ui instead!...wait...is that open?...Should you not be able to get your data out to any workspace for ML training?...Hmm.....still no Data for my AI....",Discussion,dataengineering,s56xwg
"I have two csv files. 
* F1: will always be less than _NUMBER_ MB
* F2: would go above _NUMBER_ GB. 
To read these two files, I can use either Pandas or Dask module. Pandas with chunking is showing faster reading time as comparison to pandas without chunking. 
Which module should I use to read these two files among the two? 
My current decision is to use pandas with chunking for F1 and dask for F2. Please enlighten me if there's a better option or why should I change my decision etc. 
I will use the two dataframes to do sql query and find a result using aggregation. Thanks.",Discussion,dataengineering,s2u8ij
"Are there any drawbacks to doing this to update data pipelines? 
 # fetch latest_sync from local table
 
 insert into dest_table 
 (select * from external_query);
Run this every hour or so. Its incremental so it shouldn't be too taxing. Of course it doesn't account for schema changes but neither do ""official"" CDC solutions like debezium. Why would you use something like debezium when you can just do this instead? 
I know federated queries are not always available. And they aren't as ""real time"" as debezium. But if they are available and if you don't need it to be real time?",Discussion,dataengineering,sbtry1
"For people entering the DE world and trying to suss out what would make a job a good learning opportunity vs. what might be a dead-end  - what are the green and red flags? Are they related to:
\- The tech stack?
\- The size of the company?
\- The age of the company?
\- The size of the team?
\- The manager's expertise?
\- The type of company?
\- Whether they have more of one type of role than they have DEs?
\- The job description?
\- The interview process?
\- Terminology used during interviews?
\- Etc.!
",Discussion,dataengineering,sc159z
"I've been with Google for just over _NUMBER_ years now, and I've seen that Google seems to consider Data Engineers equally skilled as software engineers. Algorithms and data structures, systems design, memory management, designing distributed systems, etc. Other companies like Meta/Amazon seem to believe that data engineering involves using no-code/low-code tools to build data pipelines.
Do you think Data Engineering is going to align closer to software engineering, or closer to specialized BIEs? Will Analytics Engineer become more popular?
My experience at Google is such that it's actually more difficult to find roles at other companies, and I thought Google would be a great resume builder. I share my experiences here _URL_",Discussion,dataengineering,sghj5q
"I’m not sure if this is a good idea but its something I plan to implement in the upcoming project.
The problem I’m trying to solve is to create a mechanism to be able to test our pyspark pipelines for any possible regression.
We use pyspark to build our ETL pipelines. A pipeline isually involves _NUMBER_-_NUMBER_ hive tables and ends up creating one hive dataset as the output.
My plan is,
When I define my datasets, I will have an abstract class defined that needs to be inherited while defining any dataset . One of the abstract functions of such a class would be to use a function called _data_sample. I will need to define a json with one sample row’s data within the class itself!
Now all my _NUMBER_ data sources  have sample data within them. I might as well create a JSON with the expected output  and save it somewhere.
When the pipeline needs to be tested, the code, instead of running spark.table to read data, will read the mock JSONs and proceed with transformations. The end output shall be asserted against the golden copy we had created earlier. This process and be automated and called CI!
What do you guys think?",Discussion,dataengineering,sban6s
"why / why not in the comments
View Poll _URL_",Discussion,dataengineering,sdb5y9
"My team needs to copy files from S3 to GCS on an hourly basis . The AWS account belongs to a partner company which is not very cooperative, so any solution we come up with has to run on our GCP project. 
We got the AWS credentials pair and are able to list directories and read files  and messages .  But they don't include the **s3:GetBucketLocation** permission which is required by Google Cloud's Transfer Service, and the other company refuses to grant us that permission for ""security reasons"". Without it, the service errors out with a *""Failed to obtain the location of the Google Cloud Storage  bucket pipeline-validation due to insufficient permissions. Please verify that the necessary permissions have been granted.""* error message.
What's the best option left?",Discussion,dataengineering,sh9v4g
"Hi all I am a DS interested in data engineering and data ops. One thing I am having trouble understanding well: I am curious to understand say from an ml perspective, what is a world without databricks or Apache spark like vs. with it like? What benefit does it bring to make lives of ml scientist or analytic professions easier? I read a lot of materials and still am confused.",Discussion,dataengineering,sdeakb
"So if something breaks, and some data is unavailable to people, they notice of course. They want it back up again quickly to start doing their jobs again. 
But it takes time to fix. So how much time is acceptable wherever you work? An hour? A day? A week? Whats the expectation?",Discussion,dataengineering,s33m26
"I normally say ""I enable Data Science / Analytics at scale"" which I think is quite succinct. If they're interested or if I have longer, I'll normally make an analogy with using Excel for data work and how that doesn't scale in the enterprise world and/or when you have large data volumes. 
What're yours?",Discussion,dataengineering,sbq7wd
"Has anyone had experience working at Macaw as a Data Engineer, or with Macaw. It is a small / medium sized consultancy company in Holland.
Just want to know if the company would be recommended to work for.",Discussion,dataengineering,sbocda
"In simple words, what is the difference between Data Architect and Data Engineer",Discussion,dataengineering,s72ssg
"I have two years of experience as a data engineer at a startup. As it's a startupit's involved wearing multiple hats for analyics, bi stuff, and deing. It's involved building pipelines, tracking user flows, developing KPIs, communicating results to stakeholders, etc.
I've been offered to join Meta as DE in their Seattle offices at an IC3 @ 124k pre tax; and an additional 25k in stocks every year + a _NUMBER_-_NUMBER_% bonus depending on performance. I'm wondering if I should try pushing the base salary to 150k? Everything on glassdoor and payscale seems to suggest I should be able tomoreso as the recruiter told me Meta typically pays the 90th percentile or higher of what a DE makes in an area. I just don't want to come across as too greedy, as all in allthis is double what I'm making currently and risk having the offer rescinded.
Edit- I've not been allocated to a specific team.",Discussion,dataengineering,sa5k9x
"If you can't think for beginners, what generally do data engineers contribute to in open source for good experience and learning",Discussion,dataengineering,s8j0jq
"Introduce this open-source project under Apache License _NUMBER_, hope it's useful and interesting to someone.
_URL_ _URL_
This project connects to streaming data from streaming engines like Kafka or cloud storage like AWS S3 and gives you a built-in UI to analyze data into graphics. It's easy to run either in Kubernetes or hand-crafted clusters with VMs. 
If interested, check out more details on the project page...",Discussion,dataengineering,s2dzos
"I have a use case where we have streaming data coming in and I want to aggregate them on _NUMBER_ min, _NUMBER_ mins.
Now what I want to do is stream the data in Kafka and through kafka streams I will do the aggregation on that topic based on the key of that topic. Now I will do this in a window of _NUMBER_ minute and _NUMBER_ minutes, push the aggregated data to Cassandra using Kafka Connect. 
I am somewhat new to the Kafka world, but I feel this use case will give me speed and scale well and if the table designs in Cassandra are fine the end user will hopefully be happy haha 
And I think late data arrival will be a problem so any suggestions on how to handle that? We are fine with missing late data as the place where we will read from its unlikely that the data will be late but I guess there should be a fail check.",Discussion,dataengineering,s9856a
"Hi Y'all,
I'm interested in hearing the community's opinion on some Pandas coding convention.
My problem:
I have to make new columns based on other columns. 
The rules are basically the same so I created one function that does this. That's simple.
However the new columns need new names that replace the previous suffix. 
For example given:
df
I'll want to name the new column:
df
Simple enough, but I have 30ish columns which all would look like:
df = funct
Except the names are slightly different.
So in theory I can write a function that takes the name of the string and returns it with the ending being changed. 
Which means I can loop instead of declaring each column.
My intuition is saying don't do this.
Anyone have a good alternative or real reason why I wouldn't do this?",Discussion,dataengineering,s1ubuu
"Hi there! I am working on a personal project to get some experience with Apache Airflow. I am wondering which of the options in the title is better for my use case. Here are the details:
I am creating a workflow with just a few steps yet to:  retrieve weather data for several towns via API calls to a server in a 'for' loop and, once downloaded, each JSON file is saved in the file system. The next step  is to get each file, extract some data to normalize and save this output as CSV files again in the file system. The tasks are based on pythonoperator and the number of towns is supposed to be dynamic in the future.
Is it better to have **ONE task with a for loop** or to create **DYNAMIC number of tasks ** to do that? This question if for both the 1st and 2nd step since the work similarly.
Thanks for you answer and suggestions!
Edit: the set up is in a local PC.",Discussion,dataengineering,s6twbv
"Like the title says, I've got json I'm pulling from an API. I read that json into databricks and convert to parquet. They I append that to our parquet reporting tables. 
 
However, there's a conditional array with a bool. If every entry for that batch is null, then databricks is unable to infer what type is ought to be a defaults to string. This breaks our downstream appending of the single batches to the larger dataset. 
 
I've found lots of suggestions for turning everything into a string, and that works, but I'd prefer to 'pickle' a schema object from a 'happy path' sample of the json and apply it to all incoming batches. Is that a thing?",Discussion,dataengineering,s285hw
"Does my question make sense? I personally build them for the internal business 
OR do you create pipeslines for the software product itself, take spotify for instance",Discussion,dataengineering,s89u76
"I am mainly familiar with GCP for data engineering. I am trying to learn AWS for data engineering also. To do so I am trying to implement a basic pipeline which I have implemented in GCP.
My current implementation in GCP is following:
* Files are being uploaded to GCS  in a bucket.
* Each upload triggers an event and publishes a message in Pub/Sub Topic
* There is a Google Dataflow  Job running which is monitoring the above Topic
* Once a new message is read, Dataflow parses the messages and get's the file's bucket location
* Dataflow reads the file and processes the data in it
* Finally it writes the processed data into BigQuery
So it looks like this:
GCS ----> Pub/Sub ----> Dataflow -----> BigQuery
Here is what I could think of based on my some knowledge of AWS:
Replace as follows:
GCS => S3 + Lambdas : Upload a file to S3, trigger lambdas to publish message to MSK Kafka Service
Pub/Sub => MSK : Upload events are stored here
Dataflow => EMR : Have a Spark Job Running which processing the messages in MSK
BigQuery => Redshift : Lands data here for warehouse purpose
I'm still exploring different services in AWS. GCP seems more cleaner approach, can this be done in much cleaner way?
How can one implement the same pipeline in AWS? What different services from AWS can be used? Is there is a similar framework like Apache Beam for AWS?",Discussion,dataengineering,sfcuy2
"For example, when you need to check or visualize parsed/scraped content stored as csv, xml or json?",Discussion,dataengineering,s4i4a3
"Theres a lot of hype about Data Mesh nowadays. Some people have gone to lengths saying that data warehouse should be replaced with data mesh. 
I dont get it. Data mesh is just a way of doing exactly the same thing using data products. No ? Or am i wrong ?
I dont see how data mesh can replace data warehouse. Can anyone please advice.",Discussion,dataengineering,s61xft
"What is the most economical way to store and index event data on the order of millions or billions of events  per day? The use-case is very write-heavy in that reads will be rare  but need to execute in less than _NUMBER_ milliseconds. I also need TTL or similar functionality to handle retention .
I've looked at DynamoDB and Bigtable but they seem expensive. Presto/Trino/Athena and similar seem too slow. RocksDB-Cloud looks interesting, but hard to judge what the costs would be and how hard it would be to maintain. What else is out there?",Discussion,dataengineering,s5krox
"Context: I will have a data product that will need to primarily get its data from more than one external 3rd party companies or data ""suppliers"". We will essentially sell the data at a higher price than what was paid for from our data supplier. I think logistically or technically what needs to happen is we would call the data supplier's API then ""wrap"" it to a different, custom response, and make that available to our customers.
I am completely new to this as Im used to having owned or have physical access to the data and on top of that, mostly relational data is what Im all used to. Now, its all REST API driven, JSON data. 
 
Curious if others had to do this also? How was this done? Pitfalls? Lessons learned? It seems like not actually owning the data or having actual physical access to the data is a major hurdle I would think. Also thinking response time would be a concern or consideration. I would imagine depending on what needs to happen in that custom response or what all the ""wrapping"" entails in our API, things may be unacceptably slow.",Discussion,dataengineering,sbsd77
"For people out there running Spark on _NUMBER_ c5.2xlarge instances on EMR or Databricks -- why not just use _NUMBER_ c5.8xlarge instance? Everything will just be so much faster. Is there some benefit of using many small instances?
Edit: I see a lot of comments about finer-grain scaling benefits. This is very true. Are there any other benefits?",Discussion,dataengineering,sdqfib
"What would be the advantages and disadvantages of each of these tools? I'm using SageMaker training jobs for the actual training of the model, but am not sure how to choose the way in which I perform the data pre processing.
Cheers!",Discussion,dataengineering,sh3ocz
"Title. It's easy to find on the internet how to structure your web dev project, or your data science project but I couldn't find any for data engineering project, can anyone share how you do it?",Discussion,dataengineering,s9bv8v
I haven’t had the best time reading/writing to and from S3 using Airbyte  and I was wondering if anyone else has had any experiences or recommendations with this combo.,Discussion,dataengineering,sh8dkd
"Hi All,
Im newbie in AWS. I have worked on EMR only in dev. Im curious to know, how Amazon EMR instances are created in production and performed big data on it.
Thanks",Discussion,dataengineering,scl3z7
"By Hadoop I mean cloudera. Sorry for the error. 
Both started as free open source, one ended in disaster while the other, alone with snowflake, is hot commodity. Will that last? What are the cautionary tales?",Discussion,dataengineering,sg1zhg
"Silly q. But how to really comprehend Apache Spark, what is the benefit of products like Data Lakehouse/Data Lake provided by companies like Databricks and Snowflake.
My understanding is Spark is a distributed computing framework, but it does not manage the machines it uses for distributed operations. It needs a cluster manager  to orchestrate the creation and scaling of infrastructure resources. Kubernetes is a popular cluster manager which accomplishes this. So Spark drives Kubes.
Is this understanding correct? It seems to be that Sparks or Databricks and Snowflake's services, which are expensive, are just doing what Kubes are doing already, if that is the case why are people still using them? What are the main difference between a data lakehouse and a data lake, offered by Databricks and Snowflake. And in the end, how beneficial are they? Can someone help me understand this also your data is still stored in the cloud like AWS or Azure, and how do you query from spark to make it faster if Spark doesn’t own the storage. Sorry if this is a dumb q",Discussion,dataengineering,sfqnl1
"Beside books and tutorials I'm trying to find some resources that contains end-to-end sample project with explanation so I can visualize what a common full pipeline looks like. After searching for a bit I came across this website _URL_ _URL_ it advertised to have full end-to-end data engineering project with detail tutorial explaining the step. However payment for it is like single option of _NUMBER_$ and it looks pretty sketchy
Do you know any resources that contain a good amount of end-to-end project with detail explanation ?",Discussion,dataengineering,sds8fs
"Hi all,
does somebody has experience with Data Vault _NUMBER_ ?
If so, are there best practices for implementing a “Housekeeping Concept“?
Example for Raw Data Vault:
Data is allowed to be kept for _NUMBER_ months.
Business Keys will be loaded into Data Vault Model only when they first appear.
Let‘s say a partition older than _NUMBER_ months is deleted and some business keys are therefore removed from hub tables - how do you deal with Data/records in the satellite tables regarding those deleted business keys, which aren‘t older than _NUMBER_ months and the inconsistency resulting from that?",Discussion,dataengineering,s6vjhn
"I am pretty noob in this field. I see sentdex talking and working on his own data. He talks about selling it as well. 
I am scratching my head. How does an individual generate data and where do they sell it?
 I read a few articles about institutions/ company selling data with all the insights and blah blah. But I am curious to know how can one/single individual do it?
If this question is answered before, can you please point to it.
thanx.",Discussion,dataengineering,san4ah
I often use CTEs in my SQL scripts that need to be executed across multiple steps. What would an elegant way to handle data transformation operations in Python/Pandas that require multiple steps? Is it simply just defining a bunch of functions and executing them in order?,Discussion,dataengineering,sft6j6
"Hello Redditors
So this isn't a job post or technical issue, more like need your creativity to come up with nifi interview task that would be not hard and not easy, about a pipeline from start to finish as my workplace is looking for data engineers with good nifi experience, I am not the one doing the interviews, I am simply helping my colleagues in their interviews
Let us see what you guys can come up with",Discussion,dataengineering,sbqln9
I have read a few articles where the authors state that dimensional modeling is dying and does not provide a clear alternative strategy. If dimensional modeling is becoming obsolete then what are the best alternatives?,Discussion,dataengineering,shp8xp
"My company has been doing this and it doesn’t seem right. To my knowledge, there’s no real way to work with data that lands as a string, especially if it’s very complex nested JSON. When it lands as a struct, you are able to query it in Databricks with JSONcolumnname.key. You could keep adding periods and keys after to continue to go deeper into the nested JSON.
I’d like to get peoples’ opinions on this topic and whether or not there is a way that I’m not aware of.",Discussion,dataengineering,sd8ai1
"Been in a data engineering role for _NUMBER_ months now. My company is trying to make a point to help pay for courses, certificates, workshops, meetups, etc for personal skill development. 
Wanting to take advantage of this I thought I would reach out and see if anyone feels passionate about what helped them learn. 
Current Tech Stack: 
- Snowflake
- Azure
- Matillion 
Current Skills: 
- Python
- SQL 
I have seen the certification exams for Snowflake and Azure Data Engineering so definitely considering those. 
Areas I think I lack in: 
- Data Streaming
- Kafka
- Spark
Any recommendations on education material that you did through your work is appreciated!",Discussion,dataengineering,s82vgj
"Hey guys! Recently, I have installed and configured airflow with a VM instance on GCP. As we know we need to execute the command 'airflow scheduler' to make the scheduler run. Is it possible to automate the execution of this command every _NUMBER_ minutes using cloud function and cloud scheduler without starting or stopping the VM instance?
Thanks for any assistance.",Discussion,dataengineering,s60ybg
"Curious which you prefer and why...
View Poll _URL_",Discussion,dataengineering,s9on5e
"Was limited in options...
View Poll _URL_",Discussion,dataengineering,sbk1ui
"I want to ditch crontab for something better, and am trying to find something that meets these requirements:
* A GUI to view/edit jobs and view logs 
* Automatic logging of all jobs including start time, end time, exit code, stdout, stderr, etc. This should include jobs in progress.
* Free and open source
* Easy git version control on all jobs
* Email or push notifications on job failures
* Should work on both Linux and Windows
* Create job flows as DAGs
* Job level user permissions for viewing/editing
The options I'm looking at are Airflow, Dagster, Prefect and Luigi. I've browsed their docs and demos but it's hard to tell... can anyone say which one fufills the most of these features? And are there any good alternatives that I'm overlooking? Some others that I've ruled out are Argo , Kubeflow , MLFlow .
I am asking on this subreddit because a lot of these tools are marketed for ETL workflows, but really I want to replace crontab even for scheduling jobs unrelated to data because most of these features are still very important for building a reliable system.
My team is currently relying on crontab + an assortment of hacks trying to fill these missing features like cronic for emailing and logging by suffixing every line with things like `... >> /var/log/myjob.log _NUMBER_>&_NUMBER_` or building logging into the scripts themselves which becomes hard to maintain and standardize.",Discussion,dataengineering,s78jvx
"We are currently planning on creating a Big Data Analytics infrastructure for our company. Since we will be mostly working on Azure, our teamleader proposed us to work with Azure Databricks. Since Azure Synapse Analytics is also an option, I would like to ask if anyone got experience with both and can list some pros and cons. I'm still unsure which one would be the right fit for us. What's your opinion?",Discussion,dataengineering,s9ya6f
"Currently our Python scripts for EL part are running as CRON jobs at EC2. For the transformation part, we are executing stored procedures via Python scripts as well. 
Currently it's maintainable but for the future, it probably won't be.
For the orchestration I'm thinking of using Airflow in AWS .
And for transformations I would like to start using DBT.
We are using Snowflake as our warehouse.
What do you think? Are there any better options? What would you do? Thanks!",Discussion,dataengineering,s341m4
"Hey guys, I'm looking for small things that most business owners can do today to improve the quality of their data.
One example would be to replace ""free form"" fields from their CRM to ""closed-ended"" fields as much as possible.
Any other examples like this one?",Discussion,dataengineering,seg1j1
"Hello,
We are creating a s3 data lake internally at a SaaS company. Want to name it something that represents its purpose - where the org can find data they need to query, analyze etc.
We obviously do not want to call it data lake or put any product name as it will be used across org. Currently scouting for names, I want to propose 'Pacific'  to reflect its deep , big and is fed from multiple sources
Want to take some input on how others name data lakes internal to the org.
TIA",Discussion,dataengineering,s07nev
"I wanted to post that I just did the datastax cassandra certification  but I don't think it warrants its own thread. I was happy with it and wanted to share with the community
Anyway, yeah I put in a good _NUMBER_-_NUMBER_ weeks in preparing. I had a bit of Cassandra experience but I wanted to learn it enough so that I can put it on my resume. The datastax site is great and all of the contributors are really cool so thanks guys",Discussion,dataengineering,s0ryvu
"I have trained a SpaCy NER model which can identify Name, Address, Institute, Degree, Skill , Company, Designation, School, Society and Location in a Resume. Now I want to structure recognized entities in such a way that CV owners name, address, skills and other details are together & Referees name , address, Designations separately. Is there a way to do it? I mean I want to have CV owners data together and Referees data together.",,LanguageTechnology,sgxqg3
"Hello ,
I've developed after my PhD a first version of an algorithm to automatically generate a literature review : _URL_ _URL_ and many remarks were given. I just deployed a new version with much more papers and I'll be thankful if you have any remarks about it :)
More about the new version here : _URL_ _URL_
Hopefully that could be useful for the PhDs  !
Cheers,",,LanguageTechnology,s3whz6
"Does anyone know how Twitter generates their “Topics”?
It seems like they could be machine generated ? It would be a lot of labor simply brainstorming a huge ontology of trending concepts in the Twittersphere.
They must have some algorithms for analysing and clustering tweet topics.
And possibly even for automatically suggesting the name of the cluster .
Anyone have any guesses how they do it?
Thanks very much",,LanguageTechnology,s103c7
"I'm  starting a master's in Computational Linguistics in the fall, and I'm curious to know what people who have done an undergrad/master/phd in cl, nlp, or even just compsci with an nlp focus end up actually doing after their degrees.",,LanguageTechnology,rn9kbo
"Hi all, I've just published another article focusing on fine-tuning a retriever model for open-domain question-answering _URL_ The retriever is a big component of the open-domain QA pipeline, allowing us to retrieve relevant *contexts* from a vector database, which then help us answer a query 
Let me know if you have any questions or feedback, thanks!",,LanguageTechnology,s26r9f
"Text-to-Speech  synthesis is achieved using current voice cloning methods for a new voice. They do not, however, manipulate the expressiveness of synthesized sounds. The task of learning to synthesize the speech of an unseen speaker with the least amount of training is known as voice cloning.
UC San Diego researchers propose a Controllable voice cloning method that offers fine-grained control over many style features of synthetic speech for an unseen speaker. The voice synthesis model is explicitly conditioned on a speaker encoding, pitch contour, and latent style tokens during training. ***Continue Reading*** _URL_
Paper: _URL_",,LanguageTechnology,s0qfes
"I've never used scatter plots in my life, so I'm trying to wrap my head around it but I'm having problems visualising what I want to do and I don't know if it makes sense.
I used LIWC to analyse positive and negative emotions in a number of texts over the years, and I would like to show how the score changed. Would it make sense to use a scatter plot? 
Besides, I also have different sources of text that I would like to compare . Would it be too much to put in one scatter plot?",,LanguageTechnology,rz9cri
"I've been Wikipedia-diving some of the history of NLP recently, and I'd like to know if anyone has any interesting stories about researchers/experiments in the field. You know, like when your high school history teacher goes on a random tangent about all the different torture methods throughout the ages. Thanks!",,LanguageTechnology,ro6v4v
"Hi everyone!
Are there any corpora or treebanks that are labeled with Penn Treebank-style constituency trees ?
I'm investigating the usage of a certain syntactic construction in English. I'm using the PTB constituency trees, but it would be nice to have as much data as possible. I found the Georgetown University Multilayer Corpus , and I'm wondering if there are others.
Thanks in advance!",,LanguageTechnology,shq4rt
"Hello all,
I'm trying to get smarter on mimicing writing style based on sample input text. The hope is a system like this:
**Inputs:**
* Input tagged sample writing/letters/emails/dialogue from desired author.
* Basic sentence to be rewrite .
**Output:**
* Translated sentence written in sample author's writing style.
I'm assuming this is a bit of an ambitious lift and may require some training on my own. Curious if anyone has any insights on stylometry papers written. Even something on generative text that is just meant to replicate an author's style could be a helpful starting point.
Thanks!",,LanguageTechnology,s7a0rx
"I'm using the Stanford NLP to help with a personal project. Last week I couldn't tell you what a Verb or Adverb was to be quite honest. I've been feeding my C# program titles/comments from stock investing subs to get the ""Sentiment"". Someone posted:
To r/ALL: _NUMBER_ Year ago, the most unprecedented move in the history of the stock market happened. \ 
The Buy Button was turned off for a specific stock. \ 
_NUMBER_ year later and there have been NO CONSEQUENCES. \ 
No one went to jail. \ 
Was there even a fine? \ 
Why? \ 
How is the answer not going to sound like a conspiracy \
I understand the algorithm is evaluating each sentence. How would one go about determining the full context? Clearly this example has a neutral/negative assumption. 
Further more, bias... so if I have a left or right leaning comment to which I could tell the program i'm in favour of L/R then the algorithm would deem that as a postitive to my bias.",,LanguageTechnology,s392e0
"I am trying to understand the masking in BERT model.
I have confusion in following line taken from paper
>The training data generator chooses _NUMBER_% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with  the \ token _NUMBER_% of the time  a random token _NUMBER_% of the time  the unchanged i-th token _NUMBER_% of the time
at point _NUMBER_ it say unchanged token  _NUMBER_% time. If we have to use original token _NUMBER_% of _NUMBER_% tokens, then why we need to mask it.
This can be more clear in **Attempt _NUMBER_: Masked LM with Random Words and Unmasked Words** section of this guide _URL_
The guide say
>So if we have a sequence of length _NUMBER_, we will mask _NUMBER_ tokens, and in those _NUMBER_ tokens, _NUMBER_ tokens would be replaced by random words, and _NUMBER_ tokens  will be used as it is.
So if we have to use _NUMBER_ tokens as it is, then why we masked them first?",,LanguageTechnology,ses9wt
I’m just wondering if anybody knows of any good Farsi  > English translation models? I’ve tried a few of the multilingual ones from Huggingface but the quality isn’t the best,,LanguageTechnology,s41hmv
I plotted MDS plot for word embeddings obtained from BERT. The \ token is plotted on the middle of the figure and other word embeddings are scattered . Is it supposed to be in middle? Is there any significance to it? MDS plot was plotted on the basis of pairwise cosine similarity.,,LanguageTechnology,s51i8l
"Hello, I'm a postgraduate student and I have a NLP project that I have to come up with and do . What are some really interesting ideas that you could recommend? 
An example of an interesting and good project is: _URL_ _URL_ .",,LanguageTechnology,scfo4t
"I’m trying to learn how to segment text into significant clauses.
Here’s a promising approach:
_URL_
chunks = 
for sent in doc.sents:
 heads = 
What are the children of a sentence’s root? Does that mean every possible lowest level syntactic element like “D”, “Quantifier”, “N”, etc.?
So the author decided to find conjunctions?
What about just looking at the syntax tree and breaking it on a lateral level - like the three elements on one level down from the root, make those the segments?
Or what about just pure machine learning for this? Just train a custom segmenter by showing it where you would break sentences, and don’t do any explicit syntax parsing?
 for head in heads:
 words = 
 for word in words:
 seen.add
 chunk = (' '.join)
 chunks.append(  )
 unseen = 
 chunk = ' '.join
 chunks.append(  )
chunks = sorted
for ii, chunk in chunks:
 print
Is this just going to the break-points in the sentence the program identified and pulling out all the words consecutively? Or, what is this doing?
Thank you",,LanguageTechnology,rs22ru
"It seems like things like HuggingFace and Spacy and whatever have done some harm to NLP as a whole.
for instance, I've heard NLP engineers have less pay potential compared to computer vision folk due to most models just being run through their pipelines.
also, it seems difficult to find tutorials post _NUMBER_ on topics like NER and such from scratch.
Everything is getting abstracted to API's and fewer people are learning things from the ground up.
What do you think?",,LanguageTechnology,rsrwqa
"With recent breakthroughs in AI, humans have become more reliant on AI to address real-world problems. This makes humans’ ability to learn and act on knowledge just as essential as a computer’s. Humans learn and gather information through learning and experience to understand everything from their immediate surroundings. The ability to comprehend and solve issues, and separate facts from absurdities, increases as the knowledge base grows. However, such knowledge is lacking in AI systems, restricting their ability to adapt to atypical problem data.
Previous studies show that pre-trained language models improve performance on various natural language interpretation and generating tasks.
A recent work of researchers at Baidu, in collaboration with Peng Cheng Laboratory , release PCL-BAIDU Wenxin , a pre-training language model with _NUMBER_ billion parameters. It is the world’s first knowledge-enhanced multi-hundred billion parameter model and its largest Chinese singleton model. 
You can read the short summary here: _URL_ _URL_ 
Paper: _URL_",,LanguageTechnology,rrgrnf
Has anyone here used Snorkel AI's LabelModel for automatically labeling text? Have you found it to be super slow?,,LanguageTechnology,s7vype
"What would be the steps involved in doing such a thing? Basically I have data from a research, 
 the form of a dataframe including: participant ID, response ID, single word object, and response, which consists of a description of the function of the single word object. Some paricipants have had more than one response from the same object. Object is the same in the whole dataset, and category codings have been done by someone else. Basically responses need to be categorized into these coded categories, denoting similar responses. I want to construct embeddings of the responses and feed them into a CNN. How can this be done quickest? I don't have much knowledge and all the information online is very overwhelming.
Basically, I want to use the bert uncased model, , but I guess I do need to average the world embeddings. Also, how do I tokenize the whole column of responses, and how to add the object into the mix since it is needed as the response and object are connected. Don't expect someone to give me a tutorial in the comments, but a list of general steps to take in this context would be incredibly helpful. You will save my life and my graduation.",,LanguageTechnology,s0p3yw
"I currently have a list of sentence fragments that loosely describe listing for sales for houses/apt/mansions etc. 
They might look something like this:
*\*
*\*
*\*
I want to apply labels  to these fragments to ""standardized"" the language which I can then use to process later. Knowing to group the following is important:
""large pool"" --> has swimming pool""
""garden with swim area"" --> ""has swimming pool""
The ""keywords"" I might want to use for the examples:
_NUMBER_. \ ---> \
_NUMBER_. \---> \
_NUMBER_. \---> \
I do not need to ""capture"" all the descriptions from the sentence fragments. And at least, I want to be able to grab the lowest hanging fruit first 
I see that I have some issues:
_NUMBER_. How do I break down these ""sentence fragments""? So that analysis can be done?
_NUMBER_. How can I ""group"" text that shows up so that I know what categories I want to create? Even better, if groupings can be automatically created/suggested
_NUMBER_. Even if I have ""labels"" that I want to assign a set of fragments how do I train a model to actually do this? (Like if I spent _NUMBER_ hours  labeling some very basic categories.... how do I use this?)
One possible wrinkle I have, is that I do not care which ""sentence fragment"" correspond to which label.  - therefore it is difficult for me to map a ""sentence fragment DIRECTLY to a group with heuristics"" . In the end, I do not necessarily care  which of the sentence fragments actually correspond to the label, just that this example should have the given labels.
I hope my problem description makes sense, and looking for any type of directed help/ approaches. I have looked at ""tokenization"", ""word count"", ""bag of words"" etc but I am unable to understand it enough to see the full picture of how to use it.
Any comments appreciated!
\",,LanguageTechnology,rqe7js
"Hi there, 
I've recently graduated with a BA in Linguistics and I'm currently pursuing a career in Computational Linguistics. I plan on applying to an Msc in a CompLing related degree in a year or two, but I'm currently taking some time off to relax and also learn Python Coding and polish my math skills. 
However, learning Python from scratch and also learning it independently has been really difficult as I find myself stuck often with nobody that I could talk to about Python, and also I find myself lacking the motivation to keep going. 
It would be really nice and helpful if I had a few people I can go to regarding Python-related things. We could motivate each / help each other out etc. 
Please let me know if you're interested!",,LanguageTechnology,rpmr79
"For example, let's say I have the first sense of the verb ""go"" and the first sense of the verb ""walk"". I want to see the connection between these two words. In other words, if I start with go#_NUMBER_ , how can I arrive at walk#_NUMBER_ ?",,LanguageTechnology,sc2u7d
"In the last few decades, neural networks have been used for a wide range of tasks, including image segmentation, natural language processing, and time-series forecasting. 
One promising use of deep neural networks is embedding, a method for representing discrete variables as continuous vectors. An embedding is a low-dimensional space into which high-dimensional vectors can be translated, making it easy for computers to understand the relationships between those concepts. Numerically similar embeddings are also semantically identical. Word embeddings for machine translation and entity embeddings for categorical data are two applications of this approach. **Continue Reading** _URL_
Paper: _URL_
Documentation: _URL_",,LanguageTechnology,sdbq8w
"QQP is a dataset of duplicate and non-duplicate question pairs from Quora. I think it was originally developed as part of a Kaggle competition:
_URL_
The competition released a training set with labels, and a test set without labels. QQP has subsequently been used in many papers developing new architectures for document similarity and duplicate detection, but unfortunately, as far as I can tell, there is no standard train/dev/test split of the dataset *that has labels for each of train, dev, and test*, and therefore people make up their own splits on the Kaggle training set. Am I mistaken? Is there a widely agreed on train/dev/split of this dataset with labels for each split?",,LanguageTechnology,sbpmnj
"Hello fellow enthusiasts, 
I have a corpus of 150k documents, and their respective OCR outputs. 
I'd like to assign a Readability score to each document, is there a metric out there for something like that? 
In retrospect to my OCR extraction, which took almost a month of runtime to run, I *could* have extracted an OCR-accuracy score along with my strings. I'd like to find an alternative solution instead of re-running it. Knowledge for next time, anyways...
I'm open to all thoughts and considerations.",,LanguageTechnology,s7r2pj
"Hello fellow researchers!
Do you read a lot of Scientific Papers?
Have you ever wondered what are the overarching themes in the papers that you've read and how all the papers are semantically connected to one another?
Look no further!
Leverage the power of NLP Topic Modeling, Semantic Similarity, and Network analysis to study the themes and semantic relations within a corpus of research papers. Just \`pip install stripnet\`
_EMOJI_ Generate the STriP Network on your own collection of research papers with just three lines of code!
_EMOJI_ Interactive plots to quickly identify research themes and most important papers
_EMOJI_ This is only the initial release, with lots of work planned.
_EMOJI_ Github: _URL_ _URL_
 
_EMOJI_ If you get the chance to play around, please share your feedback. Please leave a ⭐ to let me know that STriP Net has been helpful to you so that I can dedicate more of my time working on it.",,LanguageTechnology,rxbn4x
"Hi all, I'm working on a project to build a set of language models for the Maldivian language of Dhivehi. It's a lot of fun and super interesting, the first step  has been building a tokenizer that handles the language and its unique Thaana script. I just published a video](_URL_ and (_URL_ ([link if you hit paywall _URL_ explaining the steps and each of the components in a tokenizer .
I hope some of you find it useful, lmk what you think - thanks!",,LanguageTechnology,rqitqe
"Has work been done on selecting a minimum subset of relation types?
Ideally it could be reduced to just one. It would probably be one of the first words that that children learn. Something like ""is"" or ""has"".
Having just one type of relation would greatly simplify the representation.
""Is"" could represent categories ""cat is animal""
""Has"" could represent parts ""cat has tongue""
So what I'm thinking is that ""has"" would be a prime candidate as a single sufficient relation type, because categories and subcategories could be determined easily without any relation between entities: if one entity  has a subset of relations that another entity  has, then it means that ""cat is animal"".
To take the other route and use only ""is""  to infer parts from it -- I don't know how it could be easily done.
Anyway, perhaps it is possible to use any _NUMBER_ relation and infer all others based on that, the question is which is more natural for language as we commonly use it.",,LanguageTechnology,rpuft7
"Does anybody know of a leading AI model for glossary creation?
I’m considering using Spacy for this but so far I found their entity recognition and even their segmentation to be good but not necessarily flawless.
I could stick it custom trained models for sure, it honestly might not be that hard.
I’m wondering if anybody has gone before me here, though.
An auto-glossary creation tool at minimum should:
_NUMBER_. Recognise terms, not necessarily entities. Entities appear to be more trivial, like even just years and numbers come up sometimes. Terms are important keywords.
_NUMBER_. Retrieve context/example sentences from the source documents for each word.. AI is not strictly necessary for this, but it could be leveraged in deciding which sentence containing a term is most “representative”. Plus, AI would come in handy for lemma-matching - it should be able to search for any grammatical form of a word in source text, and not match “crudely” as in maybe a homonym of a word.
_NUMBER_. Ideally, it should auto-categorize terms . 
So: this is the project I’m currently working on. Has anybody already done something like this, ready to go?
Thank you",,LanguageTechnology,rqdq05
"Hello Geeks, I am trying to solve the problem of dangling modifiers and have not been able to think of any solutions or a better way to put this would be from where to start to solve the problem. If you people have any solution or any pointers which you could share it will be really helpful.
Dangling Modifier Examples -
_NUMBER_. Orig - Fumbling in her purse, the keys could not be found.
_NUMBER_. Modified - Fumbling in her purse, she could not find the keys.
_NUMBER_. Orig - Having injured his dominant hand, it was difficult to write the exam.
_NUMBER_. Modified - Having injured his dominant hand, John had difficulty writing the exam.
Thank you.",,LanguageTechnology,sc54sq
view of liwc dictionary _URL_,,LanguageTechnology,sbkae4
"Has anyone worked there as a linguist, ML engineer, etc.? What is the environment/ culture  like?",,LanguageTechnology,s7mosh
"Hey Everyone,
I'm building a plugin for end-to-end neural search  in Opensearch and I'd love to hear any suggestions from the NLP community of what you might find useful or about issues you've had with Opensearch/neural search in the past.
Despite the Elasticsearch website claiming in many places that you can do ""machine learning"" with Elasticsearch, I've found that it's not straight forward at all to use neural search algos with ES/Opensearch. In most cases , you have to implement the ML algorithm yourself and you only get to use ES as a storage layer. Some 3rd party plug and play frameworks that support ES seem quite promising but also lack functionality in terms of data retrieval - for example, Cherche seems to enforce that users first use tf-idf to retrieve documents before putting them through neural search.
I want to build a plugin/service that will allow users to more readily take advantage of the vector functionality and neural search in general. I've found the following issues:
_NUMBER_. Opensearch/ES have added a great deal in terms of functionality to allow for vector search , but it seems entirely up to the user to encode the word embeddings. Therefore, users must add code to manually encode any documents and queries into their chosen embeddings before searching/adding data. I think users having their embeddings is generally a good idea if they want a high level of optimisation, but for many use cases, pretrained embeddings should be a ""good enough"" solution.
_NUMBER_. If the data is in text format, it cannot easily be converted into a format to be used with algorithms like SBERT etc without reindexing the entire index and running it through a custom script to change the data into a vector format.
_NUMBER_. I'd suspect for many users who arn't NLP experts, navigating all of the potential options for embeddings/Neural Search architecture could be quite overwhelming. Having a configurable plugin where they can try different options would likely help them to accelerate getting started.
I think letting users have their own embeddings makes sense from an optimisation perspective but I think also it would be amazing to have an end to end solution where you can connect different algos directly into Opensearch. I'm also exploring extending this and allowing users to refresh/update these embeddings to continually improve them.
Let me know what you think, open to any suggestions! If you want to keep up to date with this, here is a google form _URL_ _URL_",,LanguageTechnology,s5ethj
"hi r/LanguageTechnology,
I'm working on a text classification problem, where I want to classify the textual data into different domains/categories.
We have tried couple of different approaches, like Topic Modelling and BERT. Topic Modelling didn't give out the expected results and in the case of BERT the accuracy were not at the desired level.
What are the other methodology which I can look into for this particular task?
Are there any ways in which we can improve the accuracy with on these models?",,LanguageTechnology,rx9rmp
"So, say I'm attempting to label a training data set for a sentence classification model. What would the best tool to load a bunch of documents, have each document be split into sentences, and then show me each sentence so I can label it myself. Any ideas on what I should use?",,LanguageTechnology,rrn0jg
"Large-scale language model scaling has resulted in considerable quality gains in natural language understanding , generation , and multilingual neural machine translation . One typical method for creating a more extensive model is to increase the depth  and breadth , essentially expanding the network’s existing dimensions. Such dense models take an input sequence  and route each token through the whole network, activating every layer and parameter. While these big, dense models have shown cutting-edge outcomes on various natural language processing  applications, their training costs rise linearly with model size.
Building sparsely activated models based on a mixture of experts  , where each token supplied to the network follows a distinct subnetwork by bypassing some of the model parameters, is an alternative and more common technique. Small router networks that are educated with the rest decide how to distribute input tokens to each subnetwork . This enables researchers to increase the model size  without increasing training costs proportionally. ***Continue Reading*** _URL_
Paper: _URL_ _URL_",,LanguageTechnology,s9sjqz
"Hi all, 
So I've been trying to download the wikipedia split that is used for evaluating Dense Passage Retrieval  _URL_ 
I just pulled up a google colaboratory session and followed their instructions to download the dataset as shown below.
`python data/download_data.py \`
 `--resource  \`
 ``
I don't know why, but for some reason, the colab automatically exits the download cell , and all I get is a .tmp file.
I believe I should be getting a .tsv file as instructed here  :
_URL_ _URL_
I am attaching the .ipynb file here too for convenience.
_URL_ _URL_
Anyone know what I'm doing wrong?
Thanks.",,LanguageTechnology,sa9n1f
"Hey there! I'm trying to build an HMM tagger from scratch in Python, but I'm not so sure of how to go about it. Do you know of any good resources or guides that could be useful to me?",,LanguageTechnology,rzs4ah
"Hi, I am trying to get into NLP with knowledge in CV. In CV, tasks like object detection have SOTA ensemble methods like Weighted Box Fusion. I was wondering if NER has the equivalent to WBF in terms of ensembling Transformer models of different folds.",,LanguageTechnology,rxiklt
"Apologies if this is really simple, I just want to double check. If I have a contextualized representation of a sentence, say ""Look at this cat."", and I want to extract the contextualized token representation of ""cat"", how would I go about doing this?
I was thinking of first extracting the word ID with `id = tokenizer.convert_tokens_to_ids(tokenizer.tokenize)`, then match the word ID to its token position in the sentence with `tok_index = input_ids.index`. Later, after passing the sentence through the encoder, I can use tok\_index to access the specific embedding. I'm wondering if this idea makes sense. Thanks in advance!",,LanguageTechnology,s2ufj6
"Hi, I have a project in mind and the first ""mini-project"" within it is to assign a Score to a text depending on the depth of the vocabulary. Similar to what Grammarly does. I know I have to use a dictionary, but beyond that I don't have much.
 A bonus would be to also assign a ""Class"" to the text depending on the vocabulary used; ex: While a Scientist and a Writer might have very similar ""depth"" Scores, their vocabularies are not the same, the program should assign to which ""Class"" does the text belong. But this might be a bit hard.",,LanguageTechnology,rs5rqj
"Hi,
I’m part of an art group from Switzerland currently studying at HSLU Design & Arts (_URL_ _URL_
The group consists of:
Karim Beji (_URL_ [_URL_ _URL_
Emanuel Bohnenblust (_URL_ _URL_
Lea Karabash (_URL_ _URL_
Yen Shih-hsuan (_URL_ [_URL_ _URL_
At the moment, we are working on a project on the topic if AI can augment the happiness of humans. To answer this question, we are mainly working with chatbots. The end result is going to be an exhibition at the end of March. 
For that exhibition, we want to conduct a trial in which people from over the world chat with a chatbot to find out if and how it augments the mood of the participants. 
We would give you access to a GPT-_NUMBER_  chatbot and ask you to a) record yourself through a webcam  while you are chatting and b) simultaneously screen record the chat window. 
In the exhibition we would have a) a book with all the chats and b) small videos with your faces  to assess your mood. 
We would have a Zoom meeting beforehand to discuss everything.
Looking forward to your message!",,LanguageTechnology,sgypv1
"Dear all, 
If there are any NLP/ML engineers, DS, or researchers out there, I could really use some advice. 
I am graduating from my MS in Economics with a full-time job lined job as a DS at a well-known fintech company. However, it is driving me crazy to find a clear path forward to pursue a more NLP-involved job down the line. 
Here is what I currently have that can be classified as NLP ""experiences"": 
_NUMBER_. Past Internships! I have done anything from Product management intern for data products powered by NLP to Management Consultant doing research on the data collection strategies that a client could take to improve their NLP classification outcome 
_NUMBER_. Research! I am writing a paper with researchers from NLP for applying NLP techniques to public policy related documents and is due to publish in the next couple of months 
_NUMBER_. Current job! The team that I am currently on and hired into  uses a lot of NLP for insights discovery. We also plan on launching a large scale NLP product down the line which I will be very involved in given our very lean corporate structure 
Why I think I will have a hard time advancing in the field: 
_NUMBER_. I do not have a CS undergrad or MS in CS 
_NUMBER_. My background in economics dictated that I am good at math but not at linguistics 
_NUMBER_. I do not come from a hyper prestigious school like Stanford or MIT but a mid-tier school in the East Coast 
I feel everyone in the field is so overqualified for what they are doing ! I have no clue what to do ??? 
Should I go get an MSCS to compete down the line? How does moving up in NLP careers work? Can any folks shine some light on a very confused young person! 
I will literally take any suggestions or advice haha. thank u y'all!",,LanguageTechnology,rp96xk
Hi everyone! I am a BA graduate with background in linguistics and self-taught programmer. I have been looking at two programs in language technology Msc. in Speech and Language Processing in Edinburgh (_URL_ and Human Language Technology ([_URL_ _URL_ in the States and I was wondering I anyone has any advice in which would be a better fit? I am hoping to start working in the industry after graduating  but I am worried about which one would help me be more prepared. The one in Edinburgh is a one-year program while the one in AZ is a two-year program. Would appreciate any help with your viewpoints on this :),,LanguageTechnology,s1wvwl
"Example: Qual ity -> quality
I'm using pytesseract to transcribe pdfs, and unfortunately one of the issues is PDF often splits up words at the end of column in two parts . I'm trying to figure out a way to detect when words don't make sense separately but make a normal word combined ",,LanguageTechnology,s0l796
"This feels intentional... no?
Also: _URL_ _URL_
Found some others that are sorely lacking too—what a joke.",,LanguageTechnology,rynylf
"Hello,
I am trying to do some basic preprocessing on _NUMBER_.5GB of text. More specifically, I want to do tokenization, lower casing, remove stop words and top-k words. I need to use spacy because the dataset is in greek and I think other libraries can't support this.
However, when I try to apply what the spacy documentation or most of the guides/resources mention, it takes forever to complete even half of the techniques that I mentioned above. I stop the execution every time.
Could you provide me with some resources that I might have missed, in order to make this procedure run faster?
Thanks in advance",,LanguageTechnology,rxtn0v
"Hello everyone! 
Did you hear about word embeddings? 
Nooo? Then you need to learn about it. 
But if seriously, I'm in process of create some corpus for word embedding for romanian language . 
That why I decide to start this project. 
What are the goals I am pursuing? 
_NUMBER_. The text must be clean; 
_NUMBER_. To be learned on a lot of text; 
_NUMBER_. And the corpus must be accurate. 
Here you can see some of them:
_URL_ _URL_ 
The rest will appear in the near future, and I will try to do them as soon as possible. Maybe some changes and fixes will appear, but I'll keep you posted. 
And of course you can leave a comment, what you like or dislike. I will be very grateful. 
Respectfully",,LanguageTechnology,rphw8k
"I am trying to implement word2vec for large corpus may be in billions of words if possible. I am following Word2vec tensorflow _URL_ as a reference, where they have used a vocab size of _NUMBER_ and sequence length of _NUMBER_. My question is, if I use other corpus with billions of words should I limit the vocab size to some numbers like _NUMBER_ and sequence length or create vocab for all the unique words present in the corpus ?
I want to know how did gensim and other library trained their model on large corpus, did they limit the size of vocabulary or trained on all the unique words present in the corpus ?",,LanguageTechnology,s6qaog
"I did a search in this sub for similar posts, and have gathered that the two best beginner resources in general, as recommended by this sub, would be the NTLK docs, _URL_ and the Jurafsky/Martin book [_URL_ _URL_ .
However, as an outsider to this field, I fear I'll start going through all of this learning material and exercises that are based around processing the English language, and then not have much to apply when it comes to working with the Chinese language.
I'd be really appreciative for:
_NUMBER_. Bloggers, resources, youtube videos that do a decent job of how to build up knowledge of this domain from _NUMBER_, but specifically with NLP of Chinese in mind.
_NUMBER_. If this doesn't exisit, then if someone would be so kind as to tell me which chapters/sections of the above resources  would **not** be relevant for doing NLP of Chinese?",,LanguageTechnology,sexrvx
"So I'm working on some early English text. For example, sometimes ""up"" is spelled ""vp"", or ""himself"" might be ""himselfe""... or it might not. Is there any advice or good practice for how to handle stemming/lemmata etc.? Has anyone got experience doing word embeddings with this kind of data?",,LanguageTechnology,rwihy9
"Hey Im searching the web for a document with following criteria for a nlp model 
_NUMBER_+ pages 
contain :
knowledge
rules 
instructions
\--- no speculation unclear content inside like research papers 
\------ mostly text no relevant pictures or formulas 
I would greatly appropriate any help and tips",,LanguageTechnology,s6c319
For example a project similar to scanning for cyberbullying comments but one that has been done already,,LanguageTechnology,s47pz3
"Hi everybody,
I've become interested in NLP and would like to get started on preparing for some master's courses. My background is modern languages, and I've been teaching myself Python alongside my internship. I'd love to work in machine translation, or even build my own machine translation engine. What advice do you all have for a clueless boi on getting started in NLP? Thank you!",,LanguageTechnology,s99cke
"Recent advancements in end-to-end deep learning models have enabled new and intriguing Text-to-Speech  use-cases with excellent natural-sounding outcomes. However, the majority of these models are trained on large datasets recorded with a single speaker in a professional setting. Expanding solutions to numerous languages and speakers is not viable for everyone in this situation. It is more challenging for low-resource languages not often studied by mainstream research.
Coqui’s team has designed ‘YourTTS](_URL_ to overcome these limits and provide zero-shot TTS to low-resource languages. It can synthesize voices in various languages and drastically reduce data requirements by transferring information between the training set. [***Continue Reading*** _URL_
Paper: _URL_
Github: _URL_",,LanguageTechnology,rz8epu
"Hi all, I put together an article on applying AugSBERT _URL_ from Thakur, Reimers, etc for domain transfer tasks. Great for improving sentence transformer performance in a domain where we don't have data, but we *do have* data in another similar domain. I hope it's useful, let me know if you have any questions, ideas, etc - thanks!",,LanguageTechnology,rvy09b
"Hi everyone!
Can someone explain to me how query, key and value vectors are received from the input word embeddings to an encoder or decoder layer? I see how they  are used in the multihead attention layer, but I dont understand where they come from. They have to depend on the input word embedding, but how?
In the original transformer paper  _URL_ I only found those vectors mentioned in chapter _NUMBER_:
*An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key* 
If anyone could help me answering this question, it would be great!
EDIT:
My thanks to /u/Brudaks, /u/boodleboodle and /u/mehtajineshs for clarification on this by providing explanations and resources.
I do understand now, that the vectors depend on the output from previous layers and are received by multiplying previous layer output  with randomly initialized matrices, like other weights in an FF network for example are initialized randomly as well.
And like weights in a feed forward network, the matrices for receiving QKV vectors are learned by backprop.",,LanguageTechnology,rri6dm
"Hi,
How to measure the accuracy of a generative chat bot or any generative model?
Is it possible?",,LanguageTechnology,rnnw4s
"I have a question around personalization for semantic searchwhen using semantic search api through a third party, for instance pinecone or any vector db company, how is personalization possible? Do you also pass the last _NUMBER_ session actions/ or info about the user through the api?",,LanguageTechnology,saiq21
"Hello all,
I would like to ask you which kind of tools do you use to search for something that is not in the training model when you do a computational analysis.
Let's say that I want to search for errors in a corpus , while the lemmatization procedure fails because these elements are wrong - how can you deal automatically with such events?
I hope that the question is clear enough :)",,LanguageTechnology,rqegda
"Hello guys, i'm data science student, i'm trying to replicate WebNLG _NUMBER_ challenge with OpenNMT-tf.
I have already performed the same challenge with OpenNMT-py and everything went well.
When using the tensoflow version, some doubts arose:
* how to build vocabularies from webnlg\_baseline\_input.py output files: \. since in the tensorflow version a transformation step in a bpe file is required;
* how to build the default model of openNMT-py ;
I tried to follow this notebook but, given the doubts expressed above, the results were not the same.
(_URL_ _URL_ 
How can I do? Thanks all.",,LanguageTechnology,sesv7z
"Hi,
I am looking at multi-lable classification for Twitter-like data. Does anybody know if any open source project  has any pre-trained models ready to use?
I am looking at classification taxonomy such as IAB v2 categories _URL_ or Wikipedia categories. 
Thanks!",,LanguageTechnology,s300v8
"ML and NLP Research Highlights of _NUMBER_ _URL_ by Sebastian Ruder, actually research scientist at Google in London, ex-DeepMind: Universal Models, Massive Multi-task Learning, Beyond the Transformer , Prompting, Efficient Methods, Benchmarking, Conditional Image Generation, ML for Science, Code Synthesis, Bias, Retrieval Augmentation, Token-free Models, Temporal Adaptation, Importance of Data and Meta-learning.",,LanguageTechnology,sds8q8
"So, I was in an interview and I was asked so many questions about statistical details on text data. For example 
_NUMBER_. How would you sample million sentences from billions of sentences? What strategies will you use for sampling?
_NUMBER_. Having sampled, how would determine that the sampled data follows actual data distribution? 
Follow up for these questions were, When will you decide to re-train your model. 
Now I am confused about how to perform such statistical analysis over text data. I have understanding about DL approaches within NLP, but stats is something bugging me a lot during the interviews. 
Please advice me how to solve these about mentioned questions as well as where should I start working/learning on stats for such questions. Will be very helpful.",,LanguageTechnology,sbhz36
"Hi, 
I'm looking for the latest papers on different subjects  . 
Typing on google is not helping. I find only commercial solutions. 
Thank you",,LanguageTechnology,s0q1m4
"So, for context, I've only just started learning NLP, and I've just encountered the word2vec algorithm for the first time. This algorithm calculates the probability of a word appearing at a position in a sentence as a function of what it's surrounding words are, weighted by the distance from that central word, learned from a large corpus of language. So for instance, if you fed it an incomplete sentence: ""the cat jumped over the ... "", it would assign high probabilities to words like ""table"", ""mat"", ""bed"", and assign low probabilities to words like ""blue"", ""boil"", ""running"". 
Are there any human languages in the world for which the assumptions which the algorithm are built on break? For example, any languages for which the context of a word is *inversely* proportional to it's semantic meaning, rather than proportional as this algorithm assumes? 
Are there any other interesting concepts in NLP which work for some languages, but not others?",,LanguageTechnology,rzp9h6
I’m planning to add this feature to a search bar. Any starting package/model/tool suggestion?,,LanguageTechnology,sczm2z
"Hey Everyone,
I'm building a plugin for end-to-end neural search  in Opensearch and I'd love to hear any suggestions from the NLP community of what you might find useful or about issues you've had with Opensearch/neural search in the past.
Despite the Elasticsearch website claiming in many places that you can do ""machine learning"" with Elasticsearch, I've found that it's not straight forward at all to use neural search algos with ES/Opensearch. In most cases , you have to implement the ML algorithm yourself and you only get to use ES as a storage layer. Some 3rd party plug and play frameworks that support ES seem quite promising but also lack functionality in terms of data retrieval - for example, Cherche seems to enforce that users first retrieve documents using an algorithm like tf-idf before putting them through neural search.
I want to build a plugin/service that will allow users to more readily take advantage of the vector functionality and neural search in general. I've found the following issues:
_NUMBER_. Opensearch/ES have added a great deal in terms of functionality to allow for vector search , but it seems entirely up to the user to encode the word embeddings. Therefore, users must add code to manually encode any documents and queries into their chosen embeddings before searching/adding data. I think users having their embeddings is generally a good idea if they want a high level of optimisation, but for many use cases, pretrained embeddings should be a ""good enough"" solution.
_NUMBER_. If the data is in text format, it cannot easily be converted into a format to be used with algorithms like SBERT etc without reindexing the entire index and running it through a custom script to change the data into a vector format.
_NUMBER_. I'd suspect for many users who arn't NLP experts, navigating all of the potential options for embeddings/Neural Search architecture could be quite overwhelming. Having a configurable plugin where they can try different options would likely help them to accelerate getting started.
I think letting users have their own embeddings makes sense from an optimisation perspective but I think also it would be amazing to have an end to end solution where you can connect different algos directly into Opensearch. I'm also exploring extending this and allowing users to refresh/update these embeddings to continually improve them.
Let me know what you think, open to any suggestions! If you want to keep up to date with this, here is a google form _URL_ _URL_",,LanguageTechnology,s5ethj
"As compared to multiple. My tokenizer currently splits “can’t” to “can”, “‘“, and “t”. I’m using Python",,LanguageTechnology,rx30rj
"I am quite used to the Machine learning aspects of NLP, but I am lacking knowledge on how to make raw texts accessible and how to handle meta data. Is there a good book on this - preferably in Python?",,LanguageTechnology,sh08de
Hi all. I was wondering what are the most interesting academic ethics issues we have to account for while researching in this area?,,LanguageTechnology,sg4teu
"I am looking for a good way to use pre-trained embedding models  in my C++ application running on a local CPU.
My solution so far: I am using a compiled Tensorflow C DLL in combination with cppflow (_URL_ However, I get problems when I take models which use operations from the tensorflow_text python module since I don’t know how to get their C++ API.
Has somebody experience in doing so? Or in general, has someone used local embedding models in a C++ app before?",,LanguageTechnology,sff0hf
"I am trying to vectorize _NUMBER_-news group data 
 using tensorflow TextVectorization layer 
 but in TextVectorization layer 
 if I limit the vocab size to some number say _NUMBER_ then it works fine. However if I preprocess the data or do not set the vocab size to some number then I get UnicodeDecodeError: 'utf-_NUMBER_' codec can't decode byte 0xfe in position _NUMBER_: invalid start byte 
 error.
My question is have I done something wrong in preprocessing? Because if I set the vocab size to _NUMBER_ and do the same preprocessing then this wont work. Also, do I need to set vocab size in 'TextVectorization\` but the docs says it can have unlimited size?
 
Here is what I did :
i. Get the list of files:
 train_dir_list = 
 for i in os.listdir:
 f = os.path.join
 for j in os.listdir:
 train_dir_list.append(os.path.join) 
ii. Create tensorflow data
 train_data = tf.data.TextLineDataset 
iii. Preprocess data
 def preprocess:
 lower = tf.strings.lower
 # remove emails
 email_removed = tf.strings.regex_replace
 # remove numbers
 number_removed = tf.strings.regex_replace
 # remove punctuations
 punctuation_removed = tf.strings.regex_replace( number_removed, '' % re.escape, ' ' )
 # remove multiple blank spaces
 multiple_space_removed =tf.strings.reduce_join(tf.strings.split, separator="" "")
 return multiple_space_removed 
iv. Create vectorizer: In here if I remove the standardize=preprocess 
 and keep the vocab\_size and sequence\_length 
 it works fine. But if I use standardize=preprocess 
 either with same vocab\_size and sequence\_length 
 or do not use standardize=preprocess 
 but keep the vocab\_size and sequence\_length 
 empty or as default then it gives the UnicodeDecodeError:
 vocab_size = _NUMBER_ sequence_length = _NUMBER_ 
This works fine:
 vectorize_layer = layers.TextVectorization 
This will throw error:
 vectorize_layer = layers.TextVectorization 
This will also give error :
 vectorize_layer = layers.TextVectorization 
v. calling adapt works fine
 vectorize_layer.adapt(train_data.batch) 
vi. This is were the error is thrown
 vectorize_layer.get_vocabulary 
Also, on looking up the vocab size when the error is produced, the vocab size is only around _NUMBER_-_NUMBER_",,LanguageTechnology,s6wvu7
Also possible in Python programming language,,LanguageTechnology,s2egjq
"Hello all,
Many of us are having a hard time speeding up our Transformer based NLP models for inference in production.
So I thought it would be worth writing an article that summarizes the options one should consider :
_URL_ _URL_
I hope you'll find it useful. If you can think of additional options, please let me know and I'll add them to the article!
Julien",,LanguageTechnology,s0h5nv
"What model would you recommend for extractive summarization?
I have a dataset of restaurant menus and I want to extract the dishes with their prices.
So the input will be the entire menu and the output will csv like text with the dishes as the first column and their prices as the second.
I was thinking of T5 but I just dabble in NLP maybe you have a better idea?
Thanks",,LanguageTechnology,rrehwy
"I tried using different packages but they all still just return a ""None"" value whenever i try to stem a word. Is it because of my python version ?",,LanguageTechnology,rsx4cz
"Hello Everyone,
I am a newbie in NLP research. My question is - How should we benchmark a new Language dataset/corpus  when there is no publicly available dataset for that particular language? Also what are the possible directions to perform evaluation on the newly prepared dataset. Need suggestions, please.",,LanguageTechnology,sdurp5
"Recently, I was reading some literature about text summarization and came across its evaluation metric, the ""ROUGE"" score. From what I understood from preliminary reading, the ROUGE score only measures n-gram overlap between candidate summary and reference summary which wrongly penalizes abstractive summaries containing different n-grams but conveying the same meaning. There's also a **BERTScore** metric  that does not suffer from these issues of ROUGE and computes contextual similarity rather than just n-gram overlap. How can I assess if BERTScore is a better evaluation metric compared to ROUGE? ",,LanguageTechnology,s5ao3z
"I'm thinking of creating a ChatBot. The function of the Chatbot will be the following:
Keywords: 
The keyword is used to identify the sentences which will contain salary information
Case _NUMBER_:
User: My Salary is 300K per month
Chatbot: Store _NUMBER_ in the database
Case _NUMBER_:
User: My monthly compensation is 2Lkh
ChatBot: Store _NUMBER_ in database
Explanation: 2Lkh is an amount that equates to _NUMBER_. Compensation is a word similar to one of the keywords
Case _NUMBER_:
User: My salary is quite low
ChatBot: No action
Case _NUMBER_:
User: My salary is 1M per year
ChatBot: Store _NUMBER_ in the database
For now, I'm only considering currency in rupees, and salary is stored regardless of whether it is monthly or yearly. Can you suggest to me a pipeline/Steps that I have to create in order to come up with this ChatBot?
I'm also not able to find the proper dataset for this task.",,LanguageTechnology,s7nrja
"The problem with my university is we have to first write proposal and only after approved we get a supervisor on thesis domain. But the proposal take lot of effort and time. Even choosing topic takes lot of time. So reddit is the only option I have.
I wanted my master thesis to be moderatly hard, so choosed to do empirical study instead and was finding hard to choose the topic. I found some research papers do empirical study, for eg: _URL_ _URL_ What do you guys think? Is doing empirical study on open source model like BERT is appropriate for master thesis?",,LanguageTechnology,s2gd3g
"Hello all,
Which are your favorite tools to visualize a corpus? 
Do you prefer a desktop or web solution? And which kind of analyses can you perform with such tools ?
Thanks in advance",,LanguageTechnology,rxc8t8
"I was offered a dream  NLP developer role in an AI project after many years in non-tech positions , which I was told required some knowledge of Python, which I have been teaching myself for the past _NUMBER_ months. I just realised however that their NLP tools are basically automated/low code. Will this hinder my progression into a NLP career? How do I leverage my experience from that type of role into something more er, substantial in the NLP space? My plan was/ is to teach myself Pandas, NumPy, Scikit-learn, NLTK, TensorFlow, etc. this year and complete some projects on my own. I hope that's sufficient for the next step.",,LanguageTechnology,s98t4o
"Do you know of any such experiments?
I keep reading about LLMs using external memory resources, but could they also be used to generate resources such as Knowledge Graphs on a huge scale?
Edit: preliminary results from a little experimentation with GPT-_NUMBER_ 
**Prompt**
knowledge graph described by a list of relations:
finger -> part of -> hand
finger -> subclass of -> digit
music -> subclass of -> sound
Earth -> instance of -> terrestrial planet
green -> subclass of -> color
pathogen -> opposite of -> nonpathogenic organism
color -> subclass of -> property
music -> part of -> culture
culture -> opposite of -> nature",,LanguageTechnology,rm5pzm
"Hey,
Could someone please simply explain the CKY algorithm, how it works?
let the input be a string I consisting of n characters: a1 ... an.
So the input is just characters, not tokenized words?
let the grammar contain r nonterminal symbols R1 ... Rr, with start symbol R1.
What are non terminal symbols?
let P be an array of booleans. 
This is a three dimensional array? Why does n occur twice?
Initialize all elements of P to false.
for each s = _NUMBER_ to n
 for each unit production Rv → as
 set P = true
What is unit production?
for each l = _NUMBER_ to n -- Length of span
 for each s = _NUMBER_ to n-l+_NUMBER_ -- Start of span
 for each p = _NUMBER_ to l-_NUMBER_ -- Partition of span
 for each production Ra → Rb Rc
 if P and P then set P = true
if P is true then
 I is member of language
else
 I is not member of language
_URL_
Thanks very much",,LanguageTechnology,sh0cip
"Is there any way to differentiate between the absence or inclusion of an object in a sentence? For example: there is not a cat in my yard, the cat disappeared from my yard, a cat does not exist in my yard, etc. versus there is a cat in my yard, the cat appeared in my yard, a cat exists in my yard.
Any help would be greatly appreciated!",,LanguageTechnology,s8j2qk
"_URL_ _URL_
I have put up some  parse trees online as a dataset. Not something as substantial as Penn Treebank, since the trees have NOT been human-edited, but still way more parse trees than from Penn to feed into your later-stage NLP algorithms, free of charge or hassle.
The current format is straight from where they were generated. Suggestions of alternative formats based on ease of use would be heavily appreciated!",,LanguageTechnology,s1d4p9
"I have to process undergraduate and postgraduate student essays using spaCy. One of my first step is to remove citations, both narrative and parenthetical ones. And I am using regex to do this. My regex is getting longer and longer and becoming very unwieldy. Moreover, I am assuming students are using APA 7th and not earlier versions or other styles entirely.
I am unable to get good results using NER or POS so have to rely on regex.
Are there any python NLP packages that will recognise academic citations, both narrative and parenthetical ones? E.g. ""Lee  said ..."", ""... in the study conducted "".",,LanguageTechnology,rvp049
I have _NUMBER_ sentences and _NUMBER_ classes . What can I do to learn something from them?,,LanguageTechnology,s9kbsr
"I want to use pre-trained fasttext model to load word embeddings for all words  in English. I have tried to read up how to do this , and so far haven't been able to get it to work.
I would appreciate it if someone could give me the code for it or point me to some tutorial that would help me do it. 
I am from a Statistics background, and a new Python User. Trying to navigate through everything to get things done !",,LanguageTechnology,sg20z5
"I’d like to challenge myself to write my own dependency parser for natural language  in Python.
I’m picturing taking in the sentence one word at a time and somehow working backwards to figure out the tree structure of the sentence.
Of course, it should start with tokenization. Perhaps part-of-speech tagging is a necessary next step, to attempt to group certain parts of speech that never dislocate from their complements, like “the”, or other words with predictable behavior, like “and” and conjunctions and so on?
Like the game “Mindmaster”, one can work backward layer upon layer to deduct what the original structure of the sentence is.
However, maybe I need effective segmentation for this to work? I’m wondering how periods will affect this procedure. I could ignore them and hope the dependency parse can still work. For example, “and” never appears at the end of a sentence.
Another idea is to try to design a neural network myself rather than using a library like Spacy. I just need to know what architecture is optimal and practice training it a bit.
Anyone have any recommendations on this?
Thanks very much",,LanguageTechnology,sd3sh6
"Hi, I'm looking for an Authorship Attribution dataset with small-medium texts . Looked everywhere but couldn't find any, found a great dataset for large texts  but none for small texts. Would like to search if one exists to hopefully not have to scrap it myself.",,LanguageTechnology,sac5np
"In the BLEU paper _URL_ _URL_ , why **In Example _NUMBER_, Candidate _NUMBER_ achieves a modified unigram precision of _NUMBER_/_NUMBER_; whereas Candidate _NUMBER_ achieves a modified unigram precision of _NUMBER_/_NUMBER_.** ?",,LanguageTechnology,s2neoo
"I’ve been interested in this for a long time, hoping to finally crack the code here.
In Swedish an adjective can be determined or not. Like in English the article “the” vs “a”.
This is a more obscure case, a phrase in a product catalog “On hard floor”.
Not sure what grammarians would say is going on here. It’s a general case so I would say it’s non-determined and they dropped the “a” just for brevity.
On the other hand it is more comparable to the general case which commonly uses the plural in English: “on hard floors”.
I would like to have a convenient system to check what is done in Swedish without just leafing through grammar websites and so on.
I want to access a most convenient Swedish corpus - not a database requiring a sign up but just an easily downloadable dataset, maybe Kaggle?, or as part of some software package like Spacy.
Then I want to execute a formula like “show me matches of sentences of the form “preposition determined adjective noun””. I can develop it from here but this would be a good start.
Does anyone have a suggestion for an accessible corpus with syntax parsing and searching?
Thank you very much",,LanguageTechnology,rmrmk1
"In the BERT paper _URL_ it says that during training it mask a fraction of the words and replaces them with random words:
> The training data generator chooses _NUMBER_% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with  the \ token _NUMBER_% of the time  a random token _NUMBER_% of the time  the unchanged i-th token _NUMBER_% of the time.
I can't wrap my head about the explaination it gives, can somebody point me somewhere about this part?
EDIT: What I don't understand is the justification to do the random word thing.",,LanguageTechnology,rvfi1w
"Hi everyone!
I am currently studying smoothing techniques, specifically Kneser-Ney smoothing. I understand that it helps to handle the case where the next word hasn't appeared in the given context previously. For eg, the corpus could have non zero trigram counts of 'This is a', but no occurrence of the _NUMBER_-gram 'This is a car'.
The count C is captured in the denominator of the lambda term as well, and this lambda term is multiplied with the recursion term. My question is, what if that particular count is actually zero? I hope the following the mini example can make my question clearer - 
Corpus:
<s> <s> You are my friend </s> </s>
<s> <s> They are my enemies </s> </s>
<s> <s> I have friends and enemies </s> </s>
Say we would like to find the probability of the trigram 'are you friend', or P. As per the formulation given in page _NUMBER_ of the document:
_URL_ _URL_
The denominator of two terms consists of the count of the bigram 'are you'. But from the corpus this is zero, and at the same time, each of the individual words 'are' and 'you' aren't UNK, as their unigram counts are _NUMBER_ and _NUMBER_ respectively. So how does the recursion proceed now, since we cannot divide by zero? 
Thank you!",,LanguageTechnology,scyef9
"I am thinking of creating model for extracting entities in a cv such as 
_NUMBER_. Name
_NUMBER_. Address
_NUMBER_. Institute
_NUMBER_. Degree
_NUMBER_. Skill
_NUMBER_. Company
_NUMBER_. School
_NUMBER_. Designation
_NUMBER_. Society - Ex: sport clubs, school societies... . In spaCy there are a very limited no of entities. What about training a model with these data ?",,LanguageTechnology,rm1jdc
"Hi all! I'm proving out an idea for emotion detection using smartphone recordings. Ideally I would like to gather recordings using a web-based application and the MediaRecorder API with smartphones . Does anyone have experience with doing so? Are the results good enough to work with, or am I better off working on a dedicated app with more control over recording?",,LanguageTechnology,rtngyl
"Hi everyone!
I'm an experimental social science researcher who is trying to get into some very basic NLP as a supplementary skillset.
I learned how to use LIWC  during my doctoral program, but haven't done anything related to NLP for at least _NUMBER_ years. I've skimmed through some posts here and someone said ""NLP has progressed a lot more from LIWC since the past couple years"" so I'm trying to get reacclimated with NLP.
Do you guys have any suggestions  on where to start? My goal is first to learn how to do the most basic sentiment analysis and/or any other elementary analyses using R, and then once comfortable, gradually move on to more advanced topics  ).
Another question I had was whether there was anything similar to LIWC in R, but again others seem to have commented on this subreddit that there are better tools than LIWC these days..?
Sorry for the really vague / general question - I would appreciate any comments or pointers!",,LanguageTechnology,rr9wgp
"Long-form question-answering , a paragraph-length answer created in response to an open-ended question, is a growing difficulty in NLP. LFQA systems hold the potential to become one of the most important ways for people to learn about the world, yet their performance currently lags behind that of humans. Existing research has tended to concentrate on two key aspects of the task: information retrieval and synthesis.
Researchers at OpenAI have recently developed WebGPT. They outsource document retrieval to the Microsoft Bing Web Search API and use unsupervised pre-training to produce high-quality synthesis by fine-tuning GPT-_NUMBER_. Rather than striving to improve these factors, they concentrate on integrating them with more consistent training goals. The team leverages human feedback to directly enhance the quality of answers, allowing them to compete with humans in terms of performance.
In this paper, the team offers two significant contributions. They create a text-based web-browsing environment that can be interacted with by a fine-tuned language model. This enables the use of general approaches like imitation learning and reinforcement learning to improve both retrieval and synthesis in an end-to-end manner. The team also creates replies with references, sections collected by the model when exploring web pages. This is critical because it allows labelers to assess the factual accuracy of answers without having to engage in a time-consuming and subjective independent research procedure.
Quick Read: _URL_ _URL_ 
Paper: _URL_ _URL_
Open AI Blog: _URL_",,LanguageTechnology,rmmn9e
"Prod2Vec or Item2Vec produces embedding for items in a latent space. The method is capable of inferring item-item relations even when user information is not available. It's based on NLP model Word2Vec. Click here _URL_ to know more
This project provide a class that encapsulates Item2Vec model (word2vec] as a (_URL_ and [BayesSearchCV _URL_ to find the optimal hyperparameters
_URL_ _URL_",,LanguageTechnology,s8o288
Stanford Online students will present original projects developed in the Natural Language Understanding professional course. Q&A to follow. Register here _URL_,,LanguageTechnology,s0vkki
Feedback appreciated.,,LanguageTechnology,s7jgxr
"I am searching for a natural text to speech or voice cloning program at least of the quality of synthesia.io _URL_ , Don't need the video part though. Preferably open source or something cheap.",,LanguageTechnology,rq2adm
"Hi, 
I am making a chatbot with the use of dialoGPT, but I want it to give different responses even if the same question is asked. 
eg: 
* How are you doing -> I am good
* How are you doing -> I am doing good
* How are you doing -> I am fine, how are you
something like that, so it doesn't seem repetitive.
This question might be stupid tho _EMOJI__EMOJI_",,LanguageTechnology,rmbz8l
"Hi guys, I'm a data science student and i'm trying to build a new dataset using OpenAi framework.
My idea is to use _NUMBER_ phrases and triples to train a data-to-text OpenAi model which given a triple generates of the text.
reading the OpneAi documentation, I found this script for reference:
 import os
 import openai
 
 openai.api_key = os.getenv
 
 response = openai.Completion.create
i would like that the above script follows this scructure: 
prompt = ""Triples: subject, predicate, object.\\nEnglish:text generated""
 import os
 import openai
 
 openai.api_key = ""mykey""
 
 response = openai.Completion.create
expected output from the last triple :
 The Phoenix is a fast food place in the riverside area.
For example, given the webnlg dataset  with this strcuture:
 <entry category=""Airport"" eid=""Id1"" size=""_NUMBER_"">
 <originaltripleset>
 <otriple>Aarhus_Airport | cityServed | ""Aarhus, Denmark""_USER_</otriple>
 </originaltripleset>
 <modifiedtripleset>
 <mtriple>Aarhus_Airport | cityServed | ""Aarhus, Denmark""</mtriple>
 </modifiedtripleset>
 <lex comment=""good"" lid=""Id1"">The Aarhus is the airport of Aarhus, Denmark.</lex>
 <lex comment=""good"" lid=""Id2"">Aarhus Airport serves the city of Aarhus, Denmark.</lex>
 </entry>
 
I would like to randomly extract sentences and their triples from this dataset and use them as training. How can I insert them into the ""prompt"" variable automatically without writing by hand ?.
Thanks!",,LanguageTechnology,s7niat
"January _NUMBER_, _NUMBER_ online: ""Compositional Natural Language Processing on Quantum Computers"" with Konstantinos Meichanetzidis, Cambridge Quantum](_URL_ & (_URL_ Info & RSVP: [_URL_ _URL_
\#NLProc #QuantumComputing",,LanguageTechnology,royiix
"Does anybody know if you can just import part of the Spacy library you need?
I find “import spacy” to be the slowest part of using spacy.
What takes so long to load? All the pipeline scripts or something? Because loading the language model with spacy.load and constructing the doc object with doc are faster than the initial import.
I’d like to just load the pipelines I need or something from the module, like “from spacy import ...”
Does anyone know of this is possible?
Thanks very much",,LanguageTechnology,sd2kmw
"I want to create a model that can trigger specific actions based on the input given to the model. For example, if the user asks where is the nearest petrol pump the model will trigger the google maps API and calculate the distance.",,LanguageTechnology,sccj0j
"Hi everyone, could use some input/thoughts on an NLP workflow I am helping to design based on a pretty unique situation .
Essentially this is a binary classification problem; we have around _NUMBER_ documents and because of the sheer messiness of the text, a team has provided us with the key phrases taken from these documents that denote whether the document should be a 'Yes'. As opposed to labeling the entire document itself, if that makes sense. 
Based on this, I was thinking that instead of using the entire set of documents as a corpus , that we would take the collection of key phrases as a corpus instead. Then vectorize it using the standard TF-IDF approach, then use that as the input to the classifier.
Couple questions on this:
_NUMBER_) Firstly, does the construction of the corpus in this manner even make sense, since we're not using the entirety of the document?
_NUMBER_) Is there a way to incorporate BERT embeddings into this? My supervisor is very keen on the cutting edge stuff and wants to incorporate BERT, but best I can tell there's not really a way to do this based on the workflows I have seen. The standard TF-IDF approaches seem different from the BERT workflow, which typically involves fine tuning on my corpus and then making predictions from there. 
Thanks!",,LanguageTechnology,sb6xiq
"I have a task where I need to come up with a system that takes sentences from dialogues and makes them more simple and clear. Sentences during dialogues are often filled with filler words such as uhm, uh and repetitions and my goal is to extract the information of the sentence and present it in a clear form.
E.g:
Original: ""And then I drew it on the board uhm white board because that's easier to understand for the student uh i mean students""
Simplified: ""I drew it on the whiteboard because it's easier to understand for the students""
I just took an introductory NLP course in my undergraduate degree so I'm still relatively new to the subject. Any learning resources and advice on how to do this would be greatly appreciated.",,LanguageTechnology,s2t02e
"I am doing a Master’s conversion course in Computer Science. Previously I did Linguistics for my undergrad. Due to my background, I am interested in conducting my thesis on Natural Language Processing. However, as I’ve never conducted a thesis, I don’t know where to start.
What are some good research topics I could read through to get a good idea? Any suggestions on where to start for a beginner in this field would be very much appreciated.",,LanguageTechnology,s0uwco
"Hello everyone!
After lots of research and failure, I finally was able to use BERT for classifying text in my dataset.
However, I feel like a dog that finally caught the car he has been chasing, because I am not sure what to do next.
I had a series of questions that I want to pursue but was hoping for a professional opinion.
First, I want to be able to look at some metrics for seeing how well my model performed. What are good metrics for a multiclass classification task? I know for a fact my classes are imbalanced, so what would be the best way to move forward with this?
In short, what do you ask yourselves once the model is done training and what do you do to evaluate it? How can I improve?
I am a nuclear engineer by trade and NLP/DL is still a very new concept and I was hoping to get insight from the masters out there.
Thanks in advance and happy new year!",,LanguageTechnology,rt6pvi
"I need to identify the \~ _NUMBER_ most common words in the English language as of some reliable / well-established corpus. I intended to use this repo _URL_ _URL_
but I realized that it is somewhat outdated. Any ideas where I could find a comprehensive, up-to-date list of the most common words in English? Thanks :)",,LanguageTechnology,sh7h2x
"Hey everyone,
Kaldi is a really powerful toolkit for ASR and related NLP tasks, but I've found that the learning curve is a bit steep.
**Here's** _URL_ **a tutorial I made that takes you through installation and transcription using pre-trained models**, but the cool part is that you can decide how advanced you want it to be!
Included are Python scripts to automate the entire process, so you can generate transcriptions in just a few lines of code, but I also dive into the code itself to explain what's going on under the hood!
I'd love to hear any thoughts and feedback, or future topics you want to see covered!",,LanguageTechnology,sbnic5
"I'm using symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli](_URL_ from HuggingFace. After multiple tries with different batch sizes, epochs, learning rates and even different unsupervised learning models methods such as [this _URL_ I couldn't get my sentence transformer to perform better than raw model straight from HuggingFace. I'm not sure what I'm doing wrong. I'm sure there are no bugs in my code since I followed the sentence transformer model documentation almost verbatim.
background on my task: my datasets consists of a list of sentences and a person will enter a query of say _NUMBER_-_NUMBER_ sentences and I'm supposed to find the ""correct"" matches for the query.
Currently, the base model isn't amazing but it's also not too bad. Hence, I expected better performance once I fine-tune it. However, upon fine-tuning, cosine similarity scores all drop and the fine-tuned model has never made a better prediction than the original model with no fine tuning.
I'd like to know why that might be the case and if that's a normal thing that usually happen with nlp models. My dataset is very small so my guess is my training parameters were bad? or is my training data so insignificant that my fine-tuning simply doesn't matter?",,LanguageTechnology,s6y0ah
"Hi all, I put together some material on fine-tuning reader models for open-domain question-answering _URL_ . ODQA is an increasingly popular approach to building more human/natural language information retrieval tools. Allowing users to store massive amounts of text data, and then search using natural language questions, it is one of the technologies that powers Google search. Reader models are the final step in an ODQA pipeline, allowing us to extract very specific answers to questions.
Let me know what you think, I hope it's useful, thanks!",,LanguageTechnology,s726uh
"In recent years, neural machine translation  has attracted a lot of attention and has had a lot of success. While traditional NMT is capable of translating a single language pair, training a separate model for each language pair is time-consuming, especially given the world’s thousands of languages. As a result, multilingual NMT is designed to handle many language pairs in a single model, lowering the cost of offline training and online deployment significantly. Furthermore, parameter sharing in multilingual neural machine translation promotes positive knowledge transfer between languages and is advantageous for low-resource translation.
Despite the advantages of cooperative training with a completely shared model, the MNMT approach has a model capacity problem. The shared parameters are more likely to preserve broad knowledge while ignoring language-specific knowledge. To improve the model capacity, researchers use heuristic design to create extra language-specific components and build a Multilingual neural machine translation  model with a mix of shared and language-specific characteristics, such as the language-specific attention, lightweight language adaptor, or language-specific routing layer. Continue Reading _URL_
Paper: _URL_
Github: _URL_",,LanguageTechnology,rvkz91
"Hi everyone!
I have a question for you. For context, we aggregate on a platform the various AI APIs on the market  and including NLP APIs . The idea is that a developer doesn't have to create accounts with different providers and can have them all on one API to test, compare and change whenever he wants.
However, many customers ask us how to mix the ""statistical"" approach behind these APIs with expert systems and how to achieve hybridization.
Do you have any idea how to do this?
Thanks,",,LanguageTechnology,rv4mf8
"I came across an interesting problem, Let's say given two sentences, ""Meet Harry and David and take them to London and Athens. These two cities are worth exploring. Here, the second sentence mentions that the two entities are cities. Is there any method to assign these two entities to category city? I am more concerned about different approaches we can use, may be rule based methods to deep learning based methods.",,LanguageTechnology,se48zj
"Hello, people. I still have some questions after reading the paper about Big Bird model  and will be happy if some Big Bird specialists will help me to understand this model better.
_NUMBER_. Is distribution of random attention (Figure _NUMBER_ ) fixed from advance for all inputs, or it somehow can be different for different inputs even on the same head?
_NUMBER_. In BIGBIRD-ETC, do they add some additional global tokens, aside of \?
_NUMBER_. In BIGBIRD-ITC, how is the subset of tokens for global attention chosen?
_NUMBER_. Why is infinite precision necessary for sparse transformer to be Turing complete?
Thank you.",,LanguageTechnology,s8lxk7
"The different categories include:
_NUMBER_. Posts that were deleted by the user themselves.
_NUMBER_. Posts that were banned by the Community moderators.
_NUMBER_. Posts that were banned by the Platform moderators.
_NUMBER_. Pages or communities containing posts that were banned by the Platform moderators.
I'm fairly uncertain about whether all the data that was pulled contains reasons for the ban they faced. In the case of deleted posts, there's no such label available.
Any idea how to go about this? Any link to cited paperwork that has faced and dealt with similar problems would be great. Links or mentions of authors who might have faced this issue also help as I can try reaching out to them. I'm having some trouble finding sources.
Even similar datasets links would be great as I can do a comparison study on this.
Thanks. :D",,LanguageTechnology,s4cjy3
"Newbie to this field, but nonetheless BERT was trained on _NUMBER_ billion+ words, when I do a masked learning task it is fairly successful on my healthcare dataset without fine tuning. However, when I fine tune the dataset, maybe adding only additional _NUMBER_ million words , suddenly the same task is significantly more accurate.
I understand that fine tuning is training the model on my specific task, so of course results will improve, but it is almost as if the model weights the fine-tuned additional words over itself. 
Why can such a small number of additional words improve the model so drastically?",,LanguageTechnology,rzuyqo
"Usually you move from rule-based segmentation to just machine learning when it gets too complicated.
But if we consider all rule based methods at our disposal - not just regular expressions but also having a corpus of words and symbols loaded as identifiers, plus various statistical methods that were used before machine learning like N-grams and so on -
I think it’s conceivable that if we give an AI certain parameters to play with from the gamut of rule-based segmentation methods, it could think of extraordinarily complex yet relatively effective explicit segmentation scripts.
Sort of like how Ramanujan could think of extremely complicated formulae in number theory that produced perfect results, and DeepMind recently tried to emulate a complex mind like that to discover new theorems in mathematics.
Does anyone know of any projects like this?
Thank you",,LanguageTechnology,rzuiy5
"Hi there, I'm trying to tackle quite a difficult problem with the help of sentence-transformer-models.
Ive got a bunch of JSON  files from different domains, which contain basically entities as JSON schemas consisting of data fields and descriptions. The entities can be ordered in kind of a hierarchical structure, which is not really strict though and may differ from file to file.
I assume that there exist common patterns between those files, precisely how the entities can be ordered in a semantically ""meaningful"" way . I would like to either
**a) Cluster the schemas to identify similarities between those entities**
What I tried: clustering the descriptions with KMEANS and SentenceTransformers. Problems here:
\- If I use only the descriptions they get clustered mostly by domain
\- If I try to cluster the ""raw"" JSON, most models don't find any similarity 
=> My idea here would be to fine-tune a model which encodes always two JSON parts as sentence input and I use the description similarity to generate either a classification score or even NLI scores, to train the model on this data, would this be a valid approach or what could be better ideas?
**b) More of a crazy but interesting idea: If I assume that the ""structure"" can be modeled as a ""sentence"" which consists of ""words""  than probably some sort of model could learn those ""sentences"".**
=> How to create ""words"" from sentences? I thought about creating sentence embeddings for all entities, and then building ""entity-sentences"" from the CLS-tokens? How to build a classifier for such ""sentences""? Are there any good approaches or is there any previous work done?
=> Does it make sense to create the model from scratch or would it be helpful to fine-tune an existing model with this approach?
=> Would it make sense to look at a completely different sort of ML technology?",,LanguageTechnology,rs2c2c
"Hi all. A little background: my mother is a Chinese immigrant who is always lacking self-esteem in her ability to speak ""correct"" English. Whenever she sends a text over to someone who is a native English speaker, she always bugs me to correct her sentences so it sounds more ""natural."" Her English is honestly fine at a conversational level, but could definitely use some editing.
I am wondering if there are NLP tools out there that can help my mom with this? Like if someone types a sentence like ""Hi, I almost done"" we can change it to something like ""Hi, I *am* almost done""?
Thanks in advance.",,LanguageTechnology,ruwav3
"Hello! I'm looking to annotate alignment matrices for neural machine translation. Does anyone know of a tool for doing this quickly? I mainly need a GUI interface where I can click and annotate. 
Couldn't find anything relevant from my initial Google searches.",,LanguageTechnology,rwsjix
"The demand for intelligent technologies that can interpret brief text has increased. As increasingly sophisticated solutions are produced, there is a greater need to improve and facilitate the creation of these complex situations. These scenarios are intelligent customer assistance bots to independent computers that interpret human input. The Language Cognitive Service has opted to employ a multilingual transformer-based paradigm to deal with such problems. When using this model, customers will notice a considerable increase in performance over the old Language Understanding Service .
Microsoft has released the next generation Conversational Language Understanding client library, allowing developers to use the Azure Cloud Conversational Language Understanding service to train models and use them in applications to provide related language services. Developers can use .NET or Python, and these libraries are currently under beta development.
Quick Read: _URL_ _URL_ 
* CLU documentation _URL_
* .NET reference documentation _URL_
* Python reference documentation _URL_
Microsoft Blog: _URL_",,LanguageTechnology,rpjdav
"Suppose you are writing a paper with some new transformer-variant that does well on classification tasks. You want to have a LSTM baseline. How would you go about choosing that LSTM architecture? What about training hyperparameters? Is there a standard, should it be grounded with respect to another paper, does it not matter as long as one explains what the architecture is?
Thanks!",,LanguageTechnology,s66i5v
"Hi NLP community,
Probably much of a newbie here and need some guidance. I am doing a personal project that aims to predict a person's industry from their short biography.
For example:
"" I am a retired engineer and company manager. I do not have a financial background or offer financial advice. blah blah "" => **Prediction:** ENGINEERING
and
"" Damon makes his living as a gap trader, an earnings trader, and an interday trader. In his free time, he writes for ABC, where he focuses on seasonal investing, market timing, and earnings analyses. "" => **Prediction:** FINANCE
I wanted to ask what approach should i do to make such predictions ? And what kind of public dataset would be useful to train a ML model for such task ?
Thank you so much !",,LanguageTechnology,s3iw6i
"How hard are automatic grammar checkers?
Over and over again I find that it'd be much smarter to have a program suggest whether your grammar is sound instead of trusting a person to keep track of all nuances over the course of a large number of sentences.
But I wonder how hard is it to write an automatic grammar checker.
Is it easier for languages like Lojban?",,LanguageTechnology,rywwfl
"txtai _NUMBER_ has been released with a number of new features.
* **Content Storage** \- Content can now be stored alongside embeddings vectors. No longer required to have external data storage.
* **Query with SQL** \- txtai supports both natural language queries and SQL queries 
`embeddings.search` 
`SELECT id, text, score FROM txtai WHERE similar AND score >= _NUMBER_`
* **Object Storage** \- Store binary objects alongside embeddings vectors
* **Reindex** \- Indexes can be rebuilt using stored content, no need to resend data to txtai
* **Index Compression** \- Indexes can be compressed using GZ/XZ/BZ2/ZIP
* **External Vectors** \- Use external vector models from an API or an external library. Centralize building vectors on GPU servers leaving index servers to be powered by more modest hardware.
More information can be found in following links.
* GitHub Project _URL_
* _NUMBER_ Release Notes _URL_
* What's new in txtai _NUMBER_ _URL_
* Documentation _URL_
* Examples _URL_",,LanguageTechnology,s25i2x
"Hello all, 
I am trying to pursue my MS in Computer Science and for funding I have been talking to Professor. He is asking me to brush up my knowledge on Deep Learning and NLP. I am scaring to my death. I have _NUMBER_ years of working experience but working in company is different from research. In company we just use library with surface understanding and working mechanism to get things done.
So, what might be the expectation of any Professor who is hiring any research assistant. What can be the possible questions any Professors ask in Deep Learning and NLP ?
Thanks in advance. Your help and support means a lot to me.",,LanguageTechnology,rxwrax
"Hi, I am looking for leads on how to tackle a NLP problem. 
In short: I have _NUMBER_ fixed  questions where I extracted in sum _NUMBER_ answers to from _NUMBER_ longer text documents .
I want to build a sort of question answering model that gets another text document  and suggests the question answering paragraphs of the document.
During Research the following questions arised: 
_NUMBER_. Looking through question answering models I am not sure if my text input might be too big?
_NUMBER_. Also is the problem too ""supervised"" for classical question answering approaches? 
_NUMBER_. Is there a way to transfer learn the supervised question answers from the document into existing models, any experiences on similar approaches?
Thank you all, appreciate your feedback.",,LanguageTechnology,saqjvi
"Hi All,
I am asking a pretty practical questions, so apologies if it seems overly simple. I want to unify a database of company names such that I get a list of all the parent companies. So if I have
Kroger Foods, Walmart Foods, Walmart Construction, Home Depot Construction, KMart Foods, KMart Toys, I would end up with: Kroger, Walmart, Home Depot, KMart.
I would also like to not be insensitive to word order. Just doing a BOW and filtering out common words gets me a big step there but not quite since there are always common words I haven't included. I am thinking a normalized vector with some sort of filtering out of words that have low normalized values  might be another way to do it but dont have good train/test data. My last thought was to use one of these fuzzy logic guides. Does anyone have an alternate idea or a good fuzzy logic tutorial that they recommend?",,LanguageTechnology,sd8hq7
"A lot of cloud computing services have a lot of vendors in the field but is there any other API for machine translation like Google Translate? DeepL or any others?
Thanks very much",,LanguageTechnology,s2gg6u
"Are there any open source Chinese language thesauruses? Akin to CEDICT, but with synonyms?
I have an application that could really make use of something like that, and without one existing, we'll essentially have to do it by hand, which is fairly laborious.",,LanguageTechnology,rvc54a
"Spacy is an industry standard, and I have been using it ever since I've been in the field. One thing I have always wanted is for spacy to have fit and predict methods, much like sklearn. I understand spacy has its own forms, like the evaluate method. Of course, it probably won't be hard to build a fit predict method based wrapper around spacy, but **I am wondering if anyone has ever come across any such wrapper?** 
Benefit of such a wrapper would be that when building retraining pipelines with models from various libraries, most of which use fit and predict, being able to call fit and predict on a spacy model would simplify things.",,LanguageTechnology,se6nzq
"Is there an equivalent concept in NLP to what high-level computer languages  do to manage user error?
That is:
* natural languages may see users doing errors 
In computer languages, however:
* low-level languages  see users doing errors 
* therefore people design higher level languages  that have features that correct these errors
Is there an equivalent concept for natural languages in NLP?",,LanguageTechnology,ryyhke
"I’m using KeyBERT to extract _NUMBER_ keywords from a file.
It was pretty slow when I did it for only _NUMBER_ keywords.
For _NUMBER_ it ran for almost _NUMBER_ minutes before I terminated it, I believe it was processing the entire time but it’s just a massive computation.
Can anyone advise me on speeding this up? I’m using a Digital Ocean Droplet.
What specs do I need to do something like this in hopefully a few seconds? Are we talking _NUMBER_-core CPU or a certain GPU or something?
Or is there any advice on how I can be certain it’s still running, even after like _NUMBER_ minutes?
How long would you expect an execution like this to take and why? What is it about BERT that is so computation-intensive?
Thank you",,LanguageTechnology,ruzq2s
"I have a postgres database that I want to use to store raw documents. These documents may contain lots of special characters.
I'm trying to insert the documents into a postgres db and I keep getting syntax errors. Not sure what the best approach to this is. 
Here is the code I'm using with psycopg2
sql\_statement = """""" 
PREPARE fooplan AS 
INSERT INTO ocr  VALUES ; 
EXECUTE fooplan;"""""".format 
cur.execute",,LanguageTechnology,rv4c4r
"I'm currently in the process of learning NLP. I am using catalyst on c#. 
I was able to run the sample programs and it was able to determine if the word is an noun, adjective, etc. But I can't find any sample for what I need.
Here is a summary of what I would like to achieve. 
I would like to extract certain information on a sentence. Lets say i have the following texts:
""Sally ate an orange this morning. ""
Or 
""Sally is hiding behind the cabinet and she is eating an orange. "" 
How do i use the nlp to extract what sally ate?",,LanguageTechnology,saas64
"I’ve been trying to do some basic keyword extraction and finding it harder than expected.
KeyBERT seems good but it requires a powerful GPU to be usably fast. That’s possible with AWS, but there’s a bit more set up.
I just tried PyTextRank, and I was surprised at the quality of the output - I wouldn’t say it was perfect either. Maybe I should set a threshold, like choose the top _NUMBER_ ranked keywords? It’s fine if we exclude potential good keywords just to have a smaller list of good ones.
Here’s a good article about _NUMBER_ different methods, which is helpful -
_URL_
In theory, Spacy and BERT seem like the best options but they’re both a little complex. 
I think KW extraction really only needs a few layers or as Spacy would call them pipelines.
_NUMBER_. accurate tokenization of words and punctuation symbols
_NUMBER_. accurate recognition of multi-word expressions  - think of it as “chunking”
_NUMBER_. Strong assessment of keyword “candidacy” for each MWE 
Of course, a good algorithm can often skip steps. Like BERT is so smart it doesn’t need anything but the input text.
Does anyone know of a simplest way to run a fast, effective keyword extraction?
I’m talking _NUMBER_ keywords in one second on a fast CPU.
Thanks very much",,LanguageTechnology,s7qml8
This _URL_ position is currently open and I wanted to share with you!,,LanguageTechnology,s30ccv
