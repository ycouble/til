<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>La Famille BERT et les Transformers | TIL</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="La Famille BERT et les Transformers" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Qu’est-ce que BERT ? Quel lien avec les transformers ? Pourquoi les utilise-t-on ? Comment ?" />
<meta property="og:description" content="Qu’est-ce que BERT ? Quel lien avec les transformers ? Pourquoi les utilise-t-on ? Comment ?" />
<link rel="canonical" href="https://ycouble.github.io/til/fr/ml/nlp/2021/11/23/bert.html" />
<meta property="og:url" content="https://ycouble.github.io/til/fr/ml/nlp/2021/11/23/bert.html" />
<meta property="og:site_name" content="TIL" />
<meta property="og:image" content="https://ycouble.github.io/til/images/bert_cover.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-11-23T00:00:00-06:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://ycouble.github.io/til/images/bert_cover.png" />
<meta property="twitter:title" content="La Famille BERT et les Transformers" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2021-11-23T00:00:00-06:00","datePublished":"2021-11-23T00:00:00-06:00","description":"Qu’est-ce que BERT ? Quel lien avec les transformers ? Pourquoi les utilise-t-on ? Comment ?","headline":"La Famille BERT et les Transformers","image":"https://ycouble.github.io/til/images/bert_cover.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://ycouble.github.io/til/fr/ml/nlp/2021/11/23/bert.html"},"url":"https://ycouble.github.io/til/fr/ml/nlp/2021/11/23/bert.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/til/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://ycouble.github.io/til/feed.xml" title="TIL" /><link rel="shortcut icon" type="image/x-icon" href="/til/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" /><script src="https://hypothes.is/embed.js" async></script>

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/til/">TIL</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/til/about/">Yoann Couble</a><a class="page-link" href="/til/search/">Search</a><a class="page-link" href="/til/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">La Famille BERT et les Transformers</h1><p class="page-description">Qu'est-ce que BERT ? Quel lien avec les transformers ? Pourquoi les utilise-t-on ? Comment ?</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-11-23T00:00:00-06:00" itemprop="datePublished">
        Nov 23, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      9 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/til/categories/#fr">fr</a>
        &nbsp;
      
        <a class="category-tags-link" href="/til/categories/#ml">ml</a>
        &nbsp;
      
        <a class="category-tags-link" href="/til/categories/#nlp">nlp</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/ycouble/til/tree/master/_notebooks/2021-11-23-bert.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/til/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/ycouble/til/master?filepath=_notebooks%2F2021-11-23-bert.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/til/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/ycouble/til/blob/master/_notebooks/2021-11-23-bert.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/til/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-11-23-bert.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Une-petite-r&#233;volution">Une petite r&#233;volution<a class="anchor-link" href="#Une-petite-r&#233;volution"> </a></h2><p>Ces dernières années, la recherche en Intelligence Artificielle a été complètement transformée par une avancée technologique: les transformers.
Avant de comprendre comment et pourquoi cette transformation s'est opérée, prenons un instant pour examiner comment la recherche en IA évalue une avancée technique.</p>
<p>La recherche en IA se concentre depuis plusieurs années sur les tâches dites difficiles de la compréhension du monde: l'extraction d'informations à partir de textes (discipline du traitement automatique du langage ou Natural Language Processing, NLP en anglais) ou d'image (Vision par ordinateur ou Computer Vision, CV) et plus récemment également sur l'audio.
Chacune de ces discipline a déterminé un certain nombre de tâches d'extraction d'information que l'on cherche à faire effectuer à un programme, comme par exemple :</p>
<ul>
<li><strong>La détection d'objets</strong> (localiser une personne dans une image), <strong>la segmentation d'image</strong> (identifier la portion de l'image représentant une route), <strong>la classification d'images</strong> (dire si il s'agit d'une photo de lave-linge ou d'un frigo) en vision par ordinateur</li>
<li><strong>La classification de texte</strong> (dire si un texte est positif ou négatif), <strong>l'extraction d'entités nommées</strong> d'un texte (identifier les noms de personnes, d'organisation, de pays etc.), <strong>le résumé de texte</strong> ou <strong>la réponse aux questions</strong> sur un texte dans le domaine du traitement du langage naturel.
Pour chacune de ces tâches ont été déterminé des jeux de données de référence et des méthodologie d'évaluation de l'efficacité d'un algorithme à réaliser la tâche en question.</li>
</ul>
<p>Jusque là, les meilleures performances étaient atteintes par des modèles spécialisés pour chaque tâche précise, et l'entraînement de chaque modèle était un défi en soi.
C'est dans ce contexte que les Transformers ont apporté une petite révolution: en 2018 dans l'article présentant BERT (<em>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</em>), Google présente une méthodologie de pré-entraînement et de spécialisation qui, appliquée à une architecture à base de Transformers, surpasse l'état de l'art sur chacune des tâches classique de traitement du langage naturel, et ce d'une marge conséquente.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Pourquoi-les-Transformers-et-BERT-fonctionnent-si-bien-?">Pourquoi les Transformers et BERT fonctionnent si bien ?<a class="anchor-link" href="#Pourquoi-les-Transformers-et-BERT-fonctionnent-si-bien-?"> </a></h2><p>La contribution du papier de BERT a été dans un premier temps de combiner avec succès plusieurs techniques déjà connues:</p>
<ol>
<li>Les Transformers, une architecture de Deep Learning qui avait déjà fait ses preuves. Cette architecture est basée sur le mécanisme d'attention, qui permet de prendre en compte tout le contexte d'un mot en accordant à priori autant d'importance à des mots éloignés qu'à des mots proches, contrairement aux modèles récurrents qui faisaient l'état de l'art avant l'arrivée des Transformers, en 2016 (cf. <em>Attention is All You Need</em>, Google 2016 dans les références.)</li>
</ol>
<p><img src="/til/images/copied_from_nb/bert/attention.png" alt="attention" title="Mécanisme d&#39;attention" /></p>
<ol>
<li>Le Transfer Learning, qui consiste à entraîner un modèle sur une tâche très générique (souvent non supervisée) sur un dataset gigantesque (par exemple le corpus de Wikipedia), puis a spécialiser le modèle en le modifiant sur une seconde tâche différente de manière supervisée et sur un dataset plus réduit.</li>
</ol>
<p>De plus, leur contribution est aussi et surtout dans la méthodologie de pré-entrainement du modèle de langage (le premier modèle), qui se fait en combinant deux tâches génériques non supervisées:</p>
<ol>
<li>La complétion de phrases à trou, c'est à dire trouver le meilleur mot pour compléter une phrase du type: "je suis allé au <em>__</em> ce matin pour acheter mes légumes".</li>
<li>La prédiction de la prochaine pharse dans un texte.</li>
</ol>
<p>Ces deux tâches peuvent s'effectuer grâce à un entraînement auto-supervisé sur des datasets gigantesques comme l'intégralité de Wikipedia, des corpus de sites crawlé ou à partir des réseaux sociaux publics (Twitter, Reddit ...). En passant en revue tous ces contenus, l'algorithme devient de plus en plus performant à déterminer les mots qui vont généralement ensemble en fonction d'un contexte assez large (de l'ordre de la phrase ou du paragraphe).</p>
<h3 id="Qu'est-ce-que-&#231;a-change-?">Qu'est-ce que &#231;a change ?<a class="anchor-link" href="#Qu'est-ce-que-&#231;a-change-?"> </a></h3><p>Le gros changement apporté par BERT est cette idée de modèle de langage: la plupart des tâches de NLP ont en commun d'essayer de comprendre le contexte d'un mot et de trouver les relations entre les mots d'une phrase, et c'est ce que tente d'apporter le modèle de langage. Concrètement, cela signifique que pour n'importe quelle tâche de NLP, il suffit maintenant de réutiliser le modèle de langage pour avoir cette compréhension générale, puis de spécialiser la "tête" de l'architecture pour une tâche donnée.</p>
<p>La seconde conséquence est d'ordre pratique: avec un modèle comme BERT, l'entraînement d'un modèle sur une tâche spécifique se résume généralement à n'entraîner que la tête de l'architecture, donc à un coût dérisoire par rapport à un modèle complet comme précédemment.</p>
<p>Maintenant, il est devenu extrêment facile d'utiliser et d'entrâiner un modèle très performant pour tout une diversité de tâches de NLP. Cela a été grandement facilité par la publication du code et des modèles pré-entraîné, et par l'arrivée d'Hugging Face, une entreprise française de l'open source qui propose une interface standard et un hub de modèles pour les transformers.</p>
<p><img src="/til/images/copied_from_nb/bert/bert_training.png" alt="bert" /></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Utilisation-des-transformers">Utilisation des transformers<a class="anchor-link" href="#Utilisation-des-transformers"> </a></h2><p>BERT et les autres transformers sont conçus pour être réutilisable en grande partie pour différentes tâche, il ne reste donc qu'à réadapter les entrées et sorties à la tâche visée.
Nous allons regarder dans cet article le cas de la spécialisation et de l'utilisation d'un modèle de transformer pour une tâche d'extraction d'entités nommées (NER) à partir de différentes bibliothèques open-source de NLP.</p>
<h3 id="La-t&#226;che-d'extraction-d'entit&#233;s-nomm&#233;es:-donn&#233;es-et-annotation">La t&#226;che d'extraction d'entit&#233;s nomm&#233;es: donn&#233;es et annotation<a class="anchor-link" href="#La-t&#226;che-d'extraction-d'entit&#233;s-nomm&#233;es:-donn&#233;es-et-annotation"> </a></h3><p>Le but de l'extraction d'entités nommées est de reconnaître dans un texte les mots ou ensembles de mots qui correspondent à des villes, pays, dates, langages de programmation, personnes ou tout autre catégorie pour lesquels on ne peut généralement pas lister l'ensemble des mots qui représentent ces entités.</p>
<p>Par exemple, dans la phrase "My name is Wolfgang and I live in Berlin", la tâche de NER devrait permettre d'extraction des mots Wolfgand, classé en tant que personne et Berlin comme ville.</p>
<p>Il s'agit en fin de compte d'une tâche de classification de token (un token est un mot ou une partie de mot), supervisée, et il y a plusieurs méthodes d'annotation des données. Nous allons présenter uniquement la méthode IOB, accronyme pour Inside, Outside et Begin, où chaque mot reçoit une annotation O ou I/B-[type d'entité]. Pour notre exemple cela donnerait ça:</p>

<pre><code>My name is Wolfgang and I live in Berlin
O    O   O   B-PER   0  0   0   0 B-CITY</code></pre>
<p>Ou sur un second exemple:</p>

<pre><code>I'm Yoann Couble and I write about Natural Language Processing in Python
O O B-PER I-PER   O  0   O     O   B-TECH   I-TECH    I-TECH   O  B-LANG</code></pre>
<p>L'objectif de l'algorithme sera donc de déterminer pour chaque token l'annotation à positionner.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="&#129303;-Hugging-Face-Transformers">&#129303; Hugging Face Transformers<a class="anchor-link" href="#&#129303;-Hugging-Face-Transformers"> </a></h3><p>Hugging Face a construit toute une API pour faciliter l'utilisation des transformers. La librairie propose des pipelines pré-définies et une bibliothèque de <a href="https://huggingface.co/models">modèles</a> explorables par tâches et langue et de <a href="https://huggingface.co/datasets">datasets</a> prêtes à l'emplois.
L'avantage de Hugging Face est la grande diversité de modèles expérimentaux ou éprouvés qui sont disponibles sur le hub et la facilité d'utilisation et d'expérimentation qu'il permet.</p>
<p>La configuration d'un modèle se fait très facilement (sans code) à l'aide d'un fichier de configuration: <a href="https://huggingface.co/dslim/bert-base-NER/blob/main/config.json">https://huggingface.co/dslim/bert-base-NER/blob/main/config.json</a></p>
<div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="nt">&quot;_num_labels&quot;</span><span class="p">:</span> <span class="mi">9</span><span class="p">,</span>
  <span class="nt">&quot;architectures&quot;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&quot;BertForTokenClassification&quot;</span>
  <span class="p">],</span>
  <span class="nt">&quot;attention_probs_dropout_prob&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
  <span class="nt">&quot;hidden_act&quot;</span><span class="p">:</span> <span class="s2">&quot;gelu&quot;</span><span class="p">,</span>
  <span class="nt">&quot;hidden_dropout_prob&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
  <span class="nt">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="mi">768</span><span class="p">,</span>
  <span class="nt">&quot;initializer_range&quot;</span><span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>
  <span class="nt">&quot;intermediate_size&quot;</span><span class="p">:</span> <span class="mi">3072</span><span class="p">,</span>
  <span class="nt">&quot;layer_norm_eps&quot;</span><span class="p">:</span> <span class="mf">1e-12</span><span class="p">,</span>
  <span class="nt">&quot;max_position_embeddings&quot;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
  <span class="nt">&quot;model_type&quot;</span><span class="p">:</span> <span class="s2">&quot;bert&quot;</span><span class="p">,</span>
  <span class="nt">&quot;num_attention_heads&quot;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>
  <span class="nt">&quot;num_hidden_layers&quot;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>
  <span class="nt">&quot;output_past&quot;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
  <span class="nt">&quot;pad_token_id&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
  <span class="nt">&quot;type_vocab_size&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
  <span class="nt">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">28996</span><span class="p">,</span>
  <span class="err">#</span> <span class="err">Labels</span>
  <span class="nt">&quot;id2label&quot;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&quot;0&quot;</span><span class="p">:</span> <span class="s2">&quot;O&quot;</span><span class="p">,</span>
    <span class="nt">&quot;1&quot;</span><span class="p">:</span> <span class="s2">&quot;B-MISC&quot;</span><span class="p">,</span>
    <span class="nt">&quot;2&quot;</span><span class="p">:</span> <span class="s2">&quot;I-MISC&quot;</span><span class="p">,</span>
    <span class="nt">&quot;3&quot;</span><span class="p">:</span> <span class="s2">&quot;B-PER&quot;</span><span class="p">,</span>
    <span class="nt">&quot;4&quot;</span><span class="p">:</span> <span class="s2">&quot;I-PER&quot;</span><span class="p">,</span>
    <span class="nt">&quot;5&quot;</span><span class="p">:</span> <span class="s2">&quot;B-ORG&quot;</span><span class="p">,</span>
    <span class="nt">&quot;6&quot;</span><span class="p">:</span> <span class="s2">&quot;I-ORG&quot;</span><span class="p">,</span>
    <span class="nt">&quot;7&quot;</span><span class="p">:</span> <span class="s2">&quot;B-LOC&quot;</span><span class="p">,</span>
    <span class="nt">&quot;8&quot;</span><span class="p">:</span> <span class="s2">&quot;I-LOC&quot;</span>
  <span class="p">},</span>
  <span class="nt">&quot;label2id&quot;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&quot;B-LOC&quot;</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span>
    <span class="nt">&quot;B-MISC&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="nt">&quot;B-ORG&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="nt">&quot;B-PER&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="nt">&quot;I-LOC&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
    <span class="nt">&quot;I-MISC&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="nt">&quot;I-ORG&quot;</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
    <span class="nt">&quot;I-PER&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="nt">&quot;O&quot;</span><span class="p">:</span> <span class="mi">0</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>
<p>Ici l'architecture choisie est <code>BertForTokenClassification</code> ce qui correspond à notre tâche de NER. Et on retrouve aussi les différents hyper-paramètres des transformers (nombre de têtes d'attention, paramètres d'attention, ...). Pour pouvoir réutiliser les poids d'entraînement d'un modèle pré-entraîné, il faut faire attention à ne changer que ce qui ne casse pas la compatibilité avec le modèle pré-entraîné.</p>
<p>Dans ce notebook nous n'allons pas entraîner de nouveau modèle, mais utiliser directement <a href="https://huggingface.co/dslim/bert-base-NER">le modèle qui a été entraîné avec la configuration présentée au dessus</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Pr&#233;-requis">Pr&#233;-requis<a class="anchor-link" href="#Pr&#233;-requis"> </a></h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">pip</span> install --upgrade -q torch transformers &quot;spacy&gt;=3.2.0&quot;
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForTokenClassification</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;dslim/bert-base-NER&quot;</span>

<span class="c1"># Récupération du modèle et d&#39;un tokenizer adapté (peut prendre du temps car il faut télécharger le modèle qui est assez volumineux)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="c1"># Définition de la pipeline</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;ner&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">example</span> <span class="o">=</span> <span class="s2">&quot;My name is Wolfgang and I live in Berlin&quot;</span>

<span class="n">ner_results</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">example</span><span class="p">)</span>
<span class="n">ner_results</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[{&#39;entity&#39;: &#39;B-PER&#39;,
  &#39;score&#39;: 0.99901396,
  &#39;index&#39;: 4,
  &#39;word&#39;: &#39;Wolfgang&#39;,
  &#39;start&#39;: 11,
  &#39;end&#39;: 19},
 {&#39;entity&#39;: &#39;B-LOC&#39;,
  &#39;score&#39;: 0.999645,
  &#39;index&#39;: 9,
  &#39;word&#39;: &#39;Berlin&#39;,
  &#39;start&#39;: 34,
  &#39;end&#39;: 40}]</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notons que seuls les tokens classifiés avec une entité sont montrés, le reste a reçu une annotation "O".</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Utilisation-des-transformers-avec-SpaCy">Utilisation des transformers avec SpaCy<a class="anchor-link" href="#Utilisation-des-transformers-avec-SpaCy"> </a></h3><p>Spacy est une bibliothèque open-source permettant d'industrialiser des application de traitement du langage naturel.
L'avantage de Spacy par rapport à Hugging Face est qu'il va être possible de mutualiser un même modèle de langage à base de transformers pour plusieurs modèles spécialisés pour différentes tâches sur un même document.</p>
<p>Spacy fourni également des modèles pré-entraînés et bien intégrés à Spacy. Ce sont les modèles finissant en <code>_trf</code> comme <a href="https://spacy.io/models/fr#fr_dep_news_trf">celui-ci basé sur camembert-base</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Pour pouvoir l'utiliser, il faut télécharger le modèle dans le même environnement virtuel / kernel (dans mon cas, un environnement pyenv)</p>
<div class="highlight"><pre><span></span>pyenv activate transformers_3.8.6
python -m spacy download en_core_web_trf
</pre></div>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">from</span> <span class="nn">spacy</span> <span class="kn">import</span> <span class="n">displacy</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_trf&quot;</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;My name is Wolfgang and I live in Berlin&quot;</span><span class="p">)</span>

<span class="n">displacy</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="s2">&quot;ent&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<span class="tex2jax_ignore"><div class="entities" style="line-height: 2.5; direction: ltr">My name is 
<mark class="entity" style="background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Wolfgang
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">PERSON</span>
</mark>
 and I live in 
<mark class="entity" style="background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Berlin
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">GPE</span>
</mark>
</div></span>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Une-adoption-qui-s'&#233;tend-maintenant-au-del&#224;-du-langage">Une adoption qui s'&#233;tend maintenant au del&#224; du langage<a class="anchor-link" href="#Une-adoption-qui-s'&#233;tend-maintenant-au-del&#224;-du-langage"> </a></h2><p>Nous avons pu voir et comprendre l'apport de BERT et des transformers au paysage du NLP, et l'impact des transformers continue de faire son chemin, avec la vision qui a eu tôt fait de les adopter (modèls VIT, VILT, CLIP etc.) ainsi que l'audio et la vidéo.</p>
<p>Maintenant, la recherche sur les transformers se plonge dans la convergence multi-modale (fusion Vidéo-Audio-Texte), avec notamment le très récent Data2Vec de Facebook AI Reasearch cette année.
Cela dit, le problème difficile de la compréhension du monde est loin d'être résolu, et la recherche continue d'être très active sur le sujet.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Ressources-et-r&#233;d&#233;rences-sur-le-sujet">Ressources et r&#233;d&#233;rences sur le sujet<a class="anchor-link" href="#Ressources-et-r&#233;d&#233;rences-sur-le-sujet"> </a></h2><h3 id="R&#233;f&#233;rences">R&#233;f&#233;rences<a class="anchor-link" href="#R&#233;f&#233;rences"> </a></h3><ul>
<li>Attention is all you need (2016, Transformer paper): <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></li>
<li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018): <a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a></li>
<li>Illustrated transformer: <a href="http://jalammar.github.io/illustrated-transformer/">http://jalammar.github.io/illustrated-transformer/</a></li>
<li>BERT model doc on hugging face: <a href="https://huggingface.co/transformers/model_doc/bert.html#bertmodel">https://huggingface.co/transformers/model_doc/bert.html#bertmodel</a></li>
<li><a href="https://spacy.io/usage/embeddings-transformers">https://spacy.io/usage/embeddings-transformers</a></li>
</ul>
<h3 id="Exemples">Exemples<a class="anchor-link" href="#Exemples"> </a></h3><ul>
<li>Exemple de configuration sur le hub de huggingface: <a href="https://huggingface.co/dslim/bert-base-NER/blob/main/config.json">https://huggingface.co/dslim/bert-base-NER/blob/main/config.json</a></li>
<li>Transformers pour le NER FR <a href="https://huggingface.co/models?language=fr&amp;pipeline_tag=token-classification&amp;sort=downloads&amp;search=ner">https://huggingface.co/models?language=fr&amp;pipeline_tag=token-classification&amp;sort=downloads&amp;search=ner</a></li>
<li>Use huggingface transformers within spacy: <a href="https://reposhub.com/python/deep-learning/explosion-spacy-transformers.html">https://reposhub.com/python/deep-learning/explosion-spacy-transformers.html</a></li>
</ul>
<h3 id="Cours-et-ressources-formatrices">Cours et ressources formatrices<a class="anchor-link" href="#Cours-et-ressources-formatrices"> </a></h3><ul>
<li><a href="https://www.coursera.org/learn/attention-models-in-nlp/home/welcome">https://www.coursera.org/learn/attention-models-in-nlp/home/welcome</a></li>
<li><a href="https://huggingface.co/course/chapter1">https://huggingface.co/course/chapter1</a></li>
<li><a href="https://course.spacy.io/en/chapter4">https://course.spacy.io/en/chapter4</a></li>
<li><a href="https://www.youtube.com/playlist?list=PL75e0qA87dlG-za8eLI6t0_Pbxafk-cxb">https://www.youtube.com/playlist?list=PL75e0qA87dlG-za8eLI6t0_Pbxafk-cxb</a></li>
</ul>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="ycouble/til"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/til/fr/ml/nlp/2021/11/23/bert.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/til/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/til/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/til/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Today I Learnt</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://gitlab.com/ycouble" target="_blank" title="ycouble"><svg class="svg-icon grey"><use xlink:href="/til/assets/minima-social-icons.svg#gitlab"></use></svg></a></li><li><a rel="me" href="https://github.com/ycouble" target="_blank" title="ycouble"><svg class="svg-icon grey"><use xlink:href="/til/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/yoann-couble" target="_blank" title="yoann-couble"><svg class="svg-icon grey"><use xlink:href="/til/assets/minima-social-icons.svg#linkedin"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
