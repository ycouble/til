{
  
    
        "post0": {
            "title": "OSS: Rubrix, bibliothèque de MLOps pour le NLP",
            "content": "Dernière découverte sur le front du MLOps pour le NLP : Rubrix ! . Rubrix est un logiciel Open Source de MLOps pour le NLP (traitement automatique du langage naturel). Le MLOps, Machine Learning Operations, consiste à permettre une intégration continue et une supervision efficace d’une application de Machine Learning. Plus concrètement, il s’agit d’être capable de consulter les prédictions d’un algorithme, de corriger des mauvaises décisions, d’évaluer la qualité du système et d’en surveiller l’évolution. . Il y a énormément de facettes au MLOps, et Rubrix vient apporter sa brique à l’édifice en permettant de facilement rajouter une sonde d’observation dans une application de NLP. Une fois mise en place, un échantillon des prédictions faites en production est loggué vers Rubrix. Puis, depuis une interface web, on peut facilement consulter les prédictions, corriger les annotations, et consulter des statistiques d’efficacité du modèle. . Cerise sur le gateau, Rubrix permet de visualiser des métriques d’explicabilité, afin de comprendre les biais appris par l’algorithme. . Documentation: https://rubrix.readthedocs.io/en/stable/index.html | Github: https://github.com/recognai/rubrix | . Et vous, quels frameworks de MLOps vous utilisez pour des données textuelles ? .",
            "url": "https://ycouble.github.io/til/fr/nlp/ml/python/mlops/2022/02/25/rubrix.html",
            "relUrl": "/fr/nlp/ml/python/mlops/2022/02/25/rubrix.html",
            "date": " • Feb 25, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Explaining spaCy Models with Shap",
            "content": "Explainabili...what ? . One of the most important downsides to Deep Learning is the apparent loss of interpretability that it introduces. As a matter of facts, Deep Learning models are no longer deterministic nor simple enough to be interpreted through their internal states. Models now have trillions of parameters with little to no sense at all. . On of the recent trends to tackle this issue is to use explainability techniques, such as LIME and SHAP which can both be applied to any type of ML model. Both offer a large variety of tools to help understand the behavior of a model globally (what is learnt by the model) or locally (why the model took such or such decision for a given data). One popular and helpful way to explain a prediction from a model is to highlight which features contributed the most to the prediction. LIME and SHAP offer different ways to do so: . LIME is an explainability technique that uses what we call a local surrogate model - i.e. a second model that locally approximates the model to explain - and infers explainability metrics from the second model. LIME is quite popular and very often used, even though the surrogate nature of the technique is often criticized (there is no guarantee that the surrogate model approximates well the actual model). | SHAP is based on Shapeley Values, a game-theoretic concept that tests all possible combinations of features in order to determine the marginal contributions of each one. SHAP has more theoretical guarantees and good properties, hence it is often considered more reliable than LIME, even though it is more time consuming. | . More details can be found on both techniques in this post or in this excellent book from Christopher Molnar. . In this article we&#39;ll show how to use SHAP on textual inputs, and more specifically for the popular open source NLP library spaCy. . Explaining Natural Language Processing models . The discipline of processing texts is called Natural Language Processing. It contains (but is not limited to) using machine learning models to analyse texts. . Unlike tabular data, where each feature can be represented by a number and can therefore be given directly to a model, text data need to be pre-processed before being provided to a ML model. This pre-processing text, called tokenization, basically consists of converting each word1 (token) into a number. For that the tokenizer requires a dictionary which will list all word to number matches. This dictionary is called a vocabulary. . For example if my whole corpus is the following list of documents: [&quot;I am Yoann Couble&quot;, &quot;I work for PALO-IT&quot;, &quot;I work with NLP for companies&quot;], the vocabulary would be : . { &quot;I&quot;: 1, &quot;am&quot;: 2, &quot;Yoann&quot;: 3, &quot;Couble&quot;: 4, &quot;work&quot;: 5, &quot;for&quot;: 6, &quot;PALO-IT&quot;: 7, &quot;with&quot;: 8, &quot;NLP&quot;: 9, &quot;companies&quot;: 10, } . Therefore, tokenizing the last sentence would result in this vector : [1, 5, 8, 9, 6, 10]. . The tokenizer is important to use explainability techniques, since these techniques often resort to twisting the inputs to observe the effects on the outputs. On text, varying the token integer representation would have no meaning at all. So instead, the explainer removes tokens from the input and observes the impact on the output of the model. With SHAP, the permutation explainer does exactly that and in a way that enables to approximate shapeley values of each feature. . As an example, permutation would mean passing &quot;I ... with NLP for ...&quot;, &quot;... work with ... for companies&quot;, etc. to the model and see by how much the result of the model changes. . . Some tokenization algorithms are however even more fine grained than that and split words into sub-words, phonemes, syllables to allow for more robust or versatile language representation.&#8617; . | Explaining the results of a spaCy text classification model . Now let&#39;s see in practice how to use SHAP to get some insights on a model trained with spaCy (see this article on how to do so). In this article we&#39;re going to use text classification as an example, and a custom pipeline specialized for this. You can find the pipeline here (you&#39;ll need to unzip it) and the dataset there. . spaCy wrappers for SHAP . SHAP supports text data, but has little to no support for spaCy models natively, so we&#39;ll need to create some wrappers and assemble precisely the different parts required by SHAP. . The permutation explainer, which is the one preferred by SHAP for text data, requires several parameters: . a prediction function, which takes a list of texts and returns a list of results (in the case of a text classifier, this means the classes and their corresponding probability) | a tokenizer to build a Text masker for SHAP. | . These features are present in spaCy nlp pipelines but not as functions. They are embedded in the pipeline and produce results inside the document object. Let&#39;s write some wrappers around the pipeline to conform to shap expectations. . %pip install -qqq shap &quot;spacy&gt;3.2.0&quot; pandas . Note: you may need to restart the kernel to use updated packages. . import spacy textcat_spacy = spacy.load(&quot;model-best&quot;) tokenizer_spacy = spacy.tokenizer.Tokenizer(textcat_spacy.vocab) classes = list(textcat_spacy.get_pipe(&quot;textcat&quot;).labels) # Define a function to predict def predict(texts): # convert texts to bare strings texts = [str(text) for text in texts] results = [] for doc in textcat_spacy.pipe(texts): # results.append([{&#39;label&#39;: cat, &#39;score&#39;: doc.cats[cat]} for cat in doc.cats]) results.append([doc.cats[cat] for cat in classes]) return results # Create a function to create a transformers-like tokenizer to match shap&#39;s expectations def tok_wrapper(text, return_offsets_mapping=False): doc = tokenizer_spacy(text) out = {&quot;input_ids&quot;: [tok.norm for tok in doc]} if return_offsets_mapping: out[&quot;offset_mapping&quot;] = [(tok.idx, tok.idx + len(tok)) for tok in doc] return out . Define the SHAP Explainer . Now can define the shap explainer. . import shap # Create the Shap Explainer # - predict is the &quot;model&quot; function, adapted to a transformers-like model # - masker is the masker used by shap, which relies on a transformers-like tokenizer # - algorithm is set to permutation, which is the one used for transformers models # - output_names are the classes (although it is not propagated to the permutation explainer currently, which is why plots do not have the labels) # - max_evals is set to a high number to reduce the probability of cases where the explainer fails because there are too many tokens explainer = shap.Explainer( predict, masker=shap.maskers.Text(tok_wrapper), algorithm=&quot;permutation&quot;, output_names=classes, max_evals=1500, ) . The dataset . Let&#39;s import the dataset . import pandas as pd dataset = pd.read_csv(&quot;reddit_data.csv&quot;)[&quot;text&quot;].tolist() dataset[0] . &#39;I’m looking for datasets or api source that quantifies fan base, or preferably, bettors’ sentiment regarding a team’s performance or direction. Does anyone know of an API that tracks this? For now I’m looking specifically for NBA, but am also interested in MLB, NFL, and NCAA f-ball and b-ball.&#39; . Explanations . Local explanations . With the explainer and the data, we&#39;re now able to run the explanations from shap. Before plotting everything, let&#39;s get the shap values for one entry. . shap_values = explainer(dataset[:1]) shap_values.output_names = classes . The shap values contain 3 attributes: . the values themselves (one value per class per word) | the base_value (which can be seen as a prior: what we would get for a empty string) | the data: the words as they are tokenized | . From this structure we can generate plots to help visualize the explanation. . fig_html = shap.plots.text(shap_values, display=False) . . We can see the shap text plots provide us two elements than can be interactively changed to display the shap values for each output label: . The first plot is a force plot, where the contribution of each word in favor (red) or against (blue) the selected label is displayed in a cumulative manner. The point where red and blue meet is the value predicted by the model for this label. | The second plot is the text with each word highlighted depending on its contribution to the final decision. | . That&#39;s it ! We&#39;ve seen in this article how to adapt a spaCy pipeline to be able to use shap for our NLP explanations. .",
            "url": "https://ycouble.github.io/til/en/python/nlp/spacy/ml/mlops/2022/02/24/Explaining_spacy_models_with_shap.html",
            "relUrl": "/en/python/nlp/spacy/ml/mlops/2022/02/24/Explaining_spacy_models_with_shap.html",
            "date": " • Feb 24, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Entraînement et intégration d'un classifieur de textes avec SpaCy",
            "content": "Introduction . Spacy est une bibliothèque de Traitement Automatique du Langage Naturel (TALN ou NLP en anglais) et un framework pour industrialiser des applications de NLP et de Machine Learning. . Les ressources pour entraîner un modèle de machine learning sur du texte (classification, extraction d&#39;entités, etc.) avec SpaCy ne manquent pas, mais très peu vont jusqu&#39;au bout du chemin: l&#39;intégration avec d&#39;autres composants standards et pré-entraînés de SpaCy (ou d&#39;autres fournisseurs), tels qu&#39;un DependencyParser, tagger POS qui ne nécessitent pas de spécialisation ou de fine tuning. . C&#39;est pour combler ce manque que ce notebook existe : vous guider dans le process de configuration d&#39;un composant de classification de textes, de l&#39;entraînement depuis un script python et jusqu&#39;à son intégration dans une pipeline SpaCy pré-entraînée afin qu&#39;elle soit réutilisable depuis le reste de vos applications.. . Pre-requis . Tout d&#39;abord, installons les packages nécessaires pour ce tutoriel: spacy pour le NLP, pandas pour inspecter nos données, et sklearn qui va faciliter la séparation du jeu de données en splits. . %pip install -q &quot;spacy&gt;3.0.0&quot; pandas sklearn . Note: you may need to restart the kernel to use updated packages. . Nous devons également télécharger un modèle pré-entraîné de SpaCy : https://spacy.io/models/en#en_core_web_md. La ligne de commande suivante doit être exécutée depuis le même environnement que votre kernel de notebook. . !python -m spacy download en_core_web_md . Les donn&#233;es &amp; la t&#226;che de classification . Nous travaillerons sur un dataset qui a été extrait à travers l&#39;API de reddit. Le dataset a déjà été préparé et nettoyé pour qu&#39;il puisse être facilement importé et converti en documents SpaCy. Vous pourrez le trouver ici. . Le dataset est composé du corps de texte d&#39;une selection de posts provenant de quelques subreddits liés à la data science. L&#39;objectif de notre tâche de machine learning sera de deviner à partir du corps de texte de quel subreddit le post provient. Même si l&#39;intérêt en soi est assez limité, c&#39;est un bon point de départ pour démarrer et il présente l&#39;avantage d&#39;être déjà annoté. . Jettons un oeil à ce qu&#39;il y a dans le dataset. . import pandas as pd pd.options.display.max_colwidth = None pd.options.display.max_rows = 6 data = pd.read_csv(&quot;spacy_textcat/reddit_data.csv&quot;) data . text tag subreddit id . 0 I’m looking for datasets or api source that quantifies fan base, or preferably, bettors’ sentiment regarding a team’s performance or direction. Does anyone know of an API that tracks this? For now I’m looking specifically for NBA, but am also interested in MLB, NFL, and NCAA f-ball and b-ball. | API | datasets | s0vufk | . 1 I&#39;m making an ESG stock analysis program in Java, and so far the only free ESG API I&#39;ve come across is ESGEnterprise, but I&#39;m having trouble retrieving the data. Has anyone had any success/have any recs for other ESG APIs out there. | API | datasets | ruvj9n | . 2 Hey everyone! I’m one of the creators of Sieve _URL_ and I’m excited to be sharing it! nSieve _URL_ **is an API that helps you turn petabyte-scale video data into a high-quality dataset, automatically.** nIt helps store, process, and semantically search your video data. Just think _NUMBER_ cameras recording footage at _NUMBER_ FPS, _NUMBER_/_NUMBER_. That would be _NUMBER_ million frames generated in a single day. The videos might be searchable by timestamp, but finding moments of interest is like searching for a needle in a haystack. Sieve tags useful attributes like people, motion, lighting, etc on every frame! nWe built this visual demo (link here _URL_ a little while back which we’d love to get feedback on. It’s ~_NUMBER_ hours of security footage that our API processed in &lt;_NUMBER_ mins and has simple querying and export functionality enabled. We see applications in better understanding what data you have, figuring out which data to send to labeling, sampling datasets for training, and building multiple test sets for models by scenario. nTo try it on your videos: _URL_ _URL_ nVisual dashboard walkthrough: Click on our site link! | API | datasets | rup1uj | . ... ... | ... | ... | ... | . 718 I&#39;m currently in the process of learning NLP. I am using catalyst on c#. nI was able to run the sample programs and it was able to determine if the word is an noun, adjective, etc. But I can&#39;t find any sample for what I need. nHere is a summary of what I would like to achieve. nI would like to extract certain information on a sentence. Lets say i have the following texts: n&quot;Sally ate an orange this morning. &quot; nOr n&quot;Sally is hiding behind the cabinet and she is eating an orange. &quot; nHow do i use the nlp to extract what sally ate? | NaN | LanguageTechnology | saas64 | . 719 I’ve been trying to do some basic keyword extraction and finding it harder than expected. nKeyBERT seems good but it requires a powerful GPU to be usably fast. That’s possible with AWS, but there’s a bit more set up. nI just tried PyTextRank, and I was surprised at the quality of the output - I wouldn’t say it was perfect either. Maybe I should set a threshold, like choose the top _NUMBER_ ranked keywords? It’s fine if we exclude potential good keywords just to have a smaller list of good ones. nHere’s a good article about _NUMBER_ different methods, which is helpful - n_URL_ nIn theory, Spacy and BERT seem like the best options but they’re both a little complex. nI think KW extraction really only needs a few layers or as Spacy would call them pipelines. n_NUMBER_. accurate tokenization of words and punctuation symbols n_NUMBER_. accurate recognition of multi-word expressions - think of it as “chunking” n_NUMBER_. Strong assessment of keyword “candidacy” for each MWE nOf course, a good algorithm can often skip steps. Like BERT is so smart it doesn’t need anything but the input text. nDoes anyone know of a simplest way to run a fast, effective keyword extraction? nI’m talking _NUMBER_ keywords in one second on a fast CPU. nThanks very much | NaN | LanguageTechnology | s7qml8 | . 720 This _URL_ position is currently open and I wanted to share with you! | NaN | LanguageTechnology | s30ccv | . 721 rows × 4 columns . cats = data.subreddit.unique().tolist() cats . [&#39;datasets&#39;, &#39;dataengineering&#39;, &#39;LanguageTechnology&#39;] . Le dataset est composé d&#39;un peu plus de 700 posts et de leurs subreddits associé. Créons maintenant les datasets d&#39;entraînement et de validation en y incluant les annotations. . Cr&#233;ation du dataset d&#39;entra&#238;nement . Tout d&#39;abord écrivons une fonction permettant de transformer le dataset dans le format binaire pour spacy, que l&#39;on va stocker temporairement en local. . from typing import Set, List, Tuple from spacy.tokens import DocBin import spacy # Load spaCy pretrained model that we downloaded before nlp = spacy.load(&quot;en_core_web_md&quot;) # Create a function to create a spacy dataset def make_docs(data: List[Tuple[str, str]], target_file: str, cats: Set[str]): docs = DocBin() # Use nlp.pipe to efficiently process a large number of text inputs, # the as_tuple arguments enables giving a list of tuples as input and # reuse it in the loop, here for the labels for doc, label in nlp.pipe(data, as_tuples=True): # Encode the labels (assign 1 the subreddit) for cat in cats: doc.cats[cat] = 1 if cat == label else 0 docs.add(doc) docs.to_disk(target_file) return docs . Séparons jeux d&#39;entraînement et de validation. . from sklearn.model_selection import train_test_split X_train, X_valid, y_train, y_valid = train_test_split(data[&quot;text&quot;].values, data[&quot;subreddit&quot;].values, test_size=0.3) make_docs(list(zip(X_train, y_train)), &quot;train.spacy&quot;, cats=cats) make_docs(list(zip(X_valid, y_valid)), &quot;valid.spacy&quot;, cats=cats) . &lt;spacy.tokens._serialize.DocBin at 0x12b189100&gt; . Creation et configuration du composant de classification de textes . Le workflow recommandé avec SpaCy utilise des fichiers de configuration. Ils permettent de configurer chaque composant de la pipeline, de choisir quels composant entrâiner etc. . Nous utiliserons ce fichier de configuration, qui utilises le classifieur de textes par défaut de SpaCy. La configuration peut être re-générée en suivant ce guide : https://spacy.io/usage/training#quickstart et nous l&#39;avons customisé pour qu&#39;il utilise ce model proposé par SpaCy également : https://spacy.io/api/architectures#TextCatBOW. . Il y a deux parties qu&#39;il est important de noter dans ce fichier : . La définition de la pipeline (sous le header nlp): La pipeline ne contient que le composant textcat (de classification) puisque c&#39;est celui pour lequel nous avons des données annotées et le seul que nous allons entraîner aujourd&#39;hui. Un autre détail qui a son importance est le tokenizer qui, comme on peut le voir est spécifié à cet endroit, et est ici laissé à la valeur par défaut proposée par SpaCy. C&#39;est le seul pré-requis pour notre composant textcat. | [nlp] lang = &quot;en&quot; pipeline = [&quot;textcat&quot;] batch_size = 1000 disabled = [] before_creation = null after_creation = null after_pipeline_creation = null tokenizer = {&quot;@tokenizers&quot;:&quot;spacy.Tokenizer.v1&quot;} . La spécification du modèle : Notons le paramètre exclusive_classes qui a été mis à true puisque nos posts ne viennent que d&#39;un seul subreddit. Notons aussi qu&#39;il a fallu rajouter le prefix components.textcat dans les headers à la configuration donnée dans la documentation. | [components.textcat] factory = &quot;textcat&quot; scorer = {&quot;@scorers&quot;:&quot;spacy.textcat_scorer.v1&quot;} threshold = 0.5 [components.textcat.model] @architectures = &quot;spacy.TextCatBOW.v2&quot; exclusive_classes = true ngram_size = 1 no_output_layer = false nO = null . Plus de détails sur les pipelines SpaCy Une pipeline SpaCy est une architecture logicielle hautement modulaire et configurable spécialisée pour le traitement automatique de textes. Comme on peut le voir sur l&#39;illustration plus bas, la pipeline contient une étape obligatoire qui est la tokenisation du document, brique de base utilisée par l&#39;ensemble les algorithmes d&#39;analyse de documents. Ensuite viennent une succession de composants (ou pipes) qui sont éxécutés dans l&#39;ordre spécifié, mais qui ne dépendent pas nécessairement les uns des autres. Cela sera dans le code du composant que ces dépendences vont se définir, par exemple en accédant à des attributs définis dans l&#39;objet document de SpaCy. Entra&#238;nement du composant de classification de textes . Contrairement à ce qui peut être trouvé dans la plupart des tutoriels en ligne ou dans la documentation de SpaCy où l&#39;entraînement est démarré depuis la CLI, nous allons essayer de lancer l&#39;entraînement du composant directement depuis un script python. Cela a l&#39;avantage de pouvoir faire cette étape de manière programmatique, par exemple depuis une pipeline de donnée (avec airflow, dagster, ou équivalent). . Toutefois, nous utiliserons la fonction train pré-définie dans spacy.cli.train de telles sorte à bénéficier des fonctionalités de logging, adaptations et autres vérifications qui sont utilisée dans la CLI. Notons que l&#39;on peut tout à fait, et très facilement partir directement du module spacy.training et de gérer le logging / interaction avec le système de fichier par nous même, ce qui serait recommandé dans du code de production. . from spacy.cli.train import train as spacy_train config_path = &quot;spacy_textcat/config.cfg&quot; output_model_path = &quot;output/spacy_textcat&quot; spacy_train( config_path, output_path=output_model_path, overrides={ &quot;paths.train&quot;: &quot;train.spacy&quot;, &quot;paths.dev&quot;: &quot;valid.spacy&quot;, }, ) . ℹ Saving to output directory: output/spacy_textcat ℹ Using CPU =========================== Initializing pipeline =========================== ✔ Initialized pipeline ============================= Training pipeline ============================= ℹ Pipeline: [&#39;textcat&#39;] ℹ Initial learn rate: 0.001 E # LOSS TEXTCAT CATS_SCORE SCORE - 0 0 0.67 3.31 0.03 0 200 97.52 46.14 0.46 0 400 59.67 61.38 0.61 1 600 19.23 73.74 0.74 1 800 11.17 75.77 0.76 2 1000 2.12 74.20 0.74 3 1200 1.19 75.33 0.75 4 1400 0.71 76.68 0.77 4 1600 0.35 75.91 0.76 6 1800 0.28 77.79 0.78 7 2000 0.21 77.91 0.78 9 2200 0.18 77.49 0.77 11 2400 0.06 79.36 0.79 13 2600 0.05 77.81 0.78 16 2800 0.03 77.81 0.78 19 3000 0.03 77.98 0.78 21 3200 0.03 77.05 0.77 24 3400 0.06 77.95 0.78 27 3600 0.02 76.41 0.76 30 3800 0.03 75.48 0.75 32 4000 0.02 77.60 0.78 ✔ Saved pipeline to output directory output/spacy_textcat/model-last . Nous avons maintenant un modèle de classification entraîné ! Spacy stocke le modèle dans des dossiers, et en sauve deux versions, le dernier état du modèle pour permettre de reprendre depuis ce checkpoint s&#39;il on veut affiner le modèle, et le meilleur état du modèle observé pendant l&#39;entraînement. Dans le fichier meta.json dans le dossier du modèle, on peut voir les scores interne qui ont été calculés pendant la validation, et l&#39;on peut voir ici des scores de ~.8 en Macro F score et un AUC à .93. . Le modèle ainsi entraîné et stocké peut ensuite être chargé via spacy de cette manière : . import spacy trained_nlp = spacy.load(&quot;output/spacy_textcat/model-best&quot;) # Let&#39;s try it on an example text text = &quot;Hello n I&#39;m looking for data about birds in New Zealand. nThe dataset would contain the birds species, colors, estimated population etc.&quot; # Perform the trained pipeline on this text doc = trained_nlp(text) # We can display the predicted categories doc.cats . {&#39;datasets&#39;: 0.8466417193412781, &#39;dataengineering&#39;: 0.07126601785421371, &#39;LanguageTechnology&#39;: 0.08209223300218582} . On voit que le document une fois traité par la nouvelle pipeline dispose d&#39;un attributs .cats et que dans ce cas la catégorie datasets est prédite avec 84% de confiance. . Cependant, le reste de la pipeline est vide : pas d&#39;information syntaxique, de dépendance ou d&#39;entités. . print(&quot;entities&quot;, doc.ents) try: print(&quot;sentences&quot;, list(doc.sents)) except ValueError as e: print(&quot;sentences&quot;, &quot;error:&quot;, e) . entities () sentences error: [E030] Sentence boundaries unset. You can add the &#39;sentencizer&#39; component to the pipeline with: `nlp.add_pipe(&#39;sentencizer&#39;)`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`. . Ces informations sont en revanche disponible et dans la pipeline pré-entraînée que nous avions utilisée au début (Mais évidemment, pas les catégories). . doc_from_pretrained = nlp(text) print(&quot;entities&quot;, doc_from_pretrained.ents) print(&quot;sentences&quot;, list(doc_from_pretrained.sents)) print(&quot;classification&quot;, doc_from_pretrained.cats) . entities (New Zealand,) sentences [Hello I&#39;m looking for data about birds in New Zealand., , The dataset would contain the birds species, colors, estimated population etc.] classification {} . La question est donc, comment combiner les deux pipelines nativement sans avoir à charger deux modèles séparément et écrire beaucoup de code pour recoller les morceaux ? . Int&#233;gration du nouveaux composant dans la pipeline existante . Il y a en fait plusieurs manière de le faire : . Créer un pipe et charger le modèle à partir du système de fichiers, mais cela aurait demandé d&#39;utiliser un processus d&#39;entraînement différent de celui que nous avons utilisé ici. | pipe = nlp.add_pipe(&quot;textcat&quot;) pipe.from_disk(&quot;path/to/model/files&quot;) # Note, requires a different folder structure that what we&#39;ve generated . Charger la pipeline, sauver le modèle du composant dans un fichier ou sous forme binaire, et le charger à nouveau dans un second temps depuis le disque / le binaire dans un nouveau pipe ajouté à la pipeline pré-entraînée. | trained_nlp.get_pipe(&quot;textcat&quot;).to_disk(&quot;tmp&quot;) nlp.add_pipe(&quot;textcat&quot;).from_disk(&quot;tmp&quot;) # OR nlp.add_pipe(&quot;textcat&quot;).from_bytes( trained_nlp.get_pipe(&quot;textcat&quot;).to_bytes() ) . Créer le pipe depuis une pipeline source. | nlp_merged = spacy.load(&quot;en_core_web_md&quot;) nlp_merged.add_pipe(&quot;textcat&quot;, source=trained_nlp) doc_from_merged = nlp_merged(text) print(&quot;entities&quot;, doc_from_merged.ents) print(&quot;sentences&quot;, list(doc_from_merged.sents)) print(&quot;classification&quot;, doc_from_merged.cats) . entities (New Zealand,) sentences [Hello I&#39;m looking for data about birds in New Zealand., , The dataset would contain the birds species, colors, estimated population etc.] classification {&#39;datasets&#39;: 0.8466417193412781, &#39;dataengineering&#39;: 0.07126601785421371, &#39;LanguageTechnology&#39;: 0.08209223300218582} . /Users/yco/.pyenv/versions/myreddit/lib/python3.8/site-packages/spacy/language.py:707: UserWarning: [W113] Sourced component &#39;textcat&#39; may not work as expected: source vectors are not identical to current pipeline vectors. warnings.warn(Warnings.W113.format(name=source_name)) . À partir de là, il est possible de sauver la pipeline sur le disque et de la réutiliser à volonté, ou encore de l&#39;enrichir avec d&#39;autres composants personalisés (pourquoi pas un second classifier, ou encore un modèle de NER complémentaire de celui présent dans la pipeline) avec l&#39;ensemble des fonctionalités. . Conclusion . Dans ce tutoriel, nous avons vu comment entraîner et intégrer un composant de classification de textes à une pipeline pré-existante, d&#39;une manière complètement programmatique. La grande valeur ajoutée de cette procédure est qu&#39;elle peut être complètement automatisée et permettre d&#39;entraîner / assembler un grand nombre de composants de manière modulaire et automatique (par exemple en CI/CD) . J&#39;espère que le post vous auta été utile et n&#39;hésitez pas à me contacter si vous voulez plus de détails dans les commentaires ! .",
            "url": "https://ycouble.github.io/til/fr/python/nlp/spacy/ml/mlops/2022/02/09/Entrainement-et-integration-d-un-classifieur-de-textes-a-une-pipeline-spacy.html",
            "relUrl": "/fr/python/nlp/spacy/ml/mlops/2022/02/09/Entrainement-et-integration-d-un-classifieur-de-textes-a-une-pipeline-spacy.html",
            "date": " • Feb 9, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Training and integrating a custom text classifier to a spacy pipeline",
            "content": "Introduction . Spacy is a Natural Language Processing (NLP) library and framework to productionalize Machine Learning and NLP applications. . There are loads of resources on training a Spacy component such as a NER, text classification or other basic NLP components, but I couldn&#39;t find one that finished the work well, i.e. where you&#39;d end up with a full-fledged pre-trained model for common components such as Dependency Parsing, POS tagging or NER and a custom component to predict a specific task. . This is what this notebook / post is here to do: guide you through the whole process of configuring a text classification component, training it from python and integrating it into a fully featured pre-trained model, to be reused from anywhere else. . Pre-requisites . First let&#39;s install the pacakges we&#39;re going to need for this tutorial . %pip install -q &quot;spacy&gt;3.0.0&quot; pandas sklearn . Note: you may need to restart the kernel to use updated packages. . We&#39;ll also need to download a pretrained model from spaCy english models: https://spacy.io/models/en#en_core_web_md. The following command needs to be executed from the same environment as your notebook kernel. . !python -m spacy download en_core_web_md . The data &amp; the classification task . We&#39;ll work from dataset which is extracted through the reddit API. I&#39;ve prepared it so that it can be easily imported and converted into spacy docs. . You can find the dataset here. . The dataset is a simple extract from Reddit of different posts body from a selection of subreddits related to data science. The objectif of our task will be to guess from the text to which subreddit the post comes from. Even though the interest is limited, this is a good use case to start with and easy to obtain labelled data from. . Let&#39;s load the dataset to inspect what is inside. . import pandas as pd pd.options.display.max_colwidth = None pd.options.display.max_rows = 6 data = pd.read_csv(&quot;spacy_textcat/reddit_data.csv&quot;) data . text tag subreddit id . 0 I’m looking for datasets or api source that quantifies fan base, or preferably, bettors’ sentiment regarding a team’s performance or direction. Does anyone know of an API that tracks this? For now I’m looking specifically for NBA, but am also interested in MLB, NFL, and NCAA f-ball and b-ball. | API | datasets | s0vufk | . 1 I&#39;m making an ESG stock analysis program in Java, and so far the only free ESG API I&#39;ve come across is ESGEnterprise, but I&#39;m having trouble retrieving the data. Has anyone had any success/have any recs for other ESG APIs out there. | API | datasets | ruvj9n | . 2 Hey everyone! I’m one of the creators of Sieve _URL_ and I’m excited to be sharing it! nSieve _URL_ **is an API that helps you turn petabyte-scale video data into a high-quality dataset, automatically.** nIt helps store, process, and semantically search your video data. Just think _NUMBER_ cameras recording footage at _NUMBER_ FPS, _NUMBER_/_NUMBER_. That would be _NUMBER_ million frames generated in a single day. The videos might be searchable by timestamp, but finding moments of interest is like searching for a needle in a haystack. Sieve tags useful attributes like people, motion, lighting, etc on every frame! nWe built this visual demo (link here _URL_ a little while back which we’d love to get feedback on. It’s ~_NUMBER_ hours of security footage that our API processed in &lt;_NUMBER_ mins and has simple querying and export functionality enabled. We see applications in better understanding what data you have, figuring out which data to send to labeling, sampling datasets for training, and building multiple test sets for models by scenario. nTo try it on your videos: _URL_ _URL_ nVisual dashboard walkthrough: Click on our site link! | API | datasets | rup1uj | . ... ... | ... | ... | ... | . 718 I&#39;m currently in the process of learning NLP. I am using catalyst on c#. nI was able to run the sample programs and it was able to determine if the word is an noun, adjective, etc. But I can&#39;t find any sample for what I need. nHere is a summary of what I would like to achieve. nI would like to extract certain information on a sentence. Lets say i have the following texts: n&quot;Sally ate an orange this morning. &quot; nOr n&quot;Sally is hiding behind the cabinet and she is eating an orange. &quot; nHow do i use the nlp to extract what sally ate? | NaN | LanguageTechnology | saas64 | . 719 I’ve been trying to do some basic keyword extraction and finding it harder than expected. nKeyBERT seems good but it requires a powerful GPU to be usably fast. That’s possible with AWS, but there’s a bit more set up. nI just tried PyTextRank, and I was surprised at the quality of the output - I wouldn’t say it was perfect either. Maybe I should set a threshold, like choose the top _NUMBER_ ranked keywords? It’s fine if we exclude potential good keywords just to have a smaller list of good ones. nHere’s a good article about _NUMBER_ different methods, which is helpful - n_URL_ nIn theory, Spacy and BERT seem like the best options but they’re both a little complex. nI think KW extraction really only needs a few layers or as Spacy would call them pipelines. n_NUMBER_. accurate tokenization of words and punctuation symbols n_NUMBER_. accurate recognition of multi-word expressions - think of it as “chunking” n_NUMBER_. Strong assessment of keyword “candidacy” for each MWE nOf course, a good algorithm can often skip steps. Like BERT is so smart it doesn’t need anything but the input text. nDoes anyone know of a simplest way to run a fast, effective keyword extraction? nI’m talking _NUMBER_ keywords in one second on a fast CPU. nThanks very much | NaN | LanguageTechnology | s7qml8 | . 720 This _URL_ position is currently open and I wanted to share with you! | NaN | LanguageTechnology | s30ccv | . 721 rows × 4 columns . cats = data.subreddit.unique().tolist() cats . [&#39;datasets&#39;, &#39;dataengineering&#39;, &#39;LanguageTechnology&#39;] . The dataset is composed of ~700 cleaned texts along with the subredddit they&#39;ve been extracted from on Reddit. Let&#39;s now create the Spacy training / validation data by annotating spacy docs created from the texts. . Creating the training dataset . First let&#39;s make a function to transform the dataset into a spacy dataset that we&#39;ll need to store locally. . from typing import Set, List, Tuple from spacy.tokens import DocBin import spacy # Load spaCy pretrained model that we downloaded before nlp = spacy.load(&quot;en_core_web_md&quot;) # Create a function to create a spacy dataset def make_docs(data: List[Tuple[str, str]], target_file: str, cats: Set[str]): docs = DocBin() # Use nlp.pipe to efficiently process a large number of text inputs, # the as_tuple arguments enables giving a list of tuples as input and # reuse it in the loop, here for the labels for doc, label in nlp.pipe(data, as_tuples=True): # Encode the labels (assign 1 the subreddit) for cat in cats: doc.cats[cat] = 1 if cat == label else 0 docs.add(doc) docs.to_disk(target_file) return docs . Let&#39;s now split the dataset into training and validation datasets, and store them for training . from sklearn.model_selection import train_test_split X_train, X_valid, y_train, y_valid = train_test_split(data[&quot;text&quot;].values, data[&quot;subreddit&quot;].values, test_size=0.3) make_docs(list(zip(X_train, y_train)), &quot;train.spacy&quot;, cats=cats) make_docs(list(zip(X_valid, y_valid)), &quot;valid.spacy&quot;, cats=cats) . &lt;spacy.tokens._serialize.DocBin at 0x12b189100&gt; . Creating and configuring a Text Classification component . The recommended training workflow with SpaCy uses config files. They enable to configure each component of the model pipeline, set which components can be trained etc. . We&#39;ll use this configuration file, which uses the default text classifier model from spacy. It was generated through this SpaCy tool: https://spacy.io/usage/training#quickstart and customized to use the default text classification model from: https://spacy.io/api/architectures#TextCatBOW. . The important parts in the file are the following: . The pipeline definition (under the nlp tag): The pipeline is composed only of a textcat component since it is the only one we have labelled data for and the only one we are going to train today. Also important to mention is the tokenizer component that we have left to the spacy default, but could be customized. This is the only component that we need to train our textcat component. | [nlp] lang = &quot;en&quot; pipeline = [&quot;textcat&quot;] batch_size = 1000 disabled = [] before_creation = null after_creation = null after_pipeline_creation = null tokenizer = {&quot;@tokenizers&quot;:&quot;spacy.Tokenizer.v1&quot;} . The model specification: Almost all parameters set to defaults except the exclusive_classesset to true since our posts only come from one subreddit (although that might be questionned). Note that compared to the config given in the documentation, we had to add the components.textcat prefix to the headers. | [components.textcat] factory = &quot;textcat&quot; scorer = {&quot;@scorers&quot;:&quot;spacy.textcat_scorer.v1&quot;} threshold = 0.5 [components.textcat.model] @architectures = &quot;spacy.TextCatBOW.v2&quot; exclusive_classes = true ngram_size = 1 no_output_layer = false nO = null . More details on SpaCy Pipelines The spacy pipeline is a modular and highly configurable workflow for processing texts. As shown below, there is a mandatory first step of tokenizing the text, then there are a succession of pipeline components (or pipes) that are executed in order, but do not necessarily rely on each other. It will be in the code of a component that the dependency would be set, e.g. by accessing previously set attributes in the doc element. Training the text classification component . Unlike what can be found in most of the tutorials online or in the SpaCy docs where training is started from CLI, we&#39;re going to train the component from a config file and from a python script. This has the advantage of letting us start the training programatically, e.g. from a data pipeline (using airflow, dagster or such). . However, we&#39;ll use the spacy pre-built training function from spacy.cli.train in order to benefit from all the checks and logging that are set up in the CLI. (Another approach would be to reuse parts of the code, calling directly the spacy.training module instead) . from spacy.cli.train import train as spacy_train config_path = &quot;spacy_textcat/config.cfg&quot; output_model_path = &quot;output/spacy_textcat&quot; spacy_train( config_path, output_path=output_model_path, overrides={ &quot;paths.train&quot;: &quot;train.spacy&quot;, &quot;paths.dev&quot;: &quot;valid.spacy&quot;, }, ) . ℹ Saving to output directory: output/spacy_textcat ℹ Using CPU =========================== Initializing pipeline =========================== ✔ Initialized pipeline ============================= Training pipeline ============================= ℹ Pipeline: [&#39;textcat&#39;] ℹ Initial learn rate: 0.001 E # LOSS TEXTCAT CATS_SCORE SCORE - 0 0 0.67 3.31 0.03 0 200 97.52 46.14 0.46 0 400 59.67 61.38 0.61 1 600 19.23 73.74 0.74 1 800 11.17 75.77 0.76 2 1000 2.12 74.20 0.74 3 1200 1.19 75.33 0.75 4 1400 0.71 76.68 0.77 4 1600 0.35 75.91 0.76 6 1800 0.28 77.79 0.78 7 2000 0.21 77.91 0.78 9 2200 0.18 77.49 0.77 11 2400 0.06 79.36 0.79 13 2600 0.05 77.81 0.78 16 2800 0.03 77.81 0.78 19 3000 0.03 77.98 0.78 21 3200 0.03 77.05 0.77 24 3400 0.06 77.95 0.78 27 3600 0.02 76.41 0.76 30 3800 0.03 75.48 0.75 32 4000 0.02 77.60 0.78 ✔ Saved pipeline to output directory output/spacy_textcat/model-last . We now have a trained classification model ! Spacy stores the model in folders, and usually saves both the best model and the last state of the model at the end of the training, in case we&#39;d want to continue training from this step. In the meta.json file in the model folder you can find the internal scores that were computed, and we can see that we have ~80% Macro F score and a nice .93 AUC. . We can import the newly trained pipeline from spacy to predict like this: . import spacy trained_nlp = spacy.load(&quot;output/spacy_textcat/model-best&quot;) # Let&#39;s try it on an example text text = &quot;Hello n I&#39;m looking for data about birds in New Zealand. nThe dataset would contain the birds species, colors, estimated population etc.&quot; # Perform the trained pipeline on this text doc = trained_nlp(text) # We can display the predicted categories doc.cats . {&#39;datasets&#39;: 0.8466417193412781, &#39;dataengineering&#39;: 0.07126601785421371, &#39;LanguageTechnology&#39;: 0.08209223300218582} . We see that the model predicts the subreddit datasets with 84% confidence ! . However, the rest of the trained pipeline is empty, NER, dependencies etc. have not been computed. . print(&quot;entities&quot;, doc.ents) try: print(&quot;sentences&quot;, list(doc.sents)) except ValueError as e: print(&quot;sentences&quot;, &quot;error:&quot;, e) . entities () sentences error: [E030] Sentence boundaries unset. You can add the &#39;sentencizer&#39; component to the pipeline with: `nlp.add_pipe(&#39;sentencizer&#39;)`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`. . These are however, available with high quality in the pre-trained pipeline that we used earlier. But not the classification, obviously. . doc_from_pretrained = nlp(text) print(&quot;entities&quot;, doc_from_pretrained.ents) print(&quot;sentences&quot;, list(doc_from_pretrained.sents)) print(&quot;classification&quot;, doc_from_pretrained.cats) . entities (New Zealand,) sentences [Hello I&#39;m looking for data about birds in New Zealand., , The dataset would contain the birds species, colors, estimated population etc.] classification {} . The question is, then, how do we combine both pipelines without having to implement a lot of glue code for nothing ? . Integrate the new component to an existing pipeline . There are actually several ways to do this: . Creating a pipe and loading the model from files, but this would require a different training process than what we did (on a model level and not pipeline level) | pipe = nlp.add_pipe(&quot;textcat&quot;) pipe.from_disk(&quot;path/to/model/files&quot;) # Note, requires a different folder structure that what we&#39;ve generated . Loading the pipeline, saving the model to disk/bytes and loading it back again from disk/bytes in a new pipe in the pretrained pipeline | trained_nlp.get_pipe(&quot;textcat&quot;).to_disk(&quot;tmp&quot;) nlp.add_pipe(&quot;textcat&quot;).from_disk(&quot;tmp&quot;) # OR nlp.add_pipe(&quot;textcat&quot;).from_bytes( trained_nlp.get_pipe(&quot;textcat&quot;).to_bytes() ) . Creating the pipe with a source pipeline. | nlp_merged = spacy.load(&quot;en_core_web_md&quot;) nlp_merged.add_pipe(&quot;textcat&quot;, source=trained_nlp) doc_from_merged = nlp_merged(text) print(&quot;entities&quot;, doc_from_merged.ents) print(&quot;sentences&quot;, list(doc_from_merged.sents)) print(&quot;classification&quot;, doc_from_merged.cats) . entities (New Zealand,) sentences [Hello I&#39;m looking for data about birds in New Zealand., , The dataset would contain the birds species, colors, estimated population etc.] classification {&#39;datasets&#39;: 0.8466417193412781, &#39;dataengineering&#39;: 0.07126601785421371, &#39;LanguageTechnology&#39;: 0.08209223300218582} . /Users/yco/.pyenv/versions/myreddit/lib/python3.8/site-packages/spacy/language.py:707: UserWarning: [W113] Sourced component &#39;textcat&#39; may not work as expected: source vectors are not identical to current pipeline vectors. warnings.warn(Warnings.W113.format(name=source_name)) . Removing the warning To remove the vector mis-alignment, you&#39;d have to train the pipeline by passing the pre-trained tok2vec component from the pre-trained model. In our case it&#39;s not a problem since the vectors are probably very similar, but nonetheless, I&#39;ll try to update the post when I can with the solution to this warning. From this point we can store and reuse the pipeline at will ! . Conclusion . We&#39;ve seen in this tutorial how to train and integrate a text classification component to a pre-trained pipeline, in a fully programmatic way. . Hope you liked the post and feel free to contact me if you want more details in the comments ! .",
            "url": "https://ycouble.github.io/til/en/python/nlp/spacy/ml/mlops/2022/02/01/Adding-a-text-classifier-to-a-spacy-pipeline.html",
            "relUrl": "/en/python/nlp/spacy/ml/mlops/2022/02/01/Adding-a-text-classifier-to-a-spacy-pipeline.html",
            "date": " • Feb 1, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "La Famille BERT et les Transformers",
            "content": "Une petite r&#233;volution . Ces dernières années, la recherche en Intelligence Artificielle a été complètement transformée par une avancée technologique: les transformers. Avant de comprendre comment et pourquoi cette transformation s&#39;est opérée, prenons un instant pour examiner comment la recherche en IA évalue une avancée technique. . La recherche en IA se concentre depuis plusieurs années sur les tâches dites difficiles de la compréhension du monde: les systèmes de recommandation de contenu, l&#39;extraction d&#39;informations à partir de textes (discipline du traitement automatique du langage ou Natural Language Processing, NLP en anglais) ou d&#39;image (Vision par ordinateur ou Computer Vision, CV) et plus récemment également sur l&#39;audio. Chacune de ces discipline a déterminé un certain nombre de tâches d&#39;extraction d&#39;information que l&#39;on cherche à faire effectuer à un programme, comme par exemple : . La détection d&#39;objets (localiser une personne dans une image), la segmentation d&#39;image (identifier la portion de l&#39;image représentant une route), la classification d&#39;images (dire si il s&#39;agit d&#39;une photo de lave-linge ou d&#39;un frigo) en vision par ordinateur | La classification de texte (dire si un texte est positif ou négatif), l&#39;extraction d&#39;entités nommées d&#39;un texte (identifier les noms de personnes, d&#39;organisation, de pays etc.), le résumé de texte ou la réponse aux questions sur un texte dans le domaine du traitement du langage naturel. Pour chacune de ces tâches ont été déterminés des jeux de données de référence et des méthodologies d&#39;évaluation de l&#39;efficacité d&#39;un algorithme à réaliser la tâche en question. | . Jusque là, les meilleures performances étaient atteintes par des modèles spécialisés pour chaque tâche précise, et l&#39;entraînement de chaque modèle était un défi en soi. C&#39;est dans ce contexte que les Transformers ont apporté une petite révolution: en 2018 dans l&#39;article présentant BERT (BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding), Google présente une méthodologie de pré-entraînement et de spécialisation qui, appliquée à une architecture à base de Transformers, surpasse l&#39;état de l&#39;art sur chacune des tâches classiques de traitement du langage naturel, et ce d&#39;une marge conséquente. . Pourquoi les Transformers et BERT fonctionnent-ils si bien ? . La contribution du papier de BERT a été dans un premier temps de combiner avec succès plusieurs techniques déjà connues: . Les Transformers, une architecture de Deep Learning qui avait déjà fait ses preuves. Cette architecture est basée sur le mécanisme d&#39;attention, qui permet de prendre en compte tout le contexte d&#39;un mot en accordant à priori autant d&#39;importance à des mots éloignés qu&#39;à des mots proches, contrairement aux modèles récurrents qui faisaient l&#39;état de l&#39;art avant l&#39;arrivée des Transformers, en 2016 (cf. Attention is All You Need, Google 2016 dans les références.) | . Le Transfer Learning, qui consiste à entraîner un modèle sur une tâche très générique (souvent non supervisée) sur un dataset gigantesque (par exemple le corpus de Wikipédia), puis à spécialiser le modèle en le modifiant sur une seconde tâche différente de manière supervisée et sur un dataset plus réduit. | De plus, leur contribution est aussi et surtout dans la méthodologie de pré-entrainement du modèle de langage (le premier modèle), qui se fait en combinant deux tâches génériques non supervisées: . La complétion de phrases à trou, c&#39;est à dire trouver le meilleur mot pour compléter une phrase du type: &quot;je suis allé au __ ce matin pour acheter mes légumes&quot;. | La prédiction de la prochaine phrase dans un texte. | Ces deux tâches peuvent s&#39;effectuer grâce à un entraînement auto-supervisé sur des datasets gigantesques comme l&#39;intégralité de Wikipedia, des corpus de sites crawlé ou à partir des réseaux sociaux publics (Twitter, Reddit ...). En passant en revue tous ces contenus, l&#39;algorithme devient de plus en plus performant à déterminer les mots qui vont généralement ensemble en fonction d&#39;un contexte assez large (de l&#39;ordre de la phrase ou du paragraphe). . Qu&#39;est-ce que &#231;a change ? . Le gros changement apporté par BERT est cette idée de modèle de langage: la plupart des tâches de NLP ont en commun d&#39;essayer de comprendre le contexte d&#39;un mot et de trouver les relations entre les mots d&#39;une phrase, et c&#39;est ce que tente d&#39;apporter le modèle de langage. Concrètement, cela signifie que pour n&#39;importe quelle tâche de NLP, il suffit maintenant de réutiliser le modèle de langage pour avoir cette compréhension générale, puis de spécialiser la &quot;tête&quot; de l&#39;architecture pour une tâche donnée. . La seconde conséquence est d&#39;ordre pratique: avec un modèle comme BERT, l&#39;entraînement d&#39;un modèle sur une tâche spécifique se résume généralement à n&#39;entraîner que la tête de l&#39;architecture, donc à un coût dérisoire par rapport à un modèle complet comme précédemment. . Maintenant, il est devenu extrêmement facile d&#39;utiliser et d&#39;entraîner un modèle très performant pour toute une diversité de tâches de NLP. Cela a été grandement facilité par la publication du code et des modèles pré-entraînés, et par l&#39;arrivée de Hugging Face, une entreprise de l&#39;open source fondée par deux français qui propose une interface standard et un hub de modèles pour les transformers. . . Utilisation des transformers . BERT et les autres transformers sont conçus pour être réutilisable en grande partie pour différentes tâches, il ne reste donc qu&#39;à réadapter les entrées et sorties à la tâche visée. Nous allons regarder dans cet article le cas de la spécialisation et de l&#39;utilisation d&#39;un modèle de transformer pour une tâche d&#39;extraction d&#39;entités nommées (NER) à partir de différentes bibliothèques open-source de NLP. . La t&#226;che d&#39;extraction d&#39;entit&#233;s nomm&#233;es: donn&#233;es et annotation . Le but de l&#39;extraction d&#39;entités nommées est de reconnaître dans un texte les mots ou ensembles de mots qui correspondent à des villes, pays, dates, langages de programmation, personnes ou tout autre catégorie pour lesquels on ne peut généralement pas lister l&#39;ensemble des mots qui représentent ces entités. . Par exemple, dans la phrase &quot;My name is Wolfgang and I live in Berlin&quot;, la tâche de NER devrait permettre d&#39;extraction des mots “Wolfgang”, classés en tant que personne et “Berlin” comme ville. . Il s&#39;agit en fin de compte d&#39;une tâche de classification de token (un token est un mot ou une partie de mot), supervisée, et il y a plusieurs méthodes d&#39;annotation des données. Nous allons présenter uniquement la méthode IOB, acronyme pour Inside, Outside et Begin, où chaque mot reçoit une annotation O ou I/B-[type d&#39;entité]. Pour notre exemple cela donnerait ça: . My name is Wolfgang and I live in Berlin O O O B-PER 0 0 0 0 B-CITY . Ou sur un second exemple: . I&#39;m Yoann Couble and I write about Natural Language Processing in Python O O B-PER I-PER O 0 O O B-TECH I-TECH I-TECH O B-LANG . L&#39;objectif de l&#39;algorithme sera donc de déterminer pour chaque token l&#39;annotation à positionner. . &#129303; Hugging Face Transformers . Hugging Face a construit toute une API pour faciliter l&#39;utilisation des transformers. La librairie propose des pipelines pré-définies et une bibliothèque de modèles explorables par tâches et langue et de datasets prêtes à l&#39;emplois. L&#39;avantage de Hugging Face est la grande diversité de modèles expérimentaux ou éprouvés qui sont disponibles sur le hub et la facilité d&#39;utilisation et d&#39;expérimentation qu&#39;il permet. . La configuration d&#39;un modèle se fait très facilement (sans code) à l&#39;aide d&#39;un fichier de configuration: https://huggingface.co/dslim/bert-base-NER/blob/main/config.json . { &quot;_num_labels&quot;: 9, &quot;architectures&quot;: [ &quot;BertForTokenClassification&quot; ], &quot;attention_probs_dropout_prob&quot;: 0.1, &quot;hidden_act&quot;: &quot;gelu&quot;, &quot;hidden_dropout_prob&quot;: 0.1, &quot;hidden_size&quot;: 768, &quot;initializer_range&quot;: 0.02, &quot;intermediate_size&quot;: 3072, &quot;layer_norm_eps&quot;: 1e-12, &quot;max_position_embeddings&quot;: 512, &quot;model_type&quot;: &quot;bert&quot;, &quot;num_attention_heads&quot;: 12, &quot;num_hidden_layers&quot;: 12, &quot;output_past&quot;: true, &quot;pad_token_id&quot;: 0, &quot;type_vocab_size&quot;: 2, &quot;vocab_size&quot;: 28996, # Labels &quot;id2label&quot;: { &quot;0&quot;: &quot;O&quot;, &quot;1&quot;: &quot;B-MISC&quot;, &quot;2&quot;: &quot;I-MISC&quot;, &quot;3&quot;: &quot;B-PER&quot;, &quot;4&quot;: &quot;I-PER&quot;, &quot;5&quot;: &quot;B-ORG&quot;, &quot;6&quot;: &quot;I-ORG&quot;, &quot;7&quot;: &quot;B-LOC&quot;, &quot;8&quot;: &quot;I-LOC&quot; }, &quot;label2id&quot;: { &quot;B-LOC&quot;: 7, &quot;B-MISC&quot;: 1, &quot;B-ORG&quot;: 5, &quot;B-PER&quot;: 3, &quot;I-LOC&quot;: 8, &quot;I-MISC&quot;: 2, &quot;I-ORG&quot;: 6, &quot;I-PER&quot;: 4, &quot;O&quot;: 0 } } . Ici l&#39;architecture choisie est BertForTokenClassification ce qui correspond à notre tâche de NER. Et on retrouve aussi les différents hyper-paramètres des transformers (nombre de têtes d&#39;attention, paramètres d&#39;attention, ...). Pour pouvoir réutiliser les poids d&#39;entraînement d&#39;un modèle pré-entraîné, il faut faire attention à ne changer que ce qui ne casse pas la compatibilité avec le modèle pré-entraîné. . Dans ce notebook nous n&#39;allons pas entraîner de nouveau modèle, mais utiliser directement le modèle qui a été entraîné avec la configuration présentée au-dessus. . Pr&#233;-requis . %pip install --upgrade -q torch transformers &quot;spacy&gt;=3.2.0&quot; . from transformers import AutoTokenizer, AutoModelForTokenClassification from transformers import pipeline model_name = &quot;dslim/bert-base-NER&quot; # Récupération du modèle et d&#39;un tokenizer adapté (peut prendre du temps car il faut télécharger le modèle qui est assez volumineux) tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForTokenClassification.from_pretrained(model_name) # Définition de la pipeline nlp = pipeline(&quot;ner&quot;, model=model, tokenizer=tokenizer) . example = &quot;My name is Wolfgang and I live in Berlin&quot; ner_results = nlp(example) ner_results . [{&#39;entity&#39;: &#39;B-PER&#39;, &#39;score&#39;: 0.99901396, &#39;index&#39;: 4, &#39;word&#39;: &#39;Wolfgang&#39;, &#39;start&#39;: 11, &#39;end&#39;: 19}, {&#39;entity&#39;: &#39;B-LOC&#39;, &#39;score&#39;: 0.999645, &#39;index&#39;: 9, &#39;word&#39;: &#39;Berlin&#39;, &#39;start&#39;: 34, &#39;end&#39;: 40}] . Notons que seuls les tokens classifiés avec une entité sont montrés, le reste a reçu une annotation &quot;O&quot;. . Utilisation des transformers avec SpaCy . Spacy est une bibliothèque open-source permettant d&#39;industrialiser des applications de traitement du langage naturel. L&#39;avantage de Spacy par rapport à Hugging Face est qu&#39;il va être possible de mutualiser un même modèle de langage à base de transformers pour plusieurs modèles spécialisés pour différentes tâches sur un même document. . Spacy fournit également des modèles pré-entraînés et bien intégrés à Spacy. Ce sont les modèles finissant en _trf comme celui-ci basé sur camembert-base . Pour pouvoir l&#39;utiliser, il faut télécharger le modèle dans le même environnement virtuel / kernel (dans mon cas, un environnement pyenv) . pyenv activate transformers_3.8.6 python -m spacy download en_core_web_trf . import spacy from spacy import displacy nlp = spacy.load(&quot;en_core_web_trf&quot;) doc = nlp(&quot;My name is Wolfgang and I live in Berlin&quot;) displacy.render(doc, &quot;ent&quot;) . My name is Wolfgang PERSON and I live in Berlin GPE Une adoption qui s&#39;&#233;tend maintenant au del&#224; du langage . Nous avons pu voir et comprendre l&#39;apport de BERT et des transformers au paysage du NLP, et l&#39;impact des transformers continue de faire son chemin, avec la vision qui a eu tôt fait de les adopter (modèls VIT, VILT, CLIP etc.) ainsi que l&#39;audio et la vidéo. . Maintenant, la recherche sur les transformers se plonge dans la convergence multi-modale (fusion Vidéo-Audio-Texte), avec notamment le très récent Data2Vec de Facebook AI Research cette année. Cela dit, le problème difficile de la compréhension du monde est loin d&#39;être résolu, et la recherche continue d&#39;être très active sur le sujet. . Ressources et r&#233;d&#233;rences sur le sujet . R&#233;f&#233;rences . Attention is all you need (2016, Transformer paper): https://arxiv.org/abs/1706.03762 | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018): https://arxiv.org/abs/1810.04805 | Illustrated transformer: http://jalammar.github.io/illustrated-transformer/ | BERT model doc on hugging face: https://huggingface.co/transformers/model_doc/bert.html#bertmodel | https://spacy.io/usage/embeddings-transformers | . Exemples . Exemple de configuration sur le hub de huggingface: https://huggingface.co/dslim/bert-base-NER/blob/main/config.json | Transformers pour le NER FR https://huggingface.co/models?language=fr&amp;pipeline_tag=token-classification&amp;sort=downloads&amp;search=ner | Use huggingface transformers within spacy: https://reposhub.com/python/deep-learning/explosion-spacy-transformers.html | . Cours et ressources formatrices . https://www.coursera.org/learn/attention-models-in-nlp/home/welcome | https://huggingface.co/course/chapter1 | https://course.spacy.io/en/chapter4 | https://www.youtube.com/playlist?list=PL75e0qA87dlG-za8eLI6t0_Pbxafk-cxb | .",
            "url": "https://ycouble.github.io/til/fr/ml/nlp/2021/11/23/bert.html",
            "relUrl": "/fr/ml/nlp/2021/11/23/bert.html",
            "date": " • Nov 23, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Transfer Learning",
            "content": "from keras.models import Sequential import cv2 get_ipython().magic(&#39;matplotlib inline&#39;) import matplotlib.pyplot as plt import numpy as np import keras from keras.layers import Dense import pandas as pd from keras.applications.vgg16 import VGG16 from keras.preprocessing import image from keras.applications.vgg16 import preprocess_input import numpy as np from keras.applications.vgg16 import decode_predictions from pathlib import Path mnist_path = Path(&quot;mnist&quot;) img_path = Path(&quot;mnist/images/&quot;) . Generate Dataset from scikit learn dataset . from sklearn import datasets digits = datasets.load_digits() for i, im in enumerate(digits[&#39;images&#39;]): cv2.imwrite(Path.joinpath(img_path, f&quot;{i}.png&quot;).as_posix(),im*16) (pd.DataFrame({&quot;labels&quot;: digits[&quot;target&quot;]}) .to_csv(Path.joinpath(img_path, &quot;labels.csv&quot;).as_posix()) ) nb_img = len(list(img_path.glob(&#39;*.png&#39;))) limit = int(nb_img*0.75) train_range = range(limit) test_range = range(limit, nb_img) print(nb_img, limit, train_range, test_range) . 1797 1347 range(0, 1347) range(1347, 1797) . train_img=[] for i in train_range: temp_img=image.load_img( Path.joinpath(img_path, f&quot;{i}.png&quot;).as_posix(), target_size=(224,224) ) temp_img=image.img_to_array(temp_img) train_img.append(temp_img) #converting train images to array and applying mean subtraction processing train_img=np.array(train_img) train_img=preprocess_input(train_img) # applying the same procedure with the test dataset test_img=[] for i in test_range: temp_img=image.load_img( Path.joinpath(img_path, f&quot;{i}.png&quot;).as_posix(), target_size=(224,224) ) temp_img=image.img_to_array(temp_img) test_img.append(temp_img) test_img=np.array(test_img) test_img=preprocess_input(test_img) . (224, 224, 3) . transferred_model = VGG16(weights=&#39;imagenet&#39;, include_top=False) transferred_model.summary() . Model: &#34;vgg16&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_8 (InputLayer) [(None, None, None, 3)] 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, None, None, 64) 1792 _________________________________________________________________ block1_conv2 (Conv2D) (None, None, None, 64) 36928 _________________________________________________________________ block1_pool (MaxPooling2D) (None, None, None, 64) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, None, None, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, None, None, 128) 147584 _________________________________________________________________ block2_pool (MaxPooling2D) (None, None, None, 128) 0 _________________________________________________________________ block3_conv1 (Conv2D) (None, None, None, 256) 295168 _________________________________________________________________ block3_conv2 (Conv2D) (None, None, None, 256) 590080 _________________________________________________________________ block3_conv3 (Conv2D) (None, None, None, 256) 590080 _________________________________________________________________ block3_pool (MaxPooling2D) (None, None, None, 256) 0 _________________________________________________________________ block4_conv1 (Conv2D) (None, None, None, 512) 1180160 _________________________________________________________________ block4_conv2 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________ block4_conv3 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________ block4_pool (MaxPooling2D) (None, None, None, 512) 0 _________________________________________________________________ block5_conv1 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________ block5_conv2 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________ block5_conv3 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________ block5_pool (MaxPooling2D) (None, None, None, 512) 0 ================================================================= Total params: 14,714,688 Trainable params: 14,714,688 Non-trainable params: 0 _________________________________________________________________ . re_extract_features = False if re_extract_features: # Extracting features from the train dataset using the VGG16 pre-trained model features_train=transferred_model.predict(train_img) # Extracting features from the train dataset using the VGG16 pre-trained model features_test=transferred_model.predict(test_img) else: import h5py with h5py.File(&quot;mnist/mnist_features.hdf5&quot;, &quot;r&quot;) as f: features_train = np.array(f[&quot;features/train&quot;]) features_test = np.array(f[&quot;features/test&quot;]) print(features_train.shape, features_test.shape) . (1347, 7, 7, 512) (450, 7, 7, 512) . Store the model and the features . features_train.shape, features_test.shape . ((1347, 7, 7, 512), (450, 7, 7, 512)) . import h5py . with h5py.File(&quot;mnist/mnist_features.hdf5&quot;, &quot;w&quot;) as f: dset_train = f.create_dataset(&quot;features/train&quot;, data=features_train) dset_test = f.create_dataset(&quot;features/test&quot;, data=features_test) . Note: Ajouter une phase de mise en évidence des features #explainability . Adapt features to new MLP . labels = pd.read_csv(Path.joinpath(img_path, &quot;labels.csv&quot;).as_posix(), index_col=0) labels . labels . 0 0 | . 1 1 | . 2 2 | . 3 3 | . 4 4 | . ... ... | . 1792 9 | . 1793 0 | . 1794 8 | . 1795 9 | . 1796 8 | . 1797 rows × 1 columns . train_x=features_train.reshape(features_train.shape[0], 25088) # converting target variable to array train_y=labels[&quot;labels&quot;].values[train_range] # performing one-hot encoding for the target variable train_y=pd.get_dummies(train_y) train_y=np.array(train_y) # creating training and validation set print(&quot;Training set&quot;, train_x.shape, train_y.shape) # flattening the layers to conform to MLP input (N, 7, 7, 512) --&gt; (N, 25088) test_x=features_test.reshape(features_test.shape[0], 25088) # converting target variable to array test_y=labels[&quot;labels&quot;].values[test_range] # performing one-hot encoding for the target variable test_y=pd.get_dummies(test_y) test_y=np.array(test_y) # creating testing and validation set print(&quot;Testing set&quot;, test_x.shape, test_y.shape) . Training set (1347, 25088) (1347, 10) Testing set (450, 25088) (450, 10) . Create the MLP model . from keras.layers import Dense, Activation model=Sequential() model.add(Dense(1000, input_dim=25088, activation=&#39;relu&#39;,kernel_initializer=&#39;uniform&#39;)) keras.layers.core.Dropout(0.3, noise_shape=None, seed=None) model.add(Dense(500,input_dim=1000,activation=&#39;sigmoid&#39;)) keras.layers.core.Dropout(0.4, noise_shape=None, seed=None) model.add(Dense(150,input_dim=500,activation=&#39;sigmoid&#39;)) keras.layers.core.Dropout(0.2, noise_shape=None, seed=None) model.add(Dense(units=10)) model.add(Activation(&#39;softmax&#39;)) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&quot;adam&quot;, metrics=[&#39;accuracy&#39;]) model.summary() . Train the new model . model.fit( train_x, train_y, epochs=20, batch_size=128, validation_data=(test_x,test_y), ) . Epoch 1/20 11/11 [==============================] - 2s 117ms/step - loss: 2.3253 - accuracy: 0.1903 - val_loss: 1.8503 - val_accuracy: 0.7089 Epoch 2/20 11/11 [==============================] - 1s 101ms/step - loss: 1.7412 - accuracy: 0.7974 - val_loss: 1.3985 - val_accuracy: 0.8378 Epoch 3/20 11/11 [==============================] - 1s 103ms/step - loss: 1.2646 - accuracy: 0.9024 - val_loss: 0.9943 - val_accuracy: 0.9289 Epoch 4/20 11/11 [==============================] - 1s 103ms/step - loss: 0.8349 - accuracy: 0.9636 - val_loss: 0.6862 - val_accuracy: 0.9378 Epoch 5/20 11/11 [==============================] - 1s 105ms/step - loss: 0.5149 - accuracy: 0.9812 - val_loss: 0.4834 - val_accuracy: 0.9422 Epoch 6/20 11/11 [==============================] - 1s 104ms/step - loss: 0.3083 - accuracy: 0.9904 - val_loss: 0.3589 - val_accuracy: 0.9356 Epoch 7/20 11/11 [==============================] - 1s 101ms/step - loss: 0.1978 - accuracy: 0.9961 - val_loss: 0.2829 - val_accuracy: 0.9489 Epoch 8/20 11/11 [==============================] - 1s 100ms/step - loss: 0.1226 - accuracy: 0.9994 - val_loss: 0.2266 - val_accuracy: 0.9533 Epoch 9/20 11/11 [==============================] - 1s 101ms/step - loss: 0.0802 - accuracy: 1.0000 - val_loss: 0.2048 - val_accuracy: 0.9556 Epoch 10/20 11/11 [==============================] - 1s 102ms/step - loss: 0.0559 - accuracy: 1.0000 - val_loss: 0.1785 - val_accuracy: 0.9578 Epoch 11/20 11/11 [==============================] - 1s 103ms/step - loss: 0.0417 - accuracy: 1.0000 - val_loss: 0.1694 - val_accuracy: 0.9578 Epoch 12/20 11/11 [==============================] - 1s 105ms/step - loss: 0.0323 - accuracy: 1.0000 - val_loss: 0.1577 - val_accuracy: 0.9622 Epoch 13/20 11/11 [==============================] - 1s 103ms/step - loss: 0.0264 - accuracy: 1.0000 - val_loss: 0.1556 - val_accuracy: 0.9578 Epoch 14/20 11/11 [==============================] - 1s 105ms/step - loss: 0.0216 - accuracy: 1.0000 - val_loss: 0.1488 - val_accuracy: 0.9622 Epoch 15/20 11/11 [==============================] - 1s 105ms/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.1487 - val_accuracy: 0.9600 Epoch 16/20 11/11 [==============================] - 1s 100ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 0.1480 - val_accuracy: 0.9556 Epoch 17/20 11/11 [==============================] - 1s 100ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.1480 - val_accuracy: 0.9578 Epoch 18/20 11/11 [==============================] - 1s 102ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.1465 - val_accuracy: 0.9578 Epoch 19/20 11/11 [==============================] - 1s 102ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.1461 - val_accuracy: 0.9578 Epoch 20/20 11/11 [==============================] - 1s 101ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.1449 - val_accuracy: 0.9556 . &lt;tensorflow.python.keras.callbacks.History at 0x17045cb20&gt; . predict_y = model.predict(test_x) . np.argmax(predict_y, axis=1) == labels.values[test_range] . array([[ True, False, True, ..., False, False, False], [False, True, False, ..., False, False, False], [ True, False, True, ..., False, False, False], ..., [False, False, False, ..., True, False, True], [False, False, False, ..., False, True, False], [False, False, False, ..., True, False, True]]) . Approach 2: append new layers and freeze bottom layers of VGG for training . vgg_model = VGG16(weights=&#39;imagenet&#39;, include_top=False, input_shape=(224,224,3)) # Creating dictionary that maps layer names to the layers layer_dict = dict([(layer.name, layer) for layer in vgg_model.layers]) # Getting output tensor of the last VGG layer that we want to include x = layer_dict[&#39;block5_pool&#39;].output # Adding new layers x = Flatten()(x) x = Dense(1000, input_dim=25088, activation=&#39;relu&#39;,kernel_initializer=&#39;uniform&#39;)(x) x = keras.layers.core.Dropout(0.3, noise_shape=None, seed=None)(x) x = Dense(500,input_dim=1000,activation=&#39;sigmoid&#39;)(x) x = keras.layers.core.Dropout(0.4, noise_shape=None, seed=None)(x) x = Dense(150,input_dim=500,activation=&#39;sigmoid&#39;)(x) x = keras.layers.core.Dropout(0.2, noise_shape=None, seed=None)(x) x = Dense(units=10)(x) x = Activation(&#39;softmax&#39;)(x) # Creating new model. Please note that this is NOT a Sequential() model. from keras.models import Model custom_model = Model(inputs=vgg_model.input, outputs=x) # Make sure that the pre-trained bottom layers are not trainable for layer in custom_model.layers: if layer.name in layer_dict: layer.trainable = False # Do not forget to compile it custom_model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&quot;adam&quot;, metrics=[&#39;accuracy&#39;]) . custom_model.summary() . Model: &#34;model_9&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_23 (InputLayer) [(None, 224, 224, 3)] 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, 224, 224, 64) 1792 _________________________________________________________________ block1_conv2 (Conv2D) (None, 224, 224, 64) 36928 _________________________________________________________________ block1_pool (MaxPooling2D) (None, 112, 112, 64) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, 112, 112, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, 112, 112, 128) 147584 _________________________________________________________________ block2_pool (MaxPooling2D) (None, 56, 56, 128) 0 _________________________________________________________________ block3_conv1 (Conv2D) (None, 56, 56, 256) 295168 _________________________________________________________________ block3_conv2 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________ block3_conv3 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________ block3_pool (MaxPooling2D) (None, 28, 28, 256) 0 _________________________________________________________________ block4_conv1 (Conv2D) (None, 28, 28, 512) 1180160 _________________________________________________________________ block4_conv2 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________ block4_conv3 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________ block4_pool (MaxPooling2D) (None, 14, 14, 512) 0 _________________________________________________________________ block5_conv1 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_conv2 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_conv3 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_pool (MaxPooling2D) (None, 7, 7, 512) 0 _________________________________________________________________ flatten_7 (Flatten) (None, 25088) 0 _________________________________________________________________ dense_37 (Dense) (None, 1000) 25089000 _________________________________________________________________ dropout_24 (Dropout) (None, 1000) 0 _________________________________________________________________ dense_38 (Dense) (None, 500) 500500 _________________________________________________________________ dropout_25 (Dropout) (None, 500) 0 _________________________________________________________________ dense_39 (Dense) (None, 150) 75150 _________________________________________________________________ dropout_26 (Dropout) (None, 150) 0 _________________________________________________________________ dense_40 (Dense) (None, 10) 1510 _________________________________________________________________ activation_8 (Activation) (None, 10) 0 ================================================================= Total params: 40,380,848 Trainable params: 25,666,160 Non-trainable params: 14,714,688 _________________________________________________________________ . Train the new model . custom_model.fit( train_img, train_y, epochs=20, batch_size=128, validation_data=(test_img,test_y), ) . Epoch 1/20 11/11 [==============================] - 122s 11s/step - loss: 2.4470 - accuracy: 0.1130 - val_loss: 2.1486 - val_accuracy: 0.2711 Epoch 2/20 11/11 [==============================] - 136s 13s/step - loss: 2.2192 - accuracy: 0.1839 - val_loss: 1.9199 - val_accuracy: 0.6311 Epoch 3/20 11/11 [==============================] - 129s 12s/step - loss: 1.9635 - accuracy: 0.3612 - val_loss: 1.4907 - val_accuracy: 0.7600 Epoch 4/20 11/11 [==============================] - 128s 12s/step - loss: 1.5639 - accuracy: 0.6354 - val_loss: 1.0121 - val_accuracy: 0.8244 Epoch 5/20 11/11 [==============================] - 128s 12s/step - loss: 1.0947 - accuracy: 0.7856 - val_loss: 0.6528 - val_accuracy: 0.8978 Epoch 6/20 11/11 [==============================] - 618s 61s/step - loss: 0.7248 - accuracy: 0.8800 - val_loss: 0.4142 - val_accuracy: 0.9244 Epoch 7/20 11/11 [==============================] - 124s 12s/step - loss: 0.4343 - accuracy: 0.9431 - val_loss: 0.3164 - val_accuracy: 0.9244 Epoch 8/20 11/11 [==============================] - 127s 12s/step - loss: 0.2650 - accuracy: 0.9688 - val_loss: 0.2276 - val_accuracy: 0.9467 Epoch 9/20 11/11 [==============================] - 132s 12s/step - loss: 0.1566 - accuracy: 0.9865 - val_loss: 0.1752 - val_accuracy: 0.9600 Epoch 10/20 11/11 [==============================] - 146s 13s/step - loss: 0.1018 - accuracy: 0.9874 - val_loss: 0.2028 - val_accuracy: 0.9511 Epoch 11/20 11/11 [==============================] - 138s 13s/step - loss: 0.0771 - accuracy: 0.9946 - val_loss: 0.1748 - val_accuracy: 0.9511 Epoch 12/20 11/11 [==============================] - 137s 13s/step - loss: 0.0557 - accuracy: 0.9978 - val_loss: 0.1440 - val_accuracy: 0.9600 Epoch 13/20 11/11 [==============================] - 137s 13s/step - loss: 0.0375 - accuracy: 1.0000 - val_loss: 0.1549 - val_accuracy: 0.9533 Epoch 14/20 11/11 [==============================] - 132s 12s/step - loss: 0.0300 - accuracy: 0.9993 - val_loss: 0.1574 - val_accuracy: 0.9578 Epoch 15/20 11/11 [==============================] - 132s 12s/step - loss: 0.0254 - accuracy: 1.0000 - val_loss: 0.1468 - val_accuracy: 0.9511 Epoch 16/20 11/11 [==============================] - 132s 12s/step - loss: 0.0222 - accuracy: 1.0000 - val_loss: 0.1399 - val_accuracy: 0.9578 Epoch 17/20 11/11 [==============================] - 133s 12s/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 0.1383 - val_accuracy: 0.9600 Epoch 18/20 11/11 [==============================] - 133s 12s/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.1463 - val_accuracy: 0.9489 Epoch 19/20 11/11 [==============================] - 131s 12s/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.1487 - val_accuracy: 0.9578 Epoch 20/20 11/11 [==============================] - 126s 12s/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.1502 - val_accuracy: 0.9556 . &lt;tensorflow.python.keras.callbacks.History at 0x1741d5160&gt; . custom_model.save(&quot;mnist/custom_model.h5&quot;, save_format=&quot;h5&quot;) . custom_model = keras.models.load_model(&quot;mnist/custom_model.h5&quot;) . import matplotlib.pyplot as plt examples = [4, 150, 1500, 1689] for ex in examples: im = image.load_img( Path.joinpath(img_path, f&quot;{ex}.png&quot;).as_posix(), target_size=(224,224) ) plt.figure() plt.imshow(im) plt.show() y = custom_model.predict(np.array([image.img_to_array(im)])) print(np.argmax(y)) print(&quot;&quot;) . 4 . 0 . 1 . 2 .",
            "url": "https://ycouble.github.io/til/en/ml/raw/2021/02/17/transfer_learning.html",
            "relUrl": "/en/ml/raw/2021/02/17/transfer_learning.html",
            "date": " • Feb 17, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Interpretability (Part 3)",
            "content": "Some quotations . A change in a feature by one unit changes the odds ratio (multiplicative) by a factor of exp(βj). We could also interpret it this way: A change in xj by one unit increases the log odds ratio by the value of the corresponding weight. . These are the interpretations for the logistic regression model with different feature types: . Numerical feature: If you increase the value of feature xj by one unit, the estimated odds change by a factor of exp(βj) | Binary categorical feature: One of the two values of the feature is the reference category (in some languages, the one encoded in 0). Changing the feature xj from the reference category to the other category changes the estimated odds by a factor of exp(βj). | Categorical feature with more than two categories: One solution to deal with multiple categories is one-hot-encoding, meaning that each category has its own column. You only need L-1 columns for a categorical feature with L categories, otherwise it is over-parameterized. The L-th category is then the reference category. You can use any other encoding that can be used in linear regression. The interpretation for each category then is equivalent to the interpretation of binary features. | Intercept β0: When all numerical features are zero and the categorical features are at the reference category, the estimated odds are exp(β0). The interpretation of the intercept weight is usually not relevant. | . Another disadvantage of the logistic regression model is that the interpretation is more difficult because the interpretation of the weights is multiplicative and not additive. . On the good side, the logistic regression model is not only a classification model, but also gives you probabilities. This is a big advantage over models that can only provide the final classification. Knowing that an instance has a 99% probability for a class compared to 51% makes a big difference. .",
            "url": "https://ycouble.github.io/til/xai/en/ml/2020/12/04/interpretability_chapter12_interpretable_models_logistic_regression.html",
            "relUrl": "/xai/en/ml/2020/12/04/interpretability_chapter12_interpretable_models_logistic_regression.html",
            "date": " • Dec 4, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Interpretability (Part 2)",
            "content": "Interpretable Models - Linear Regression . Linear regressions are simple models, which force the output to be a linear combination of the inputs. This means that the model is additive and thus easily explainable. The weights are the explaination. . Assumptions used to compute the confidence intervals: . Linearity: forced by linear regression | Normality: the probability of the target outcome given the feature should follow a normal distribution. | Homoscedasticity: (constant variance) The variance of the error terms is assumed to be constant, which is generally not verified (variance typically often increase for large values) | Independence: assumption that each instance in independent of any other, often not verified when you have several repeated measurements. If this is not the cas you need to have specific linear regression models such as mixed effect models of GEE (//TODO). | Fixed features: inputs are considered exact and without measurement errors (always wrong, but it would be highly impractical otherwise) | Absence of multicollinearity: When two features are strongly correlated, it blurrs the importance of the two (weights could go either way, and the model would be as efficient with only one of them) | . Interpretation . Interpretation depends on the type of feature: . numerical: increase in feature –&gt; increase * weight on outcome | binary/categorical: presence/absence/selection –&gt; weight on outcome | intercept: if features are normalized and bin/cat 0 = reference –&gt; outcome of all feature at their mean | . R-squared . Another important measurement is the R-squared, which tells how much of the total variance of the target outcome is explained by the model. . Higher R-squarred is better. | R² = 1 - square sum of errors / square sum of data variance. | R² increases with the number of features, so it is better to use the adjusted R² = 1 - (1 - R²) (n-1)/(n-p-1) where p is the number of features and n the number of instances | low adjusted R² –&gt; not interpretable because it does not explain much of the variance. | . Feature importance . Importance of a feature in LR can be measured by the absolute value of its T-statistic, which is the estimated weight scaled with its standard error. (standard error = standard deviation of the outcome in a outcome = intercept + feature_weight * feature_value function) . –&gt; possible to plot for each feature (like a facet plot) the y = i + w*x curve, with standard error shown, which highlights the distribution of the ground truths around the predictions. . Visual interpretation . Weight plots (https://christophm.github.io/interpretable-ml-book/limo.html#weight-plot) . Weight plots show for each feature the weight estimate and the standard error. . Low SE represents a reliable feature | High weight estimate means a high influence in the outcome | Scaling makes the estimate weights more comparable | . Effect plots . Box plots for each feature. Only effects are represented. Effect = weight * value. . vertical line = effect of the median | box = 25% and 75% quantiles effect | horizontal line = span +- 1.5 InnerQuartileRange | dots = outliers | . Explain individual predictions . Position the individual feature effects on an Effect plot: it enables to see how and why the outcome was decided (in particular, outlier effects are interesting) | . Encoding Categorical Features . Two encoding presented: . Treatment coding: N - 1 features for N categories, 1 hot encoding | Effect coding: Compare each category to the mean and use this value for encoding (only N-1 categories encoded) | . Do Linear models create good explanations ? . “linear models do not create the best explanations” . Contrastive, but the reference instance is a data point where all numerical are 0 and categorical are at their reference category (usually meaningless). If all features are mean centered, and cat features are effect coded, then the reference instance is the point where all features are at the mean. | Selectivity can be achieved by LR using less features or training sparse linear models, but by default, explainations are not selective. | Truthfulness: yes as long as the Linear Model is appropriate (aR² high). | Linearity makes the explanation mode general and simple | . Sparse Linear Models . Regularization makes the model more frugal. . By adding a lambda * norm of the weights, in the minimization term, it penalizes models that have a lot of non null weights. The higher the lambda, the less feature are going into the model. | Usually, lambda is tuned during cross-validation | | .",
            "url": "https://ycouble.github.io/til/xai/en/ml/2020/12/02/interpretability_chapter11_interpretable_models_linear_reg.html",
            "relUrl": "/xai/en/ml/2020/12/02/interpretability_chapter11_interpretable_models_linear_reg.html",
            "date": " • Dec 2, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Interpretability (Part 1)",
            "content": "Interpretable Machine Learning . Some notes on the small book Interpretable Machine Learning by Christoph Molnar. . Introduction and overview . Christoph Molnar overviews in his book the concept of model interpretability, and surveys several ways to help ML engineers design more interpretable machine learning model, i.e. enable humans to understand the reasons why the ML model outputs what it outputs instead of any the other possible answers. The motivation for model interpretability is qui immediate, as a matter of fact it is often observed that results from a recommender system such as YouTube video recommendations are somewhat obsucre, and the more importance machine learning algorithms take into our lives, the more important it will be that we are able to understand the “whys” of the ML models decisions. . Efforts are being made by many of the major actors, as for example Amazon’s recommendation systems providing a set of articles that are commonly bought together, or the “customers that bought what you just bought also often purchased this other article” and such. However, it is far from being widespread, and it also has an important impact for many many companies or labs that usually focus on ML model performance and trust almost blindly the output of their model. Note that this is not only applicable to ML, it is also often the case for sufficiently complicated models and algorithms such are optimization (among which discrete optimization is probably a great candidate), where we are unable to track or get an intuition of why the result is such and not something completely different. . The consequences of bad model interpretability are numerous, but I see one very important point, which is functional model debugging failures. If you trust you model/algorithm blindly, it then becomes very hard to see if the model behaves as it should. For example, imagine that you have a discrete optimization problem, which is NP-hard and that you solve using a custom algorithm. Since the problem is too complex, it is nearly impossible (or too long) to know the optimal solution for a large instance, and you are left with trusting your algorithm or only confronting your algorithm to the known optimal on very small instances that may not be representative of larger instances, and certainly not a proof. In this kind of situations, it becomes very important to have some other ways of checking the result of your algorithm, its consistency, why the result on such instance is much lower than on other similar instances, etc. . The same applies to machine learning, with an even more critical factor which is that ML models are much more obscure on their internal mechanisms. Indeed, unlike instruction-based algorithms which are easily probed and for which each step bears meaning, machine learning models internal steps bear no meaning and are only the result of the model training on data. . Model interpretability . What the author defines as interpretability is the capacity of a model/solution to justify its output. First he mentions that there are models that are intrinsincly interpretables sur as linear regressions, decisions trees and such where the internal parameter values already offers a sufficiently good explanation of the output. For example, for a linear regression, the weights of each feature will tell us how important are each feature compared to the others. . But for more sophisticated model, interpretability is obtained through more indirect or approximation methods, among which: . Partial dependance plots (WIP) | Individual Conditional Expectations (WIP) | Accumulated Local Effects (WIP) | Feature Interaction (WIP) | Feature Importance (WIP) | Global Surrogate, which consist of approximating the output of the model by a simple, intepretable machine learning model. | Local Surrogate, which is the same as global but only for the neighborhood of an instance. | Shapley Values, inspired from game-theory where the relative importance of each feature in the “coalition” is derived from similar instances in the dataset. | . Interpretability evaluation . Interpretability is not easily evaluated since there is no quantification of interpretability and since it depends largely on the social and technological context of the recipient of explanations. Obviously the explaination of the model output will not be the same for an field expert and for the end customer of a recommender system. . Several criteria of evaluation are underline in the book, first as general criteria for an explanation method, then for individual explanation produced by such methods: . Explanation power: how expressive the provided explanations are (do they use natural language, if-then-else type conditions…) | Translucency: how related to the model internal is the explanation (for example an explanation consisting in the weights for a linear model is a very translucent explanation) | Portability: how tied to a model is the explanation, i.e. can it be ported to other models without modifications (in the last example, the weight is not a portable explanation) | Algorithmic complexity | . Individual explanations: . Accuracy: how fit to the data is the explanation | Fidelity: how fit to the model output is the explanation | Consistency: how robust to a model change is the explanation (closely related to the method portability) | Stability (Always desirable): how robust to some small perturbations is the explanation | Comprehensibility | Certainty: how sure / confident in the explanation | Degree of Importance: feature importance for explaining the data | Novelty of the data: how exceptional is an instance compared to the known dataset | Representativeness: range covered by the explanation | . Human friendly explanations . For a model to be easily intepretable by humans the author argues that the explanations should follow several rules: . Explanations have to be selected: a good practice is to refrain from providing every possible bit of explainatory data and to limit the explanation to the 1-3 most important pieces of explanations. | Explanations should be constrastive: usually humans prefer explanations that compare the instance with similar one that have a different ouput: “why is this instance so special that it doesn’t output the same result as intuitively similar instances ?”. | Explanations should be targeted according to the target of the explanation | Explanations should focus on abnormal data whenever possible: the most important piece of explanation is not always the most numerically significant but maybe the one which bears the highest abnormality. | Explanations should be as truthful as possible, i.e. it should remain applicable on other instances and not be invalidated | If no abnormality can be found, a general and probable explanation is also preferable to many pieces of unexceptional explanations. | .",
            "url": "https://ycouble.github.io/til/xai/en/ml/2020/11/20/interpretability_chapter0.html",
            "relUrl": "/xai/en/ml/2020/11/20/interpretability_chapter0.html",
            "date": " • Nov 20, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "So you want to be a Python Expert",
            "content": "So you want to be a Python expert ? . Talk by James Powell: https://www.youtube.com/watch?v=cKPlPJyQrt4 . Topics . Python data-model | Decorators | Generators | Meta classes (not retranscripted here, he basically presented the mechanisms behind the ABC @abstractclass decorator) | Context Managers | . Python data-model . Python is entirely inspectable and has a very linear execution pattern (everything is read from top to bottom). The language makes it very easy to define operations, and to define different behaviours for any object. . class Summer: def __init__(self, *args): self.coefs = list(args) # It is possible to define a __call__ function so that the object is callable def __call__(self): return sum(self.coefs) # To be able to use built-in operators you may define these operation # with the corresponding &quot;dunder&quot; (double under) method def __add__(self, other): return Summer(*(self.coefs + other.coefs)) def __repr__(self): return (f&quot;Summer({self.coefs})&quot;) . a = Summer(1,4,5,3,10) print(a, a(), a+a, (a+a)()) . Summer([1, 4, 5, 3, 10]) 23 Summer([1, 4, 5, 3, 10, 1, 4, 5, 3, 10]) 46 . Therefore, there is very limited difference between an empty object with a __call__ method and a function. . class Adder: def __call__(self, x, y): return x + y add_obj = Adder() # Function def add(x, y): return x + y . print(add_obj, add) print(add_obj(10, 20), add(10, 20)) . &lt;__main__.Adder object at 0x104a6f5f8&gt; &lt;function add at 0x104bf12f0&gt; 30 30 . Decorators . Decorators are a syntaxic sugar of Python to define a function to wrap around any kind of function. Their main goal is to factor code and make it easier to maintain wrapping functions . def add(x, y): return x + y def sub(x, y): return x - y def mult(x, y): return x * y print(add(10, 20)) print(sub(10, 20)) print(mult(10, 20)) . 30 -10 200 . For example if you want to debug this simple code, by printing the inputs and their types before executing the function, you could put the code in each function like this: . def add(x, y): print(x, y) return x + y def sub(x, y): print(x, y) return x - y def mult(x, y): print(x, y) return x * y print(add(10, 20)) print(sub(10, 20)) print(mult(10, 20)) . 10 20 30 10 20 -10 10 20 200 . Or, you can define a function that, given a function, prints the inputs before returning the result of the function . def printer(func): def wrapper(*args): print(*args) return func(*args) return wrapper def add(x, y): return x + y add = printer(add) def sub(x, y): return x - y sub = printer(sub) def mult(x, y): return x * y mult = printer(mult) print(add(10, 20)) print(sub(10, 20)) print(mult(10, 20)) . 10 20 30 10 20 -10 10 20 200 . And then, Python provides a syntactic sugar that does exactly these add = printer(add), but more beautifully: . def printer(func): def wrapper(*args): print(*args) return func(*args) return wrapper @printer def add(x, y): return x + y @printer def sub(x, y): return x - y @printer def mult(x, y): return x * y print(add(10, 20)) print(sub(10, 20)) print(mult(10, 20)) . 10 20 30 10 20 -10 10 20 200 . Generators . Generators are essentially functions that can give the hand back to the caller in the middle of their execution. It is a way to introduce lib-level to user-level interaction. . def one_then_two(): print(&quot;first step&quot;) yield 1 print(&quot;second step&quot;) yield 2 print(&quot;third step: None&quot;) print(one_then_two) . &lt;function one_then_two at 0x104bfd378&gt; . a = one_then_two() b = next(a) print(b) b = next(a) print(b) next(a) . first step 1 second step 2 third step: None . StopIteration Traceback (most recent call last) &lt;ipython-input-10-1e40d6f632f3&gt; in &lt;module&gt;() 4 b = next(a) 5 print(b) -&gt; 6 next(a) StopIteration: . When there is no more yield statement, the generator raises a StopIteration exception to indicate it. . Generators enables bidirectional interaction with the send built-in function . a.send? . Docstring: send(arg) -&gt; send &#39;arg&#39; into generator, return next yielded value or raise StopIteration. Type: builtin_function_or_method . def lib(word): message = &quot;&quot; for _ in range(4): message = yield (message + &quot; &quot; + word) print(&quot;lib&quot;, message) def user(lib, word): message = None try: for _ in range(10): message = lib.send(message) + &quot; &quot; + word print(&quot;user&quot;, message) except StopIteration: print(&quot;the end&quot;) . ping = lib(&quot;ping&quot;) pong = user(ping, &quot;pong&quot;) . user ping pong lib ping pong user ping pong ping pong lib ping pong ping pong user ping pong ping pong ping pong lib ping pong ping pong ping pong user ping pong ping pong ping pong ping pong lib ping pong ping pong ping pong ping pong the end . Generators are also used for lazy computing: Iterating over a list of items without computing all of their values at once. It is especially useful when you may not need all of the values. . def integers(): i = 0 while True: yield i i += 1 for i in integers(): print(i) if i &gt;= 5: break . 0 1 2 3 4 5 . At all points in the above code, only one integer was kept in memory, compared with a huge (inifite) list of all integers. . Context Managers . Context managers are functions that help manage before and after steps, like opening a connection to a db, opening a file, creating a table and dropping it after etc. . def prepare(d): d[&quot;list&quot;] = [] d[&quot;dict&quot;] = {} d[&quot;description&quot;] = &quot;&quot; def destroy(d): d.pop(&quot;list&quot;) d.pop(&quot;dict&quot;) d.pop(&quot;description&quot;) def some_actions(d): d[&quot;list&quot;].append(1) d[&quot;list&quot;].append(2) d[&quot;dict&quot;][&quot;hello&quot;] = 100 d[&quot;description&quot;] = &quot;arbitrarily filled dict&quot; d = dict(a=19) prepare(d) print(d) some_actions(d) print(d) destroy(d) print(d) . {&#39;a&#39;: 19, &#39;list&#39;: [], &#39;dict&#39;: {}, &#39;description&#39;: &#39;&#39;} {&#39;a&#39;: 19, &#39;list&#39;: [1, 2], &#39;dict&#39;: {&#39;hello&#39;: 100}, &#39;description&#39;: &#39;arbitrarily filled dict&#39;} {&#39;a&#39;: 19} . To avoid doing the prepare/destroy each time you want to use the function some_action, it is possible to use a context manager, which are designed exactly for that. . A context manager is essentially an object with an __enter__ and a __exit__ method, it is called with the withstatement. . class DictManager: def __init__(self, d): self._dict = d def __enter__(self): self._dict[&quot;list&quot;] = [] self._dict[&quot;dict&quot;] = {} self._dict[&quot;description&quot;] = &quot;&quot; def __exit__(self, *args): self._dict.pop(&quot;list&quot;) self._dict.pop(&quot;dict&quot;) self._dict.pop(&quot;description&quot;) d = dict(a=19) print(d) with DictManager(d): print(d) some_actions(d) print(d) print(d) . {&#39;a&#39;: 19} {&#39;a&#39;: 19, &#39;list&#39;: [], &#39;dict&#39;: {}, &#39;description&#39;: &#39;&#39;} {&#39;a&#39;: 19, &#39;list&#39;: [1, 2], &#39;dict&#39;: {&#39;hello&#39;: 100}, &#39;description&#39;: &#39;arbitrarily filled dict&#39;} {&#39;a&#39;: 19} . And since __enter__ and __exit__ are called in order, there can be a generator that creates the sequence: . class DictManager: def __init__(self, gen): self.gen = gen def __call__(self, *args, **kwargs): self.args, self.kwargs = args, kwargs return self def __enter__(self): self.gen_inst = self.gen(*self.args, **self.kwargs) next(self.gen_inst) def __exit__(self, *args): next(self.gen_inst, None) def tempdict(d): d[&quot;list&quot;] = [] d[&quot;dict&quot;] = {} d[&quot;description&quot;] = &quot;&quot; print(&quot;init d&quot;) yield d.pop(&quot;list&quot;) d.pop(&quot;dict&quot;) d.pop(&quot;description&quot;) print(&quot;restored d&quot;) tempdict = DictManager(tempdict) d = dict(a=19) print(d) with tempdict(d): print(d) some_actions(d) print(d) print(d) . {&#39;a&#39;: 19} init d {&#39;a&#39;: 19, &#39;list&#39;: [], &#39;dict&#39;: {}, &#39;description&#39;: &#39;&#39;} {&#39;a&#39;: 19, &#39;list&#39;: [1, 2], &#39;dict&#39;: {&#39;hello&#39;: 100}, &#39;description&#39;: &#39;arbitrarily filled dict&#39;} restored d {&#39;a&#39;: 19} . And that is basically what a context manager is. There is a predefined contextmanager decorator which does this for us: . from contextlib import contextmanager @contextmanager def tempdict(d): d[&quot;list&quot;] = [] d[&quot;dict&quot;] = {} d[&quot;description&quot;] = &quot;&quot; print(&quot;init d&quot;) try: yield finally: d.pop(&quot;list&quot;) d.pop(&quot;dict&quot;) d.pop(&quot;description&quot;) print(&quot;restored d&quot;) d = dict(a=19) print(d) with tempdict(d): print(d) some_actions(d) print(d) print(d) . {&#39;a&#39;: 19} init d {&#39;a&#39;: 19, &#39;list&#39;: [], &#39;dict&#39;: {}, &#39;description&#39;: &#39;&#39;} {&#39;a&#39;: 19, &#39;list&#39;: [1, 2], &#39;dict&#39;: {&#39;hello&#39;: 100}, &#39;description&#39;: &#39;arbitrarily filled dict&#39;} restored d {&#39;a&#39;: 19} . So, in the end, all you have to do to create a proper context manager is to import the contextmanager decorator from contextlib, and put it on a function that calls yield once, surrounded preferably by a try/finally statement to always perfome the closing instructions .",
            "url": "https://ycouble.github.io/til/en/python/raw/2020/11/17/Powell-So_you_want_to_be_a_python_expert.html",
            "relUrl": "/en/python/raw/2020/11/17/Powell-So_you_want_to_be_a_python_expert.html",
            "date": " • Nov 17, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Hadoop",
            "content": "Hadoop Platform and Application Framework . Lesson 1 . Hadoop Stack: [ Clients ] &gt; [ MapReduce ] &gt; [ YARN ] &gt; [ HDFS ] . Hadoop File System: Distributed, scalable and portable file-system written in Java for the Hadoop framework replicates accross several hosts | system is composed of Namenode(s) which keep some metadata on the contained folders (e.g. name, number of replica…) and Datanodes which contain the (replicated) data blocks. | secondary namenode: scans and builds snapshots of the primary namenode (raptures information, location etc.) | A Hadoop based system always sits on some version of a MapReduce engine: | . Job/Task trackers: job tracker on the namenode (client’s job tracking) and task tracker on the datanodes (operation tracking) | MapReduceV2 -&gt; YARN (Hadoop 2.0): Separates the research management and process component (generalization of the hadoop architecture to other processing than mapreduce) | Before YARN, the hdfs stack was [ MapReduce ] &gt; [ HDFS ], now it is possible to have others data processing: [ Map Reduce | Others ] &gt; [ YARN ] &gt; [ HDFS ] - Yarns = scheduling, MapReduce (in V2) = data processing | . | The Hadoop Zoo Started from the Google FS, and incrementally added functionalities (SQL like queries, BigTable, Sawzall, …) -&gt; variations accross big tech companies, but with the same global architecture: (cloudera’s implem) [ UI Framework (hue) | SDK (hue) ] [ Workflow mgmt (oozie) | Scheduling (oozie) | Metadata (Hive) ] [ Data Integration (flume, sqoop) | [ Languages, compilers (pig/hive) ] &gt; [ Hadoop ] | Fast read/write access (hbase) ] [ Coordination (zookeeper) ] . | . | Hadoop Ecosystem Major Components PIG: | . High level programming on to for Hadoop MapReduce | Multiple languages: JPython, Java … | Data analysis problems as data flows | Pig for ETL: inport, extract, transform, write back on the hdfs [Q: difference with Beam ?] - Hive: | Facilitates queriying and managing large datasets in distributed storage | Hive QL - Oozie: | Workflow scheduler to manage Hadoop jobs | Coordinator jobs | Supports: MapReduce, Pig, Hive, Sqoop… - Zookeeper: | Provides centralized, AOM and synchronization - Flume: | Distributed, reliable and available service for collecting, aggregating and moving large amount of log data - Many others (Impala, Cloudera search, Spark, Majout, …) - Spark: | Parallel, in-memory, large scale data processing | . |",
            "url": "https://ycouble.github.io/til/en/bigdata/2020/11/02/hadoop-course-notes.html",
            "relUrl": "/en/bigdata/2020/11/02/hadoop-course-notes.html",
            "date": " • Nov 2, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Multi-spot beam satellites",
            "content": "Edit(2017/09): this article was written early in my PhD, with little knowledge of actual systems. It happens that the Uplink/Downlink frequency bands are not so linked together. I might update this article later, or publish a new one with more accurate assumptions. . . Let ( Delta F_{FL}), ( Delta F_{DL}) and ( Delta F_{UL}) be the Frequency bands for, respectively, the whole forward link (i.e. Gateway → Satellite → Users), downlink (Satellite → Users) and uplink (Gateway → Satellite). . In the simple scheme where a satellite covers a full continent with a single beam (traditional TV Broadcast model) we have this relation: . ΔF_DL=ΔF_UL=12ΔF_FL Delta F _{DL} = Delta F _{UL} = frac{1}{2} Delta F _{FL}ΔF_DL=ΔF_UL=21​ΔF_FL . Multi-spot beam satellites allow to have different coverage beam, where the same frequencies can be used and not interfere with each other (the beams are sufficiently well focused and spaced). . . In this case, a frequency can be used (k) times, where k is the frequency reuse factor. . For example, in a 4-color distribution scheme, with 120 spots, there is a theoretical reuse factor of 20. In reality, the reuse factor is closer to 12 than 20, but that’s another topic. . In the case where the reuse factor is (k=2) (meaning that a frequency is used twice over the whole coverage), taking the hypothesis of full use of the bandwidth over each spot beam, the necessary bandwidth in the uplink would be: ΔF_UL=2ΔF_DL Delta F _{UL} = 2 Delta F _{DL}ΔF_UL=2ΔF_DL . Extending this to (k), we have: ΔF_UL=kΔF_DL Delta F _{UL} = k Delta F _{DL}ΔF_UL=kΔF_DL . Provided that the global bandwidth for this forward link is constant ( Delta F_{FL}=cst), we have the following relation of the uplink bandwidth according to the reuse factor: ΔF_UL=k(ΔF_FL−ΔF_UL) Delta F _{UL} = k( Delta F _{FL} - Delta F _{UL})ΔF_UL=k(ΔF_FL−ΔF_UL) ΔF_UL=kk+1ΔF_FL Delta F _{UL} = frac{k}{k+1} Delta F _{FL}ΔF_UL=k+1k​ΔF_FL . ] . . Sources: . Jean-Baptiste Dupé Thesis | Second Generation DVB Interactive Satellite System (DVB-RCS2); Part 2: Lower Layers for Satellite standard |",
            "url": "https://ycouble.github.io/til/satellite/2015/10/29/mbsat.html",
            "relUrl": "/satellite/2015/10/29/mbsat.html",
            "date": " • Oct 29, 2015"
        }
        
    
  
    
        ,"post12": {
            "title": "MF-TDMA Applied to DVB-RCS2",
            "content": "Traditionally in video broadcasting services, there is one entity broadcasting its content over different frequencies. That is a classical bandwidth sharing technique called Frequency Division Multiplexing (FDM). Each flow is sent over a specific frequency dedicated to it so that it doesn’t interfere with other users. . In digital telephony (e.g. ISDN), a different approach is used: users transmit only during short time slots, periodically and on the same frequency. This is call Time Division Multiplexing (TDM). The timeslots are assigned statically to users, and hence unused when the user doesn’t use it. . But when the potential user number overpasses the capacity, it is necessary to use an access method to dynamically assign resources. FDM and TDM have their corresponding access methods with Frequency Division Multiple Access (FDMA) and Time Division Multiple Access (TDMA) techniques. Instead of assigning statically the frequency/time resources, they just define an Frequency/Time division plan that will be use by the MAC layer as allocable resource. FDMA was used in mobile telephony and some satellite systems, TDMA is used in GSM and some optical networks. . MF-TDMA is a combination of FDMA and TDMA: multiple frequency channels are available, and each is divided in timeslots. Users are provided dynamically one or more slots of time/frequency for their traffic. . The DVB-RCS2 standard uses MF-TDMA. In the following we’ll try to explain how this standard organises all (frequency, time) couples that are available. . In this article, we’ll focus on a specific case of the standard, with linear modulation and burst transmission. . . DVB-RCS2 Waveforms . DVB-RCS2 defines several waveforms to be used. Some (up to 128) are predefined by the standard, and others can be defined by the system user (up to 128). Each waveform defined for a system shall be provided in advance to each RCST. A waveform is defined by a few parameters such as the modulation scheme, coding rate, burst length in symbols, pilot scheme etc. . Below are a few examples of waveforms. Many waveforms use the same symbol length, so that ACM can be done easily by interchanging the waveforms. . . MF-TDMA entities . Bandwidth-Time Unit (BTU) . The smallest unit in MF-TDMA is the BTU. A BTU is a slot, with defined duration, carrier bandwidth and symbol rate. . Hence, only a limited number of waveforms can fit to a BTU type, given its duration and symbol rate. . Timeslot . A timeslot is a sequence of BTU of the same type over a single carrier frequency. Hence, the duration of a timeslot is a multiple of the BTU duration, and the bandwidth is the same as the bandwidth of a BTU. . There are different types of timeslots, depending on how many BTU they cover, or their function. As far as I know, there are 5 types of timeslots: . Type G (Generic): they are 1 BTU large and can be aggregated together just in time (right before being transmitted, according to the need) to form other types of timeslots. | Type CB (Control Burst): like the type G BTUs, they are one BTU large, but they are used for signalling only. | Type LB (Logon Burst): this timeslot type is specific and uses 3 BTU, used for logon. | Type TRF1 (Traffic 1): they are 2 BTU large and used for user traffic. | Type TRF2 (Traffic 2): they are 6 BTU large and used for user traffic. | . Frame . A frame is an combination of timeslots. It is possible to combine different types of timeslots in the same frame, as long as all the timeslot types use the same type of BTU. . It is possible to define several frame types, according to the need of the implementer. . Below are 3 examples of frame templates, without the repartition of timeslots. . . A frame is the combination of one of the above “templates” and a timeslot scheme, as shown on the following figure extracted from the standard. . . Superframe . A super frame is an aggregation of one or more frames. Frames do not have to be contiguous (time and frequency wise). A super frame is identified through its superframe count. A superframe total duration is generally between 25ms and 750ms (even though it can be higher in some implementations.). . Frames inside a superframe can be of various types, as long as their common carrier frequency bandwidths are the same. An example of superframe is shown on the simple example below. . . A group of super frames of the same type is called a SuperFrame Sequence (SFS). . Conclusion . In this article we presented the MF-TDMA access method used in the case of linear modulation with bursted transmission for the Return Link of DVB-RCS2 standard. . We overviewed the decomposition of the bandwidth, broadcasted to every user connected to the terminal periodically, along with the dynamic allocation scheme. . This is done to allow thousands of users to access the channel without interfering with each other, and it works ! . . Sources: . Jean-Baptiste Dupé Thesis | Second Generation DVB Interactive Satellite System (DVB-RCS2); Part 2: Lower Layers for Satellite standard |",
            "url": "https://ycouble.github.io/til/satellite/2015/10/28/mftdma.html",
            "relUrl": "/satellite/2015/10/28/mftdma.html",
            "date": " • Oct 28, 2015"
        }
        
    
  
    
        ,"post13": {
            "title": "Crowdfunding Space Exploration",
            "content": "The Planetary Society recently successfully raised funds through the crowdfunding site Kickstarter. With an initial goal of $200.000, they managed to raise a crazy amount of money, $1.241.615! With more than 23.000 people contributing to the project. This is an amazing amount of people who contributed to a science project. . Led by Bill Nye -a famous US science popularizer and anchorman-, the LightSail project is planning on sending a second cubesat in space to demonstrate solar energy propulsion. The money raised will be used to support the spacecraft construction, its validation and integration, operations and eventually gathering and analyzing the data collected by the spacecraft. . With the emergence of cubesats and the success of crowdfunding it is very likely that we see a bunch of similar projects poping up on the internet. Here are a few examples of past projects regarding cubesats. Only a few achieved their target funding but the exposure given by the LightSail project may encourage more to take the plunge. . LightSail: A Revolutionary Solar Sailing Spacecraft (Funded, raised $1.240.000 from 23.000 backers) | LUNARSAIL: An Open-Source Cubesat &amp; Solar Sail Lunar Orbiter (Funded, raised $16.000 from 260 backers) | CAT: A Thruster for Interplanetary CubeSats (Failed to reach full funding, but raised $68.000 from 1.250 backers). | . After the partial failure of the fundrising, the project creator made another call for funds, with a revised budget : CAT: Launch a Water-Propelled Satellite into Deep Space (Funded, raised $97.000 from 1.200 backers). . I couldn’t find any other crowdfunding website supporting this kind of bold projects, but maybe it will soon come! (Ulule, DO SOMETHING!!!) . . Sources: . Kickstarter website | Light Sail website |",
            "url": "https://ycouble.github.io/til/satellite/2015/07/05/crowdfunding-space.html",
            "relUrl": "/satellite/2015/07/05/crowdfunding-space.html",
            "date": " • Jul 5, 2015"
        }
        
    
  
    
        ,"post14": {
            "title": "CubeSats",
            "content": "This article gathers a few information on cubesats. This is extracted from diverse wikis, press articles, papers, websites etc. . Launching a satellite into orbit is, as can be expected, very costly. So when a company decides to launch a satellite, they generally prefer to use well known, proven and often deprecated technologies. . This was true and accepted until the end of the 90’s when California Polytechnic State university (CalPoly) developped a system specification to send normalized nanosatellites into space carrying experimental payloads: Cubesats. We’ll cover here a few aspects of these cubesats, from their specification and launch modalities to their usefulness for telecom applications. . A framework for space experimentations . Figure 1: Artistic view of a cubesat (AAU CubeSat) deployed in orbit [JPG] . A CubeSat is basically a simple 1.5kg 10cm cubed container normalized and validated for space conditions. . They are launched into space aboard another full satellite launch from the ISS. Once brought onto their final orbit, they are thrown using a Picosatellite Orbital Deployer (basically just a triggered spring and a guide) called P-POD in the case of CalPoly cubesats, a P-POD can carry up to 3 1U cubesats. . Other nanosatellites and their respective POD have been designed, with their own respective names. You may stumble on X-POD, J-POD in some articles, they are different mainly because of the size of nanosatellite they allow to deploy. . Figure 2: The 1U cubesat specification figure from the cubesat spec rev13 [PNG] . The 10 cm cubed version is the 1U cubesat, as shown on Fig. 2. Other versions of cubesats have been normalized: 1.5U, 2U, 3U, 3U+ which are just the linear stacking of 1U cubesats. 4U and 6U are also possible, but they use a modified version of the P-POD. . Figure 3: The PPOD, extracted from the cubesat spec rev13 [PNG] . Launching a CubeSat . A cubesat ride, as is often called a launch aboard a standard heavy rocket along with a full-size satellite, costs around 100k$ for the 1U version which is nothing compared to the tens of million $ spent on bigger satellite launches. This allows universities and smaller companies to be able to launch their own experiments, for research or educational purposes. . Regarding deployment orbits, everything is possible. Most of the time, they are deployed on LEO orbits where remains of atmosphere will create a small drag to bring the cubesat back to earth safely (for other orbiting objects) after its lifetime (cubesats have no thrusting system so they are basically just debris in LEO which) a few months or years later. Sometimes however, they are launched on a higher orbit, or even in deep space for exploration matters. . Payloads . A cubesat can carry commercial off the shelf components as embedded electronics, which allows to demonstrate state-of-the-art technologies in real-life conditions. The limitations are the payload size, weight and power budget as well as evironmental conditions. . Power budget is a proble as there is a very small surface to cover with solar panels and very little room for a battery. . Figure 4: The LightSail Cubesat being prepared by the LightSail team. [JPG] . The amount of cubesat missions is growing each year, with a record of more than 70 launches in 2014. Among them: . LightSail is a 3U cubesat funded by the Planetary Society carrying a 32 square meters solar sail demonstrator. Launched in May 2015, it experienced a software glitch but quickly recovered from it. | JAXA’s cubesat launches from the Kibo robotic arm aboard the ISS. And in particular the PhoneSats launches, the 1U cubesats launched in 2013 that brought Samsung Galaxy S1 smartphones into space as on-board computers. (They ran on Android). | The Flock-1 constellation of Earth imaging cubesats, deployed from the ISS, demonstrating that large cubesats deployements are feasible and worthy for short term missions. | The AAU CubeSat by the Aalborg University in Denmark, which was the first student-built cubesat to demonstrate the capabilities of the cubesats. | And last but clearly the most exciting cubesat launch of this list, the two MarCo, or Mars Cube One that will watch and help maintain communications with Earth ground stations during descent of 2016 Nasa’s InSight mission. | Another hypothetical cubesat constellation launch (or nanosatellite launch) could be what Elon Musk reffered to in his big Space Internet announcement in January. | . Figure 5: The insight mission with cubesats as communication relays [PNG] . Cubesats are regularly accepted as secondary payloads by NASA , SpaceX (Falcon 1 and now 9), ISC Kosmotras (Dnepr rockets), ESA (Vega), as well as other Russian and Indian space agencies. NASA is currently calling for more cubesats launch services, and working on a new dedicated cubesat launch system, the VCLS. . . Sources: . The Cubesat Specification | Light Sail website | B.Klofas: Survey of CubeSats communication systems | P.Muri &amp; J.McNair: Survey of communication subsystems for ISL &amp; Cubesats missions | A.Toorian, J.Puig-Suari &amp; R.Twiggs: Cubesats as Responsive Satellites | SingularityHub.com | SPACE.com: cubesats &amp; deep space exploration | ESA: Vega maiden flight cubesats | Nasa InSight’s CubeSats | Power Budget matters for cubesats | Makezine | Wikipedia |",
            "url": "https://ycouble.github.io/til/satellite/2015/06/02/cubesats.html",
            "relUrl": "/satellite/2015/06/02/cubesats.html",
            "date": " • Jun 2, 2015"
        }
        
    
  
    
        ,"post15": {
            "title": "Dynamic Firewalling",
            "content": "With the now very common Triple Play, Voice over IP and Video Streaming applications are widely deployed and used. And as many events reminded us, securing applications is a critical matter nowadays. . However, deploying firewalling solutions when VoIP or Streaming applications are present on the network can be very risky as it is very hard for the network administrator to have feedback on the user experience. In this article, we’ll try to sum up a few important points on how to support VoIP and Streaming applications passing through a firewall. . Context &amp; available Solutions . VoIP services often use the Session Initiation Protocol (SIP, specified in RFC 3261 &amp; RFC 3265), the other protocols (MGCP, H323 and others) are still used but are progressively less used by end-users. . Streaming and videoconference services on the other may use SIP, RTSP, or even HTTP to set up their media sessions. We will focus here on SIP, but the remarks here are also applicable to other protocols. . Dynamic protocols . VoIP and Streaming are seen as dynamic protocol because the media payload is often sent on a different channel than the session information. This is inherited from Legacy telephony with separated control and data planes. . It generally looks like this: . Figure 1: Media session setup [PNG] . SIP . SIP is a dynamic protocol, where one or more sessions are negotiated by two user-agents before starting to send the user data (here, voice of video). . A classical SIP conversation between two direct agents will look like this: . Figure 2: Simple SIP conversation [PNG] . SIP generally relies on the Session Description Protocol -which is more a language to describe a media session than a protocol- to negotiate the parameters of the session. . SDP is carried in some SIP packets (e.g. INVITEs, 200 OKs, ACKs), and provides information such as UDP ports which will be used to send/receive the media content. . Allowing SIP and its content through a firewall . On a firewall, allowing VoIP will first require the admin to authorize the SIP protocol, which classically uses UDP port 5060 both ways. . But then, how do we, as the admin, allow the voice/video flows? . A simple solution is to authorize a window of UDP ports that we know will be assigned to the media flows. | . These ports are generally assigned within configurable limits and asking the person in charge of the VoIP solution may be sufficient to make it work. But this solution does not satisfy high security requirements as thousands of ports are potentially open and uncontrolled. . A more complex solution is to implement statefulness in the firewall, and by inspecting the SIP/SDP payload, to identify “on the fly” which ports need to be open and to open them only for the duration of the call. | . Consequences of dynamic firewalling . Hardware . This type of operation requires to have Deep Inspection capacities, and enough resource to save and maintain a state table of each session. In other words, a big CPU and enough RAM, as well as a full implementation of every dynamic protocol. . And therefore higher end firewalls, obviously. . Impact on other functions . On next generation firewalls, other functions such as IPS (Intrusion Protection System), Application control, DoS prevention may be activated on dynamic protocol flows, and must be adapted to support the media flow. . For example, the application control, which is in charge to check the authenticity of the application headers of a packet, will need to be aware that a new type of payload needs to be authorized. . DoS Protection . With dynamic firewalling, it is possible to dynamically identify which flows are legitimate media flows and which ones may be malicious ones. Thus, it enables the admin to apply different policies to identified media sessions and other generic unknown packets. . Also, by isolating media session flows in a dedicated DoS policy, one may ensure that none of these packets are impacted by a global DoS attack (udp flood for example) . Stateful firewall clusters . When the firewall is redundant/load-balanced, what happens when the master firewall fails? . With the simple solution, both firewalls have the same simple configuration and media streams are not interrupted. But when dynamic management of sessions is implemented and configured, it is a bit more complicated. . All sessions must be synchronized between the firewall who initiated the session and the other firewalls, so that when it fails, the firewall which will inherit of the session knows which ports must be open for the media streams. This is a pretty important feature to be developped when dynamic session management is used, or the service user experience may be seriously degraded in case of a failure (all calls/media streams interrupted at once). . Example on FortiOS . Let’s have a look at how it is implemented in FortiOS, the firewall OS for Fortinet products (Fortigate series). . Service configuration . Service customization . SIP is already configured by default on FortiOS, so you don’t need to modify it. However, if you need to use specific ports for your SIP application, it is possible to change it manually. config vdom edit test config firewall service custom edit &quot;SIP-custom&quot; set protocol TCP/UDP/SCTP set udp-portrange **&lt;SIP _custom _port&gt;** set comment &quot;custom SIP service&quot; next end next end NB: if VDOM are not used, use config global instead. . Session-helpers . Each manufacturer has its own name for this feature, Fortinet uses the words session helpers. Session helpers are implemented directly by the manufacturer and may not be configurable, or limited in configurability. For fortiOS, it is only possible to change the service port (the one the session protocol is using). . To display all session-helpers: config global show system session-helper And then look for the block with sip as name: edit 13 set name sip set port 5060 set protocol 17 next NB: if you need to customize or duplicate a session-helper entry, it is important not to change the name, which is how FortiOS associates the session-helper with the actual code. . Policy . The association between a service and its session-helpers is done automatically. . The last step for the basic configuration is to create the policy: config vdom edit test config firewall policy edit 1 set srcintf **&lt;portX&gt;** set dstintf **&lt;portY&gt;** set srcaddr **&lt;addr _source&gt;** set dstaddr **&lt;addr _destination&gt;** set service SIP *//or the name of the created custom service* set action accept next end next end . Application Control . If a strict Application profile is applied to your policy (which is only possible on high end devices), it may be necessary to add the RTP/RTCP or any media transport protocol used to you application control list. . IPS . Similarly, you may want to monitor IPS threats regarding the media protocols and associate the IPS/Application Control profiles to the policy. . Dos Policies . DoS policies are better used when they are more specific: if a type of packet crosses its associated thresholds, packets will start to be dropped by the firewall. . With session-helpers, the media flows are identified as belonging to the same service as the SIP, and are treated as such by the DoS protection mecanisms. Therefore it is necessary to dimension the DoS policy in a consistent way, including both SIP and RTP traffic, which may represent high udp packet rates. . (with G711.A codex, 1 unidirectional media stream generates about 45 pps, versus maximum 2 or 3 pps when SIP is alone) . High availability parameters . In FortiOS, high availability and session synchronization settings for media traffic are setup as follow: config global config system ha set session-pickup enable set session-pickup-connectionless enable end end . Sources: . RFC 3261: SIP: Session Initiation Protocol | FortiOS technical documentation |",
            "url": "https://ycouble.github.io/til/security/2015/04/27/dyn-firewall.html",
            "relUrl": "/security/2015/04/27/dyn-firewall.html",
            "date": " • Apr 27, 2015"
        }
        
    
  
    
        ,"post16": {
            "title": "April Fool's Day",
            "content": "Today is April 1st, so it is the occasion to shine a light on a few fake RFCs made on that day. . The most famous of all is the experimental RFC1149 of 1990, A Standard for the Transmission of IP Datagrams on Avian Carriers. As the title suggests, the authors specified an entire protocol to send IP packets using pigeons as physical layer. The RFC has been updated and even tested in Norway. . There are a few others that made me laugh: . RFC1605: SONET to Sonnet Translation by William Shakespear: where SONNET means “SONET Over Novel English Translation”, a way to compress SONET’s 810 bytes frames into 14 lines of 90 bytes frames. | RFC1925: The Twelve Networking Truths. From this one I will only quote the following: “With sufficient thrust, pigs fly just fine. However, this is not necessarily a good idea. It is hard to be sure where they are going to land, and it could be dangerous sitting under them as they fly overhead.” | RFC2795: The Infinite Monkey Protocol Suite (IMPS) describes practical issues of having an infinite number of monkeys typing randomly on an infinite number of keyboard to eventually write a copy of shakespear’s work. | . This reminds me of a friend’s video about the number ( pi) . RFC3251: Electricity over IP which has a lot of cool accronyms in it. | RFC6921: Design Considerations for Faster-Than-Light (FTL) Communication. | . Wikipedia, from which all of these were found has a nice list of April fool’s RFC which I encourage you to take a look at. .",
            "url": "https://ycouble.github.io/til/misc/2015/04/01/april.html",
            "relUrl": "/misc/2015/04/01/april.html",
            "date": " • Apr 1, 2015"
        }
        
    
  
    
        ,"post17": {
            "title": "IP Multicasting",
            "content": "This article tries to synthetise a few RFCs about Multicasting over IPv4. . Services such as TV and radio were initially designed to be broadcasted over hertzian media for which broadcasting is the standard way of communicating. But these services among many others had to transition to the IP world to follow and widen their audience. . Until here, no problem. Except with IP, broadcast services are no longer alone and they have to live along many many others. Most of which are unicast services. . The IP protocol, as specified in RFC 791, allowed one to one (unicast) and one to everyone (broadcast) only. Simply broadcasting the traffic is not an option as it would flood every network worldwide (or more likely be blocked at the first router encountered). Which leaves us with sending traffic to each client requesting the service in unicast mode. . This becomes expensive bandwidth and energy wise for the broadcast source and for the network in general after only a few clients as the content is duplicated on many segments of the network. Reducing data duplication and optimizing network ressources while also extending the reach of the broadcasting service are the main objectives of IP Multicasting. . To better understand what is at stake with IP multicast, we can look at the figures below. In the case of legacy broadcasting over a dedicated network, a broadcasting point has to be deployed close to each client while with IP multicast, only the very necessary flows are generated over an already deployed network. . Figure 1: Case of broadcasted content over Hertzian media [PNG] Figure 2: Case of multicasted content over IP networks [PNG] . Multicast group management in IPv4 . In 1989, the IETF published the informational RFC 1112 to gather recommendations on how to implement IP Multicast inside the IP stack implementations. It also defines in the appendices the protocol IGMP. IGMP has been updated twice in 1997 (IGMPv2) and 2006 (IGMPv3). . Integration in the IP stack . IGMP is a part of IP, in the same way ICMP is. IGMP only introduces a few additional services offered to the transport layer and requires an adapted Data Link layer for Multicast. Initially, all an IP host/router has to do to be multicast compliant, is to accept and process packets destined to multicast groups if it belongs to the group, know which groups it belongs to and of course the ability to join/leave a group. . Figure 3: IGMP integration in IP stack, extracted from RFC 1112 [PNG] . Multicast IP addresses . The IANA has reserved the 224.0.0.0/4 for multicast group addresses. See RFC 5771 for more details and best practices about IPv4 multicast addresses. . Many of the (2^{28}) addresses are reserved, As an example, the 224.0.0.0/23 is reserved for network protocols (local &amp; internetworks), here are a few noticeable addresses: . 224.0.0.1: All hosts multicast group address. | 224.0.0.2: All routers multicast group address. | 224.0.0.13: multicast group address used by PIMv2 | 224.0.0.22: multicast group address used by IGMPv3. | . A few other addresses are reserved for router redundancy/routing protocols, as the 224.0.0.18 group address which is reserved for VRRP, and 224.0.0.5-6 reserved for OSPF. . More generally, the IANA has divided the multicast IPv4 block into blocks, each one being destined to a specific usage. Among these groups, we can find: . The 233.0.0.0/8 block is the GLOP Block, with (2^{16}) /24 blocks assigned to each 16bits ASN. | The 239.0.0.0/8 block for private use (equivalent in multicast of the 10.0.0.0/8 block for unicast). | . L2 muliticast addresses . IP multicasting requires an adaptation of layers underneath IP. Here we’ll only deal with Ethernet. . Ethernet has defined an OUI specific for Ethernet multicast addresses: 01-00-5E (hex). IP multicast addresses are then translated into the corresponding MAC addresses with a simple principle, copying the 23 lowest-order bits from the IP address into the 23 lowest-order bits of the MAC address. . Figure 4: IP multicast address translation into MAC multicast address. [PNG] (Credits: The TCP IP guide) . Evolution of IGMP . IGMP is the protocol that enables hosts to inform routers what group they belong to, and routers to ask hosts on the LAN for this information. The protocol reach is local, and an IGMP packets are never routed. Besides, IGMP messages are carried within IP packets. . IGMP defines two types of actors: . Hosts: the end users in a LAN who join and leave multicast groups. | Routers: the IP routers who coordinate the multicast groups in a LAN. In IGMPv2 &amp; IGMPv3, routers are diffentiated according to their IGMP role. The Querier is the active IGMP routers on the LAN. He is the one to ask for membership reports. | The Non-Queriers are other routers on the LAN, implementing IGMP but not performing any IGMP action. They must be IGMP silent. | . | . IGMPv1 . IGMPv1 only describes how host advertise the groups they belong to, and how routers poll request host to report their memberships. . In IGMPv1, the IGMP router sends a Query to the All Host multicast group address (224.0.0.1) to know what are the multicast groups needed on the LAN. Hosts, when recieving these queries, respond by sending a group report for each group they belong to. . The relevant information for the router is which groups are required on the LAN, but not how many clients there is. Hence, in order to reduce the amount of IGMP packets flowing on the LAN right after a Query, hosts must set up a timer for each multicast group and send each report when its timer expires, if no other reports has been recieved for the group. . The IGMPv1 frame is quite simple and forecast future IGMP versions with additional data fields and packet types. . Figure 5: IGMPv1 Frame format, extracted from RFC1112 [PNG] . IGMPv2 . Version 2 of the protocol improves IGMPv1 by enabling hosts to tell when they leave a group, allowing faster multicast stream closure and a more efficient use of bandwidth globally. . The IGMP frame is modified in version 2, with a max-response-time field to allow router to constrol and speed up the process of retrieving a report from a host. Routers keep sending periodical General Queries to have a group membership overview for a LAN. . This version of the group management protocol requires hosts to send an IGMP Leave Group message whenever the leave_group service is asked to the IP/IGMPv2 stack. This message is sent to the All Router multicast group address with a new packet type. . When recieving a group leave report from a host, routers must perform a Group Specific Query to the x.x.x.x to check if there is at least one host belonging to the x.x.x.x multicast group. . Figure 6: IGMPv2 Frame format, extracted from RFC2236 [PNG] . IGMP Snooping Switches . IGMP describles the control plane of multicast communications. On the data plane side, over a LAN, switches simply broadcast multicast frames by default. Over large LANs, it becomes very inefficient in terms of bandwidth usage. . Figure 7: Impact of snooping switches on a LAN [PNG] . Manufacturers such as Cisco quickly developped software to allow user to configure switches to only forward multicast frames to the ports where multicast reports were recieved for that specific group. Of course the the All Host multicast group ethernet address (01-00-5E-00-00-01) should always be broadcasted. . Implementations are highly manufacturer dependant, even after the IETF produced the Best Practice RFC4541 in 2006. . This mechanism reduces considerably the bandwidth used in a LAN by multicast traffic, and extends the multicast principles to the LAN. . Figure 8: How snooping switches handle multicast [PNG] . Snooping switches should always broadcast a multicast frame intended for a multicast group where no report has been sent. But, it is not always implemented. A common issue with IGMP snooping is that routing protocols using multicast address to communicate between routers are blocked by the snooping switch. . Also, IGMP snooping doesn’t live well with some IGMPv1 and IGMPv2, where hosts may assume that the traffic is broadcasted and not see any interest in sending a report if another host has sent one earlier. As a matter of fact, RFC1112 recommends that a host cancels its report-send-timer uppon reception of a report for the same multicast group. The snooping switch, on the other end will never see a multicast frame originating from the host for that group and will not forward multicast frames towards the unfortunate host. . Along the IGMP snooping best practices, the IETF published a Standard Track, RFC3376 for the IGMPv3 specification, snooping aware. . IGMPv3 . IGMPv3 was specified with a few flaws of the previous versions or improvements in mind: . Snooping switches as discussed above. | Hosts would often need the traffic from one source rather than the whole multicast group traffic. | . For example, an IPTV service could use one multicast group to broadcast all its channels, but a client of this service only needs one source’s stream at a time. . Host suppression (cancelling sending a membership report because another host already has), doesn’t allow a clear host identification by routers | . Frame format . IGMPv3 use a larger frame for its messages, and uses two different formats for Membership Queries and Membership Reports. A few other additional fields have been added in IGMPv3 to better control multicast group members and optimize multicast flows. . One of the main changes is that this version allows host to aggregate their group reports inside one. But then the report has to be sent to a generic multicast address, 224.0.0.22 dedicated to IGMPv3. All IGMPv3 router must belong to this group as Hosts. . Membership Query messages are now always group specific and include a list of available multicast sources for that group. . Figure 9: IGMPv3 frame format, as specified in RFC3376 [PNG] . Source specific multicast . As multicast sources are provided to every IGMPv3 host, they are able to choose and tell routers which sources they want to listen to. . This allows a more precise traffic engineering over WANs (only sources which have clients will be multicasted, and only to those interested). I think this change was introduced because there is only so many multicast addresses for each operator compared to the amount of services that can benefit from multicast. Source specific multicast allows a company/operator to “broadcast” (used here to say “make a content available to everyone”) multiple services over a unique IP. . Suscribers usage can then be traced more precisely inside the multicast group, and the audience can easilly be measured for each service individually. . Snooping awareness . With source specific multicast, two hosts requesting membership of the same multicast group may not ask for the same sources. Hence a lot of mecanisms such as host suppression are not relevant or necessary anymore. Moreover removing host suppression solves all snooping problems encountered in v1 or v2. . Source inclusion/exclusion . There are two types of reports: . Inclusive reports (with a Group Record field set to 1 aka MODE_IS_INCLUDE), only ask for a limited number of sources. | Exlusive reports (with a Group Record field set to 2 aka MODE_IS_EXCLUDE), ask for all sources except a limited number of sources | . This provides more flexibility to hosts, while also factorizing the join and leave actions into one single message. . For example the join group report from IGMPv1/v2 is a MODE_IS_EXCLUDE report with no source specified, and the leave group report is a MODE_IS_INCLUDE report with no source specified. . Interoperability of different versions of IGMP . Each version of IGMP is specified to be retro-compatible, but are known to get along with each other very badly. Generally, if the later version (v2/v3) detects an IGMP router with an earlier (v1/v2) version it automatically switches to the oldest version of the two. . Nevertheless, IGMPv3 can theoretically function with IGMPv1/v2 hosts in the LAN. . Multicasting inside a LAN . IP Multicast is mainly designed for the transport and routing of multicast over networks. But even in a LAN, it can find some useful applications. . Multicasting over solely a LAN provides some challenges aswell and I will try to overview some theoretical possibilities in a later article dedicated to the topic. . However in the meantime, I remain open to any feedback or advises on architectures, specifications or products you may know. So don’t hesitate to leave me a comment below! . Multicast routing . Routing multicast traffic basically consist in building the shortest tree to cover all suscribers for a service. . Multicast routing is another vast subject that I am not able to study and syntethize immediately, so it will hopefully be addressed in a future article. . Sources: . RFC 1112: Host Extension for IP Multicasting | RFC 2236: Internet Group Management Protocol, Version 2 | RFC 3376: Internet Group Management Protocol, Version 3 | RFC 5771: IANA Guidelines for IPv4 Multicast Address Assignments | RFC 4541: Considerations for Internet Group Management Protocol (IGMP) and Multicast Listener Discovery (MLD) Snooping Switches | Wikipedia article on PIM routing |",
            "url": "https://ycouble.github.io/til/networking/2015/03/31/ip-multicast.html",
            "relUrl": "/networking/2015/03/31/ip-multicast.html",
            "date": " • Mar 31, 2015"
        }
        
    
  
    
        ,"post18": {
            "title": "MPLS - Label Switching",
            "content": "This article focuses on label switching principles in MPLS protocol, this is the second part of my wiki for the IP/MPLS technology. See my introduction the MPLS for a wider view on MPLS. . In this article, we will go deeper into what constitues a MPLS network, and how data is switched over the network. . MPLS is a core network protocol and doesn’t define how access is to be handled. Hence, a provider using MPLS will be interconnected to one or more access operators, who are then its customers. . With that in mind, the router interconnecting the core network provider and the access operator is the Customer Edge Router (CER or more generally only CE). On the other hand, routers participating to the MPLS, but only connected to other MPLS routers are called Provider Edge Routers (PER or PE). . The MPLS standards [RFC 3031] define new denominations that we will prefer here. A MPLS router is called a Label Switch Router (LSR), and when it is on the edge of the MPLS network, it will be called E-LSR (Edge-LSR) or sometimes LER (Label Edge Router). . The CE/PE terminology is inherited from an older protocol with similar node roles, ATM. Operator were used to these names and MPLS devices generally implemented ATM as well, so the old names stayed rather than being replaced by the newly defined ones. . Figure 1: A MPLS network, with 3 customers. [PNG] . The E-LSR or LER is the node in charge of encaplusating/decapsulating packets into/out of the MPLS network, and the LSR does the switching according to the MPLS header. Classically, a LSR offers more functionalities than a LER. Sometimes, a LER can also fulfill the LSR role. . Figure 1 shows a MPLS Network, interconnected with 3 customers. Customer 1 has 2 geographically distant sites. The LER/LSR 1 here is the entry point, and hence is a LER, for customer 1 but can also be used as a LSR to switch MPLS frames from LER 2 to LER 3. . MPLS is a packet switching protocol, like Ethernet: ingress packets on port X with label a will always be switched to port Y according to the forwarding table. . Except in Ethernet the destination MAC address -which is a unique identifier for an endpoint- is kept from one hop to the other, while MPLS will modify the label at each LSR. This is called label swapping. The meaning of a label is only link-local and doesn’t identify anything but a type of packets that are passing from one LSR to the other. In fact MPLS doesn’t have any endpoint identifier, which retrospectively seems quite obvious for a core network. . Hence, packets are not forwarded based on who it is destined to but rather according to how the MPLS network classified it at network ingress. . This is a fairly important distinction and allows MPLS to perform its own Quality of Service independantly from its customers. . Another significant difference between MPLS and Ethernet is that ethernet doesn’t use a control plane, which enables an easier automatic decentralized configuration. MPLS on the other hand relies on its control protocols to determine what are the label switched paths (LSP), classification, QoS etc. . QoS, classification and the control plane are wide topics and will be describe thouroughly later, in articles dedicated to them. . Here, let’s just focus on how a packet is really forwarded from a MPLS entry point towards its destination network, assuming that all forwarding tables are already computed in the whole network. . Below is a simple example for a 4 node MPLS network and 2 customer networks. . Let’s also assume the switching tables described in Table 1. . Figure 2: Example with 2 LER and 2 LSR [PNG] . Node ingress port ingress label egress port egress label . LSR2 | 1 | L0 | 2 | L4 | . LSR2 | 1 | L1 | 3 | L5 | .   |   |   |   |   | . LSR3 | 2 | L4 | 3 | L7 | . Table 1: MPLS forwarding tables for nodes LSR2 &amp; LSR3 . Let’s say a packet p is routed towards the destination network: . At LER1 ingress, packet p is for example pushed the label L1 and is forwarded towards LSR2. | At LSR2 ingress, p is switched to port 3 and label swapped to L5 according to the forwarding table. | At LER4 p exits the MPLS network and its label is popped. | Figure 3: Path for a packet assigned with label L1 at network ingress. [PNG] . Now assume that another packet p’ with different characteristics but towards the same destination network: . At LER1 ingress, unlike p, packet p’ is pushed the label L0 and is forwarded to LSR2. | At LSR2 ingress, p’ is switched to port 2 and label swapped to L4 according to the forwarding table. | At LSR3 ingress, p’ is switched to port 3 with label L7 | Ats LER4, p’ is label popped, exits and is delivered to the destination network | Figure 4: Path for a packet assigned with a different label at network ingress. [PNG] . In this example, we can see that the label a packet is assign will determine how it will be switched all along its path trough the MPLS network. In this case, p’ gets a longer path. While it may seem under-optimized traffic handling, it may also be used for two things, which happen to be 2 of the most important feature of MPLS: . Given a label, one may not access all the MPLS network egress nodes. Also, a customer can be given a label dedicated to him and all his traffic handled the same way, towards its authorized destination. MPLS VPNs are based on that observation. | If LSR2-LER4 is usually congestioned, p’ may have a more advantageous path regarding delay or other metrics. Organizing the MPLS network paths according to different constraints is called traffic engineering and is widely used over the internet along with QoS. These two aspects will be further detailed in their respective articles. | . To continue with the ethernet comparison, it is noticeable that every ethernet frame with the same destination are always switched using the same path, provided that the network topology doesn’t change. . MPLS carries a very small amount of information mainly composed of the label, a TTL and the EXP field, used for QoS. Below is the frame format for one label. A MPLS header, called “label stack”, is composed of 1 or more labels. . Figure 5: MPLS label format. [JPG] . Stacking labels can be used to provide additional services. For example the Virtual Private Networks based on MPLS use an additional label to identify the ‘service’ packets belong to additionally to the first layer of labels that are used for switching packets. This, again, will be further explained in a dedicated artcile. . In the case of MPLS over Ethernet, the Ethernet frame at the MPLS ingress node is transported inside the MPLS packet, over another Ethernet frame. (IP over Ethernet over MPLS over Ethernet). . Below are the packet detail of an example MPLS packet. The packet is available here. Another example can be found here (with two labels). . Figure 6: Example of MPLS over Ethernet encapsulation (protocol view). [JPG] Figure 7: Example of MPLS over Ethernet encapsulation (bit view). [JPG] . . Sources: . Experience | Courses from B.Paillassa at ENSEEIHT | RFC 3031: MPLS Architecture | RFC 3032: MPLS Label Stack Encoding | Wikipedia |",
            "url": "https://ycouble.github.io/til/networking/2015/03/03/mpls-switching.html",
            "relUrl": "/networking/2015/03/03/mpls-switching.html",
            "date": " • Mar 3, 2015"
        }
        
    
  
    
        ,"post19": {
            "title": "DTN & Bundle Protocol",
            "content": "This article follows the previous introduction to DTN. . Satellite Networks differ from classical ground wired/wireless networks in many ways. Among which: . Long to very long delays | Error bursts on the signal channel, especially on the ground to space segment | Unreliable topology: a node may become unreachable for a period of time without notice, or have a scheduled time span of availability | Each node is really expensive to deploy, hence aborting or failing a packet transmission is very costly | . Network utilization and its resiliency to error is a critical issue in satellite networks, and its optimization is a vast area of research. . The delay-tolerant or disruption tolerant network aims to unifying communications in a global overlay network, defining how actors may transfer data according to their needs. Different services are defined, as well as their goals, in the DTN Architectures RFC and its direct protocol translation: the Bundle protocol. . Both RFC have been written by the same authors (NASA, Google, MITRE, Intel). . The Bundle protocol is intended as an overlay, transport independant protocol, proposing Quality of service, custody transfers with delivery options for unicast, anycast, multicast and broadcast communications. . Below is the ascii version of the Network stack. . Figure 1: The Bundle Protocol Network Stack, extracted from RFC 5050. [PNG] . The Bundle protocol is closely similar in idea to the postal services. Actually, it is largely inspired from it. For example: . the postal services give the possibility to ask for an acknowledgement, which is shipped with the mail itself. | Postal offices keep mails for a long period of time before a postman delivers it to destination. | A very remote village with only one postman delivering mail once a week can easily be compared to a satellite being in line of sight of the closest node only one day a week. | Likewise, the possibility to monitor the delivery status of one’s package is often offered by mail services, with each post office reporting the reception/forwarding of the package. | . The Bundle Protocol works exactly like that, and offers the same type of services to the applications using it. . The Bundle protocol defines two entities: the node and the endpoint: . The node is the router running a bundle protocol instance | Nodes can register to enpoints. An endpoint is the final destination node of a bundle. A node can participate in one or more endpoints but has to be part of an endpoint where he is the only one, called a singleton. A node is identified by an endpoint ID. The singleton endpoint ID of a node is then a unique identifier for this node. | . Endpoint ID are similar to a postal address or a web URL. An URI scheme is defined for the DTN architectures (more generically than just for the Bundle Protocol). . The Bundle Protocol Data Unit . The classical Bundle Protocol Data Unit (BPDU) is shown below. This is only an example, as only a template is defined. The BPDU has customizable fields (the same way HTTP has). . Figure 2: Example of a Bundle Protocol Data Unit, extracted from RFC 5050. [PNG] . Delivery options . There are seveval types of acknoledgement that can be requested by the application agent to the bundle layer. Each of them are independant and non exclusive. . Recieved bundle ack | Forwarded bundle ack | Application layer ack | Custody accepted ack | A few other more | . They are used as indicators for the sender, for example to release a node which was storing temporarly a bundle in ‘custody’ (see below for more details on custody transfers). On a more practical point of view, acknoledgement are used by application agents to locate and emphasize the progress of the data transfer. . Quality of service . The Bundle protocol offers two parameters of quality of service: . an importance degree (or drop me last priority) | a delay degree (or forward me first) | . Both have 3 levels, which gives 9 levels for Quality of service. . Altgough expiration time is not expressively a QoS criteria, it has its impact on forwarding queues in intermediary bundle nodes. . Reliability and Custody transfers . This is in my opinion the main new feature of the DTN. . Custody transfers are store-and-forward transfers pushed at their maximum. The bundle passes from one custodian endpoint to the other, and each custodian endpoint stores the bundle until it has confirmation that the bundle was accepted in custody by another custodian endpoint. . This is somewhat similar to a prisonier transfer, hence the name. . The bundle/prisonier is escorted from one custodian endpoint/prison to the other by an agent who will expressively report the success of the transfer. Except the bundle stays in the first prison while also being transferred to the second! Handy, isn’t it? . Bundle nodes are free to chose to accept or refuse a bundle’s custody request. If they chose to refuse custody, they still can simply forward it, and send acknoledgements according to the bundle’s delivery options. . This is illustrated on the figure below. A step-by-step figure is available here . Figure 3: Example of a custody transfer. [PNG] . Application to common L7 protocols, and conclusions . This will be the focus for a future article related to DTN. . . Sources: . RFC 4838: Delay Tolerant Networking Architectures | RFC 5050: Bundle Protocol Specification | RFC 6255: Delay Tolerant Networking Bundle Protocol IANA Registries |",
            "url": "https://ycouble.github.io/til/satellite/networking/2015/02/24/dtn-bundle.html",
            "relUrl": "/satellite/networking/2015/02/24/dtn-bundle.html",
            "date": " • Feb 24, 2015"
        }
        
    
  
    
        ,"post20": {
            "title": "Introduction to Delay Tolerant Networking",
            "content": "This article gathers a few notes i made during my readings about Disruption Tolerant Networking (DTN) and one of its implementation: the Bundle Protocol. . Satellite communications are unique in the way that the transport media is very erratic. Transmission is rarely guaranteed as deep-space communications bring their fair share of delay and packet corruption (long to very long distances, weather, orientation problems, obstacles etc.). With a classical “assured end-to-end transmission” protocol like TCP, messages would either over-congestion the network or never make it to destination. . Figure 1: Interplanetary communication scenario. [JPG] . For example, on Figure 1 we have an earth ground-satellite station, a geostationary satellite (GEO sat.) and an observation satellite orbiting the Moon (LLO sat.). While Earth GEO links are always operational because of the nature of the orbit, the GEO sat. is not always in the line of sight of the Moon, and even more so of the LLO sat. If the LLO sat. disappears behind the Moon during the transmission of say, a very high resolution picture of the Moon’s hidden face, with standard TCP, the whole session is lost and has to begin again. . Figure 2: Inefficient data transport. [JPG] . As shown on Figure 2, if an error occurs on any of the communication segments, the transmission time is tripled and the retransmission has to be done on each hop, wasting some precious bandwidth. This scenario gets even worse if the LLO sat. disappears behind the Moon during the retransmission. . An intuitive and simple solution is to tell the GEO satellite to keep the packet in memory until he is sure that the whole session has been delivered. It will ensure that every segment is used only as much as it is necessary, and that data will be delivered even in the case of a temporary absence of route to destination. Figure 3 illustrates this technique in the same scenario as Figure 2. . Figure 3: An improvement of Fig.2 transport scheme. [JPG] I’ll leave this post as is, and will make another post to go further on the DTN architectures and its main implementation: the bundle protocol. .",
            "url": "https://ycouble.github.io/til/satellite/networking/2015/02/13/dtn.html",
            "relUrl": "/satellite/networking/2015/02/13/dtn.html",
            "date": " • Feb 13, 2015"
        }
        
    
  
    
        ,"post21": {
            "title": "Router Redundancy Protocols & device boots",
            "content": "This articles addresses a particular aspect of redundancy management. This is a result from my experiments at Alstom &amp; personal thoughts. . In a Highly Available architecture with routing needs, it is of course necessary ensure continuous routing. There are a few different solutions for that, but the one I am interested in here is router redundancy. . Virtual Router Redundancy Protocol (VRRP) is one of the most widespread router redundancy protocol across router manufacturers. . I’m not going to go thoroughly into all VRRP principles and features, but some important aspects have to be reminded: . VRRP uses a virtual IP &amp; virtual MAC address | RRP elects a Master &amp; Backup instances across all available instances, the master gets to carry the virtual IP | VRRP uses announces between all instances to manage who gets to be master. | Each VRRP instance has a priority (0-255) | A pre-empt attribute can be defined to takeover the Mastership is one’s priority is higher. | Conditions are configurable, depending on the implementation on how to behave when an instance detects a pre-empt possibility. For example it can choose to wait for X seconds before pre-empting the Mastership. | . During boot process, some device polarize their interfaces to perform a health check. Some manufacturers even leave these interfaces up for a while even though the device is not able to forward any data during the process. Moreover, these interface polarization may happen multiple time during boot process. This is illustrated on figures 1 to 3 below. . Figure 1: All devices up. [JPG] Figure 2: Switch fails/is rebooted. The Slave router takes over mastership of the VRRP instance. [JPG] Figure 3: During boot, the switch polarizes its ports. R1 claims Mastership of VRRP instance and route all the traffic towards the booting device. [JPG] . While it may be understandable for the equipment manufacturer, directly connected devices are not aware of the booting process and may mis-interpret it. For example a router with VRRP enabled, in Init of Backup State will see its interface up and start announcing again its priority as if back to nominal mode. If the device ends up getting back in Master state, then all traffic routed on this interface will be sent towards the still-booting device, which is still not ready to forward anything. The router has absolutely no clue the traffic is lost in most cases (Ethernet + IP don’t have any acknowledgement mechanisms). . Such a booting process stage may take -and I am using real life numbers here- up to one minute ! One minute of traffic is the death of most on-going TCP connections, VoIP calls (humans are not as patient as UDP) etc. I feel that a huge deal in many cases, with as worst case scenario vital function relying on those redundancy features! . It bugs me a bit, and I can’t explain why there is so little ways of protecting another device from this annoying phenomenon that are actually implemented in many manufacturer software. . OK, so what theoretical ways do we have to prevent these naive VRRP role change? . There are actually a lot of possibilities, here are a few of them I have seen: . Delay the interface operational “Up” status. | This way, the interface is a little bit less naive and says “OK, let’s see if you can last more than a minute” to the adjacent booting device. In other cases (plugging a wire for example), it should not interfere with anything else. . It’s brutal but it should work in any cases as long as the timer is long enough. . Configure pre-empt conditions. | Like a timer or more complex conditions. This does work only when the VRRP announces are not using the failed device. . Configure BFD for the interface May only work if BFD doesn’t use the failed device/BFD is available on the failed device/the VRRP router and the failed device belongs to the same organization. | Configuring a ping-server/Tracking an IP | It is a bit of an overkill, and may not always work if firewalls are present in the neighbourhood (not so uncommon, as we are at a routing point). . Delay the VRRP instance operational “Up” status. | This is probably the cleanest way as it only affects the VRRP instance (which is the only one having a problem here) and not other functions (administration of the failed device for example). Also, it works even if the VRRP announces passes through the failed device (which to me feels like a cheap use of VRRP). . This is different from an initialization delay, as the Init state is reached way before the link is falsely restored. . Other ways may exist, but that is all I have seen so far. If you happen to know some, feel free to contact me, I’ll be glad to learn ! | As a conclusion, I would like to point out that these workarounds are mostly implementation dependent, as I don’t think the RFC details this type of cases. . Also, this situation only happens when the VRRP announces uses the same medium as the traffic itself, or when a pre-empt is configured. . .",
            "url": "https://ycouble.github.io/til/networking/2015/02/11/rrp.html",
            "relUrl": "/networking/2015/02/11/rrp.html",
            "date": " • Feb 11, 2015"
        }
        
    
  
    
        ,"post22": {
            "title": "The Oberth effect",
            "content": "These notes are about astronautics, a field in which i learn everyday and i might say utterly stupid things!. . The Oberth effect is used in astronautics to choose the best time to fire the power engines to modify orbit most efficiently. Basically it says that the faster you go, the more change in kynetic energy (or change of speed, aka Delt-V) you are going to get from your fuel. This is more useful for high power rocket engines (like chemically propelled rockets) than for low thrust engines (e.g. ion drives) as the former concentrate their energy over a lower amount of time, spending it all at the highest speed. . In more mathematical terms, let’s consider the definition of Kynetic Energy: E_k=12mv2E _k = frac{1}{2}mv^2E_k=21​mv2 Where (E_k) is the kynetic energy, (m) the mass of the object, and (v) speed. Hence, Δ(E_k)=E_k1−E_k0=12m(v_0+Δv)2−12mv_02 Delta(E _k) = E _{k1} - E _{k0} = frac{1}{2}m(v _0 + Delta v)^2 - frac{1}{2}mv _0^2Δ(E_k)=E_k1−E_k0=21​m(v_0+Δv)2−21​mv_02 Which leads us to: Δ(E_k)=mv_0(Δv)+m(Δv)22 Delta(E _k) = mv _0( Delta v) + frac{m( Delta v)^2}{2}Δ(E_k)=mv_0(Δv)+2m(Δv)2​ So at constant ( Delta v), the kynetic energy gain is proportional to the speed at the moment when the engine was fired. . And as we know from Kepler’s laws, the apoapsis of an orbit trajectory is the point where the distance and the speed are the greatest. So that’s the point with the most ( Delta v) efficiency. . In the end the Oberth effect is an astronautics basic, allowing fuel-wise cheaper orbit transfers and trajectory modifications in interplanetary travels. It’s the fundamentals on which bi-elliptic transfers and gravitational assists techniques are based. Hermann Oberth was a German scientist who worked on Nazi rocketry projects (among which the V2 rocket weapon), he also worked for rocketry projects all over the world, as for example for NASA with his former student Wherher Von Braun. He is considered as one of the fathers of rocketry and astronautics. . . Sources: . Wikipedia |",
            "url": "https://ycouble.github.io/til/space/2015/02/05/oberth-effect.html",
            "relUrl": "/space/2015/02/05/oberth-effect.html",
            "date": " • Feb 5, 2015"
        }
        
    
  
    
        ,"post23": {
            "title": "MPLS Basics",
            "content": "This article is an introduction to MPLS principles, and is intended as the first part of a wiki i’m making for my work. . MultiProtocol Label Switching (MPLS), as its name suggests, is based on label switching. The objective of this protocol is to reduce the processing time of a packet in a router by bypassing route tables as much as possible. MPLS messages are not switched relatively to their destination but according to the label the have been applied. . Basically, all packet are routed/classified according to their destination, type of service, customer and many other criteria when they enter the MPLS network. After that, the only thing that matters for MPLS devices is the labels assigned to the packet. . Naturally, MPLS is a core network technology and is designed to be used for high volume transportation (when routing tables become too long and that traffic can be classified). . At MPLS network ingress a label is pushed inside the packet, then for the whole travel through MPLS network, labels are swapped at each node and eventually popped when egressing the MPLS network. . Label switching is pre-calculated and advertised through a signalling protocol. To ensure the availability of bandwidth and avoid congestions, ressources in the MPLS network are reserved by this protocols, creating pathes and protection pathes from one point to the other. . MPLS allows to classify traffic according to many criteria into several Forward Equivalent Class (FEC). Each class is supposed to include traffic that have the same kind of needs and expectations from the network. Classify traffic allows to share the ressources accordingly. . There are typically 5 very different types of classes: . Network control messages: signalling, network time, etc. They must be forwarded whatever the conditions. e.g. NTP, IS-IS… | Voice: low bandwidth, but with very restrictive requirements on latency and jitter. e.g. SIP, RTP… | Live Streaming: high bandwidth, highly jitter-dependent and sometimes demanding low jitter. e.g. RTSP, RTP, Streaming protocols. | Data that have to be carried in a reasonable time. e.g. http pages | Low priority data: low requirement traffic. e.g. FTP… | . MPLS allows to extend the range of a local network over IP networks. For example it is possible to have multiple geographically distant sites on the same local network, with the MPLS network being seen a a single switch. These VPNs are called epipes, Virtual Leased Lines(VLL) or Virtual Private LAN Service VPLS. . Also, it is possible to only hide the internet for a customer and to interconnect “directly” two distant sites with a virtual router carried over the whole MPLS network. These VPNs are called Virtual Private Routed Networks. . Most of MPLS signalisation protocols have been extended to a traffic engineering version with new algorithms for path calculation allowing to optimize pathes accross the MPLS network following several criteria. . As the signalling protocol has the vision of the whole network, it is able to organize paths and protection schemes. For example, it is possible to protect a specific path from a signle (or double, or p simultaneous) failure(s). Retrospectively, this is the kind of algorithms i was working on during my internship at GERAD. .",
            "url": "https://ycouble.github.io/til/networking/2015/02/05/mpls-basics.html",
            "relUrl": "/networking/2015/02/05/mpls-basics.html",
            "date": " • Feb 5, 2015"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Yoann Couble",
          "content": "Data scientist expérimenté, mes centres d’intérêts sont le NLP (modèles de langues, extraction de connaissance, NLU, NLI, NLG), l’explicabilité et l’équité du Machine Learning mais surtout l’opérationalisation de ces techniques. Vous pouvez en savoir plus sur moi sur les différents réseaux sociaux présentés en footer. . . Experienced data scientist, my interest is now on Natural Language Processing (laguage models, knowledge extraction, NLU, NLI, NLG), explainability and fairness in Maching Learning, but most of all the productionalization of these techniques. You can find me on my social links below. .",
          "url": "https://ycouble.github.io/til/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ycouble.github.io/til/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}