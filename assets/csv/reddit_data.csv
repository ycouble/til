,text,tag,subreddit,id
0,"I’m looking for datasets or api source that quantifies fan base, or preferably, bettors’ sentiment regarding a team’s performance or direction. Does anyone know of an API that tracks this? For now I’m looking specifically for NBA, but am also interested in MLB, NFL, and NCAA f-ball and b-ball.",API,datasets,s0vufk
1,"I'm making an ESG stock analysis program in Java, and so far the only free ESG API I've come across is ESGEnterprise, but I'm having trouble retrieving the data. Has anyone had any success/have any recs for other ESG APIs out there.",API,datasets,ruvj9n
2,"Hey everyone! I’m one of the creators of Sieve _URL_ and I’m excited to be sharing it!
Sieve _URL_ **is an API that helps you turn petabyte-scale video data into a high-quality dataset, automatically.**
It helps store, process, and semantically search your video data. Just think _NUMBER_ cameras recording footage at _NUMBER_ FPS, _NUMBER_/_NUMBER_. That would be _NUMBER_ million frames generated in a single day. The videos might be searchable by timestamp, but finding moments of interest is like searching for a needle in a haystack. Sieve tags useful attributes like people, motion, lighting, etc on every frame!
We built this visual demo (link here _URL_ a little while back which we’d love to get feedback on. It’s \~_NUMBER_ hours of security footage that our API processed in <_NUMBER_ mins and has simple querying and export functionality enabled. We see applications in better understanding what data you have, figuring out which data to send to labeling, sampling datasets for training, and building multiple test sets for models by scenario.
To try it on your videos: _URL_ _URL_
Visual dashboard walkthrough: Click on our site link!",API,datasets,rup1uj
3,"Hi guys,
I'm a new here.
Recently I am trying to develop something using Python.
What I need is CO2 emission data.
Could you guys recommend me an API of CO2 emission data?
Plotting CO2 emission data on y axis and x axis can be countries of the year, months, years of the country something like this.",API,datasets,rrbl7b
4,"Hi,
When pulling historical data from API's I often spend a lot of time reading their documentation, and writing boilerplate code to pull their data in.
As an alternative, I've built an API where you can query historical data using natural language, e.g. `w2v.get_one` or `w2v.get`
Under the hood I connect with many data providers, and process their data so that each result is a single time series. I calculate sentence embeddings for each result to make them easily queryable.
Does this seem interesting to anyone?",API,datasets,rz0888
5,"Show off, complain, and generally have a chat here.
Discuss whatever you've been playing with lately. Share/ask for tips suggestions and in general talk about services/tools/sites you find interesting.
Here you can rant, go off-topic, or self promote even but please be civil.
P.S: Suggestions for this subreddit are always welcome.",META,datasets,shgekp
6,"We have built 180Protocol, an open-source toolkit for data sharing and creation of unique data sets. It targets enterprise use cases and improves the value and mobility of sensitive business data.
Our alpha release is live on GitHub _URL_ Developers can quickly build distributed applications that allow data providers and consumers to securely aggregate and exchange confidential data. Developers can easily utilize confidential computing  to compute data aggregations from providers. Input/Output data structures can also be easily configured. When sharing data, providers get rewarded fairly for their contributions and consumers get unique data outputs.
Read more on our Wiki _URL_",code,datasets,s9d3yt
7,"Allen Downey's python boks and videos are all excellant 
Here is a video tutorial by him on survival analysis
_URL_ _URL_
The notebooks
_URL_ _URL_
The dataset on lightbulbs he uses
_URL_ _URL_
And his twitter
_URL_ _URL_
I have no connection with him other than liking his work.",code,datasets,s0l6ml
8,_URL_,dataset,datasets,sg44o3
9,"This dataset contains daily and annual gold rates from _NUMBER_ and _NUMBER_ respectively. It contains rate of gold per troy ounce in six major countries' currency. The countries included are India, UAE, Europe, Great Britain, China, US). Time series analysis and prediction can be performed on this dataset. Any views or inputs are always welcome!! 
Link to the dataset: _URL_ _URL_",dataset,datasets,seo5yn
10,"Hello m8s
I crawled reviews of _NUMBER_ different chocolate bars as well as metadata and US and Canadian chocolate producers. 
The data is available here _URL_
Also, the simple crawler that captured this data is available here _URL_
As always, credits go to the Manhattan Chocolate Society:
> Manhattan Chocolate Society, Flavors of Cacao \Internet\]. Available from: [_URL_ _URL_ 
Happy new year;
Cheers",dataset,datasets,rux7cw
11,"
_URL_ _URL_
I have put up some  parse trees online as a dataset. Not something as substantial as Penn Treebank, since the trees are not human-edited, but still way more parse trees than from Penn to feed into your later-stage NLP algorithms, free of charge or hassle.
The current format is straight from where they were generated. Suggestions of alternative formats based on ease of use would be heavily appreciated!",dataset,datasets,s1dit6
12,Pls how can I get a student-bot conversation dataset?,dataset,datasets,s8vcbb
13,"Hi everyone,
Can someone suggest to me some real-world datasets for anomaly detection? I have surfed the web enough for this and I am looking for those unique datasets for specifc domain  which have some tested anomalies.",dataset,datasets,s6nol0
14,"Canadian Tire is one of Canada's largest retailers with _NUMBER_ locations, and at least one location in all provinces/territories other than Nunavut.
Dataset includes:
* Store url
* Location Name
* Street Address
* City
* Province
* Postal Code
* Country
* Longitude
* Latitude
* Region
* Collection Date
* Store Phone Number
* Store Services 
* Store Hours
_URL_ _URL_",dataset,datasets,sbx2uo
15,"As part of my Master's thesis, I was to work on a few novel research topics. The core idea behind it was to train modified sparse versions of RNN, LSTM, and GRU for sequence learning purposes. While doing the research, I found out that the paper that introduced LSTM _URL_ worked on Reber Grammar Sequences, and realized this would be a good dataset to work with.
However, I could not find a dataset of Reber sequences, and therefore, I went on to create one for my thesis. I successfully defended my Master's thesis with very good grades, and therefore, now I am free to make this dataset public.
I have uploaded the dataset on Kaggle at: _URL_ _URL_
Its corresponding visualizations are at: _URL_ _URL_",dataset,datasets,sa6ak0
16,"I am looking for dataset related to object detection to detect object after house/office has gone through fire. Do any one have lead on this kind of data are available or not , or any leads regarding this kind of system. 
It would be helpful if I get any lead on this.",dataset,datasets,s0m46l
17,"Hi r/datasets,
CEO of DoltHub here. We just finished our basketball database bounty called SHAQ. Here's the data if anyone wants to use it:
_URL_ _URL_
And here's the write up on how it did:
_URL_ _URL_
We're on to our next data bounty, US Housing Prices, _URL_ _URL_ if the idea of getting paid to build databases intrigues you.",dataset,datasets,s0ud6e
18,"Hi im trying to predict twitter use age through the tweet text that they use. Is their any dataset I can use? 
Thank you!",dataset,datasets,s9mlwy
19,"Hello 
I hope this is ok to post in here. I have been working on a product launch for a brand new traffic product that provides a deep level of traffic data granularity through connected car data. It covers the whole of the US with hourly, day of week, monthly and directional traffic data. 
We’re launching this on Feb _NUMBER_, _URL_ _URL_ join the event to find out more",dataset,datasets,sdawol
20,"This is self-promotional in a way, but it fits the philosophy here.
At dolthub we're promoting our bounties to build collaborative datasets. This month we're hosting a competition to build the world's largest open dataset for housing data -- tracking it down to the sale.
_URL_
Feel free to join us on discord: _URL_",dataset,datasets,ryk6zd
21,"I was linking the Manifesto Project](_URL_ dataset to the (_URL_ which is a very painful enterprise. Luckily, I found the [Party Facts project _URL_ They had already linked those two datasets and many others. I hope someone finds it useful.",dataset,datasets,sbve6d
22,im searching video games sales since _NUMBER_ - _NUMBER_,dataset,datasets,s2fjbk
23,is there any benchmark dataset with object bounding boxes and its attributes,dataset,datasets,s9966h
24,"I published a dataset of 5m source code files from 15k open source files. Its long term goal is to enable identifying causality in software engineering. 
Data and code _URL_ 
Describing paper: End to End Software Engineering Research _URL_
People from ML, NLP, causality, and SE, might find it interesting.
The dataset enables investigating code similarity , program difficulty, defect predictions, etc.
The code is extracted every two months in order to investigate the difference. By the difference one can investigate if a change in a possible cause  leads to influence  _URL_ in which context. 
I plan to keep extending the dataset and would like to get feedback on it - ease of use, new use cases, etc.
If you have related dataset that can be merged with, related data that you would like to obtain or ideas for research directions, please contact me.",dataset,datasets,rvupxk
25,"hello, I am taking a course at my university and want to find data for a project. 
I essentially want to find out what are the most popular videos on YouTube that have been related to motorsports .
It would be really helpful to have the name of the video, date published, number of views, number of comments.
Any help would be greatly appreciated.",dataset,datasets,scg3gs
26,"Scraped all public votes in German federal parliament/Bundestag . A total of _NUMBER_ voting sessions are recorded. For each of the voting sessions, the votes of each of the around _NUMBER_ parliamentary member are recorded by name of the member. Note that the voting is not strictly along the party lines. Available as excel files and zip:
_URL_ _URL_
Original source :
_URL_ _URL_",dataset,datasets,s0ypi4
27,"Hello! I'm sharing a dataset of metadata for _NUMBER_ TikTok videos, scraped between _NUMBER_-_NUMBER_-_NUMBER_ and _NUMBER_-_NUMBER_-_NUMBER_. All the data was publicly available with no login required at the time of scraping. The data is available as flat JSON, and as a MySQL database. There are probably minor inconsistencies between the two formats, but they should be _NUMBER_% similar. Everything in the JSON file is unaltered response from TikTok, the MySQL database is a bit more trimmed down.
Total uncompressed size is around 200GB
magnet:?xt=urn:btih:475ea4ba18becf5e5f54cd0200999c7c45674fe6&dn=tiktok-_NUMBER_%5F07-_NUMBER_&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80%2Fannounce
## Other Stats
In addition to the videos, there is metadata on:
- _NUMBER_ sounds
- _NUMBER_ challenges 
- _NUMBER_ authors 
## Credits
Thanks to David Teather _URL_ for his TikTok-API project!
_URL_",dataset,datasets,sfq1zk
28,"Is there a data set or data base out there that contains comments from social media such as tiktok or Instagram?
Ive just browsed google about it but I haven't found anything",dataset,datasets,rwel5l
29,"Hi all,
**Would anyone know a good multimodal healthcare/medical data set?** 
The purpose is to conduct a study on machine learning models trained on multimodal health data.
Many thanks!",dataset,datasets,sd8rbe
30,"I am trying to build an app which would give me product details when i search it by name.
Example: I search for Apple iphone _NUMBER_
I should get all the informations such as description, Image, etc with it.
Please let me know if there is some open source datasource available
Please note that this information should not limited just to electronics good",dataset,datasets,s3rbof
31,"Hi, what I'm looking for is pretty simple but I've been unable to get to it. I want to do a bit of a cohort analysis on covid cases. I want to know what percentage of daily covid cases are occurring among the:
* unvaccinated
* post dose _NUMBER_
* post dose _NUMBER_
* post dose _NUMBER_
* etc..
Ideally for the UK. Thanks",dataset,datasets,rvzq1k
32,"Hello!
I'm looking to find a dataset that shows what percentage of demographics voted towards either major US political party in the past election. I've seen something that looks like this _URL_ but I was wondering if there was something free that was put up by an official govt source. If anyone found something like this, please share! I'd really appreciate it :) Thanks!",request,datasets,s44007
33,"hi all any good leads on datasets for fuel prices ad fuel consumptions? at least till dec _NUMBER_
thanks",request,datasets,rtxpp1
34,"I've noticed that it's become quite difficult to find a good open dataset for real-estate data. Mainly the features I'm looking for are:
* Bedrooms
* Bathrooms
* Amount of space 
* Price 
Anything else would be bonus. Any recommendations appreciated! Thanks!",request,datasets,rvgaxy
35,Hello! I want to make a diagram on how much the world spends for what. I read that we spend as much money each year on tobacco or soft drinks as we need each year to close the biodiversity funding gap. Does anyone have a dataset that uses similar examples? Thank you very much!,request,datasets,rzeyww
36,"Hi, looking for a data set that contains global air pollution for the years _NUMBER_-_NUMBER_
could only find till year _NUMBER_
any help would be appreciated :)",request,datasets,sgbkzf
37,industrial process datasets for anomaly detection. live stream data or static past data.,request,datasets,sar2yl
38,"Hey all!
I am looking for as much data about global wine as possible. Ideally I can get as much of this data in geospatial shape files of the wine regions as possible, but data detailing them could be used to create the region. 
Any data sets that include information on producing countries/regions, varietals, producers/wineries etc are helpful. 
Likely this data will be in pieces by country or region but I am willing to combine the data into a complete data set. 
Production and export numbers/values aren't required but wouldn't be turned away.
I have found a few sources that have some of the data by have not found any opensource geospatial data so far.
Thanks everyone!",request,datasets,rt2fjt
39,"I am looking for one or more datasets to build a business dashboard. Something that allows me to create some KPIs and progress charts for sales, billing, production, etc.",request,datasets,scls6v
40,"Hi community,
I planned to build a machine learning models that can read text biographies and extract out certain attributes such as the degree and the industry a person is working in. This is for a postgrad thesis i am working on.
I thought this linkedin dataset would be perfect to train such a model:
_URL_ _URL_
Unfortunately, the link is down :( Would anyone have an archival copy of this dataset ? Happy to buy a beer for that kind soul",request,datasets,s492ru
41,Looking for a dataset of books sold on Amazon. Any help woild be appreciated. I would also be interested in ways to scrap the data myself.,request,datasets,rsjzyk
42,"Does anyone know where to find this data? I'd prefer to not license the data from places like schooldigger and not having much luck apart from state-specific websites—I need it standardized for all schools nation-wide. 
I've looked at _URL_ _URL_ and they have great demographic and geographic data, but I'm not seeing the assessment data . 
Thanks!",request,datasets,rs3ur0
43,I’m trying to look for a dataset about spotify music trend analysis that has a big dataset linked up and at least from _NUMBER_. A full analysis would be helpful as well but just like a dataset is good enough and it has around _NUMBER_ samples . Thanks,request,datasets,s6ymzt
44,"Hi, i am looking for datasets that give me a comprehensive list of car makes, models and any assisted driving features they have such as lane keeping, cruise control and blind spot detection. Do you have any suggestions for where to find this data?",request,datasets,rs0euz
45,"I want to see the data for how each member of the house over or underperformed the presidency results, is it possible to find that?",request,datasets,rsl44b
46,Could someone please tell me if there exists a dataset of segmented images of ct scan for lung cancer detection?,request,datasets,sbklrg
47,"I'm looking for a data set that meets the criteria named in the title  based on latitude and longitude. I would say that at least _NUMBER_ rows for North America is a must , so imagine a 10x10 grid over North America with some information about each square.
Preferable size _NUMBER_ rows, but anything above _NUMBER_ would do .
Most important is the type of terrain , mean elevation and temperature.",request,datasets,sa7goq
48,Hello everyone! I'm looking for a dataset consisting images of various sneakers for my thesis project. Is there anything you know of?,request,datasets,s5opid
49,"Hey, I 3rd year student and now I’m doing my business intelligence project. I am interested in METAVERSE tokens but I found only a few datasets. If you have some datasets recommendations, pls recommend me. THANKS!!",request,datasets,s5ugnf
50,"I'm looking for video of people entering/exiting buses. Camera mounted inside of the bus on the ceiling, and/or door frame, aimed down towards the door and floor.
General task is to count the number of people entering and exiting a bus. Labels are not needed, but helpful.",request,datasets,rw9yr9
51,"Good day and thanks for reading. 
Is there an api or source for drug prices around the world? I’d like it to be an api that gets updated but I’d settle for a dataset that is recent. 
Thanks !",request,datasets,rsx6jp
52,"Hello everyone! So I'm in the lookout for historical documents datasets, that are preferably annotated for object detection.
Most of what I can find is either non annotated, or annotated for text retrieval. But in any case, please share with me anything related to the topic, it might just be useful, thank you!",request,datasets,s1g4aj
53,"I'm looking for the average daily land surface temperature of the entire Earth, calculated once per day",request,datasets,rvnylm
54,"Hi everyone, 
I’m searching any datasets of sports matches, like : football, hockey, tennis, basketball, etc. where I can have all the detail and statistics of a game. 
Is there anyone who know where I can find these datasets ? 
I’m newbie in web scrapping but if someone know a method to build my own dataset thanks to web scrapping I really want to know",request,datasets,s7qicr
55,Does anyone know where to find a dataset  about art  created/assited by AI...?,request,datasets,s4b5no
56,"Hi guys!
I am working on a project and i need a marketing strategy dataset to both analyse the efficiency of a marketing campaign and to create customer segments based on the strategy they prefer. The ones i found all used only phone calls/emails and i'm looking for one that contains digital marketing as well.
Would really appreciate the help!",request,datasets,sb4uiy
57,Looking for a dataset for the songs used in movie trailers or anything somewhat related.,request,datasets,rseek1
58,"Are there any datasets out there of antlered big game? Specifically, I am looking for images of deer/elk/moose/sheep after they've been shot.",request,datasets,rw4yoj
59,"I submitted a post asking for spatial datasets yesterday, but didn't provide enough information for people to give really useful examples.
I'm looking for at least _NUMBER_ datasets which have the following properties:
- The two datasets cover the same region in part or in full
- The data in both datasets is in the form of individual points
 - Not aggregated over large areas ex: states/counties or grids
 - Either the existence of a point in a location is the entirety of the data provided by a point, or a variable gives the value at each point
- The data should satisfy assumptions for Kriging
 - Data will be used to demo a framework in R  which performs Kernel Density Estimation, Kriging, and various forms of interpolation on two datasets and compares the resulting models
An example of the type of data I'm searching for would be a dataset containing the address  at which a crime occurred, where each point denotes a crime. ",request,datasets,shk608
60,"Hey everyone, I really appreciate the awesome community that all of you have built here together. This is my first post here and it is extremely important. 
I would really appreciate, if you could help me find EEG datasets. 
P.S. Please send for EEG, and kindly avoid iEEG.
Thanks in Advance.",request,datasets,sezs5d
61,"Looking for a data set that contains game by game attendance data. ESPN only provides season totals/averages, and I am having a hard time finding a data set that provides the game by game data. Can anyone help?",request,datasets,s7u1d2
62,I have only searched of this on google but I haven't had any luck,request,datasets,rwfv9i
63,"Hi there, I’m starting a blog and I am writing a piece where data on Twitter usage by US state would be incredibly useful. As recent as possible, I am seeking a state by state count of active Twitter accounts.",request,datasets,s0cav7
64,"Does anyone have any recommendations for how I can get hockey equipment data in North America? The data would be used to calculate market share for sticks, skates, helmets, protective equipment, and goalie equipment by brand and by geography .
 
**If you have good sources / data providers that would be able to provide information on an ongoing basis that would be great. One example I'm aware of is Nielson, but I'd have to look into if they offer this information.**",request,datasets,rxsq4a
65,"Hello 
I hope this is ok to post in here. I have been working on a product launch for a brand new traffic product that provides a deep level of traffic data granularity through connected car data. It covers the whole of the US with hourly, day of week, monthly and directional traffic data. 
We’re launching this on Feb _NUMBER_, _URL_ _URL_ join the event to find out more",request,datasets,sdazfk
66,"Hey,
There's plenty of datasets online that give stats around football games and they tend to go as granual as goals, cards, penalties and subs.
I'm looking for one that goes as far as to show every event, including passes, shots etc. Similar to you see on real time betting apps like bet365, flashscore
I don't need real time data so don't want to look into 3rd party services that offer this. Just a historical dataset of a league would be enough.
Any help would be greatly appreciated.
Thanks",request,datasets,s66j37
67,Looking for something that I could do ??? to in order to have some kind of comparative study of idioms... if for nothing else a new look at things.. thank you!,request,datasets,rrovod
68,"Hello everyone. I am looking for spatial data which tracks police use of force. A great example of what I am looking for is the NYPD Stop, Question, and Frisk data, which lays out firearm use, taser deployment, and OC spray with XY coordinates for each calendar year. Unfortunately, recent reports have uncovered that around _NUMBER_% of the data are missing, making it unusable.
Anyone ever come across a similar dataset?",request,datasets,s1h8cp
69,Hi may I please know whether it is possible to find datasets about Singapore’s transport system where many deep insights can be found?,request,datasets,s1alaw
70,I’m making a machine learning app for a school project where users upload a picture of possible bed bugs infestation and I want to create with tensor flow lite but I can not find a dataset can anyone help me :/,request,datasets,s0miot
71,"I'm looking for a dataset that contains a list of Jackpot wins that occurred in Las Vegas over time. 
It would look something like this but updated: _URL_ _URL_
I googled and found a bunch of articles, but wasn't able to find a list of wins.
Does anybody know where I can find such dataset? Please and thank you!",request,datasets,rzal4s
72,"looking for data sets on Missouri genealogical data birth, marriage, death, burial, etc. Any time period.",request,datasets,ryufpf
73,"Hi all,
I am looking for a dataset that consists of cities and their time zones. I am working on a fraud call dataset with fraud call times in GMT for all cities across several hundred countries. I want to convert the fraud call times into their local times and analyse if I can find any pattern. I really appreciate any help you can provide.",request,datasets,sbdu5w
74,"Hello. I'm doing project on tea bud identification and tea leaf disease identification. I've been searching for datasets for past few weeks & mailed few authors who made papers on this topic, unfortunately I can't find anything related to this.  Would really appreciate any kind of reply. Thanks in advance.",request,datasets,s7p1z8
75,I was wondering if anyone could find a report about Covid-_NUMBER_ or anything related to it that was published before December 30th _NUMBER_. I am putting together a timeline for something for college and was wondering if any article predates that. The earliest I could find is December 30th _NUMBER_.,request,datasets,rtzqsh
76,"where can I find one? I prefer to be in Kaggle, I just need a dataset to make a project about panel regression, I still new in DS and can't get data from big sites, need prepared dataset. thanks",request,datasets,seaen3
77,"Pretty much the title. I'm very new to this but have this deliverable to the client about providing a dashboard for fundraising. I just want to use any made up sample data but a bit extensive and with a few metrics. I looked on kaggle, data world but even if i find something close to fundraise it's just very different from what I'm looking for. I'm probably not searching correctly but please help me if you can find any dataset for this. Also, it should not be paid :)
Thanks!",request,datasets,s0u0vo
78,"I found the Healthiest Communities data and I would like to use it for a project looking at disparities in all counties across the US. It looks like I can easily download the top five hundred scoring counties, but I’m hoping to get the full data set. Does anyone know where I can find that, or a contact to submit a request?
Edit to include link to what I’ve found available: _URL_",request,datasets,sh3tdh
79,"I was wondering if there were any datasets floating around on player count for games in the pro circuit - up to date as of _NUMBER_, ideally with demographic information included - but just player count by month/day would be good enough.
I'm writing a paper on how gaming participation affects stadium attendance of major gaming tournaments etc.
Any help would be appreciated!",request,datasets,rv7v65
80,"Any recent dataset on US crime rate per state? Could it be found as government open data? Where would you go for this? 
Thanks!",request,datasets,sc3ko3
81,"Hello, I would like some assistance with regards to ordering data for orders. I have a screenshot of said data which I will try to upload in the comments section.
The issue is:
-orders are collected in column A using Product code numbers.
-data in Column B and C correlate. Column B are the product codes and column C product description for all the products in the store.
-evidently, not all products are ordered.
- I need to link the product codes for the products ordered to the corresponding product description in column c
Of note:
- these are not the actual products ordered. Just an example
- the data cleaning will be required for the past _NUMBER_ years
- some of the product descriptions and codes have been updated and are thus missing in the columns b and c. 
- this is only an example. The actual excel files contain over _NUMBER_ rows for orders placed and _NUMBER_ for the product code and description references.
 
Is there a simple way to clean up this data rather than doing it manually? I’m quite new to this so I would be very grateful for some guidance.",request,datasets,s96lzd
82,I am doing a project which requires the use of a dataset that has the origin and destination of people going to work. I was originally going to use LODES but I found some problems with it when looking at it in just specific parts of states. I found that most Census blocks did not have data. Are there any alternatives you know of or other suggestions?,request,datasets,rucfwq
83,For my dissertation im looking for datasets related to the sharing economy. Please help me out!,request,datasets,sclac3
84,I want to get started working with data viz in the new year and I've always loved animals so this would be a fun project to dip my toes into the water. Any datasets that reflect what I'm looking for would be awesome,request,datasets,rtpls9
85,I am looking for datasets for Aspect based Sentiment Analysis. I already know of the Laptop and Restaurant datasets but was wondering if there are any others,request,datasets,rtwktm
86,"Looking for a dataset of car and electric car sales, preferably a minimum of the last _NUMBER_ years. Purpose: to draw a training dashboard. Could be useful parameters: production volumes, shipments, sales, prices, buyers, countries, delivery times, etc.",request,datasets,rtbkb2
87,"I've been looking for a NASCAR dataset that has per-race stats for every NASCAR season that is normalized an mostly ready to go for a database. I'm looking for a dataset that at least has the NASCAR Cup series, but a dataset that has Xfinity and/or Trucks would be more valuable.",request,datasets,s778my
88,"Hello , I am looking for dataset for vibiration readings of servo motors detailed its health  to use in a predictive maintenance model and I would appreciate any help regarding this matter, thank you in advance .",request,datasets,s4ejxc
89,Looking to conduct a research on minority owned banks amd their effects on surrounding areas and their people. Need data on the county level to do a DiD regression.,request,datasets,ru8tpy
90,"Title. Is there market share or sales of total hybrid vehicle sales anywhere? If not, is there regional data?",request,datasets,rxr0xc
91,"I am looking for Infrared Scans and data sets. Ideally from arial photogrammetry and have radiometric and GIS meta data associated with them. We are developing software for disaster management, emergency services and SAR. location doesn't matter but content would be ideal the more interesting the better example) scans of animals or humans, wildfires, urban and rural areas. 
Any leads I can get the better we and sort through bugs in the software.",request,datasets,shdcx5
92,"phonemes are just sounds that make up words, for an open source speech to text library I’m making! Thanks.",request,datasets,s82xiv
93,"Title says it all. I am doing some research into how the human pose changes when a person is concealing a firearm. So, I am preferably looking for a dataset of people concealing their firearms. I need a full-body photo or video footage of this. Because I'm using pre-trained pose detection, I don't really need any specific labelling, preferably just something to say whether they are carrying or not.
Thanks in advance!",request,datasets,s0ferq
94,"I'm looking for datasets related to mental health, ideally tweets  where users self-reported their diagnoses  or share that they attempted suicide, then including these users' tweet histories. That means I'm looking for datasets with users annotated with their mental illness. 
",request,datasets,sa42im
95,"Hello all, help needed or at least a push in the right direction for statistical data on pleasure crafts community .",request,datasets,sg93sb
96,"I’ve got an exciting idea for my capstone. Some friends and I are planning to renovate a garage and add an ADU. For the capstone, I would like to design a model which can predict building costs . 
I’m looking for data but coming up short. I’ve looked through Kaggle with limited success. Also, I’ve found some papers on Google Scholar describing different techniques of estimating building costs, yet I’m having trouble getting ahold of any actual datasets _EMOJI_ 
Do you have any ideas as to where I might search? 
Another exciting idea I’m considering is building a web scraper to collect the data. Any thoughts on this? Would it be too wide of a scope for the project? 
Not sure how to move forward exactly and any help would be great. Thanks!",request,datasets,sbyub2
97,"The idea is to determine impossible travel given two countries and duration.
Edit :was able to obatin the data using geopandas",request,datasets,rwio84
98,"Hi,
I’m part of an art group from Switzerland currently studying at HSLU Design & Arts (_URL_ _URL_
The group consists of:
Karim Beji (_URL_ [_URL_ _URL_
Emanuel Bohnenblust (_URL_ _URL_
Lea Karabash (_URL_ _URL_
Yen Shih-hsuan (_URL_ [_URL_ _URL_
At the moment, we are working on a project on the topic if AI can augment the happiness of humans. To answer this question, we are mainly working with chatbots. The end result is going to be an exhibition at the end of March. 
For that exhibition, we want to conduct a trial in which people from over the world chat with a chatbot to find out if and how it augments the mood of the participants. 
We would give you access to a GPT-_NUMBER_  chatbot and ask you to a) record yourself through a webcam  while you are chatting and b) simultaneously screen record the chat window. 
In the exhibition we would have a) a book with all the chats and b) small videos with your faces  to assess your mood. 
We would have a Zoom meeting beforehand to discuss everything.
Looking forward to your message!",request,datasets,sgyqay
99,"Hello, 
I'm looking for a large dataset  of pixel art character, but the characters need to have the same shape concept , i would even prefer if the image was only of the character's face. 
Does anyone knows any datasets or websites that would suit my goal ?
Preferred size would be 24x24",request,datasets,sd7vo9
100,"Hey guys,
does anybody have a dataset for aircraft price. It doesn't matter of used or new onces. I have just a dataset with small aircraft, like c172 or piper... But I need a price/cost dataset with large airplanes like _NUMBER_, _NUMBER_ or older aircraft types. 
If anybody can help me find remotely similar it would be a huge help. Thank you!",request,datasets,rssq7r
101,"Can anyone point me to any barometric time series datasets that have a high time resolution? Ideally, there would be at least _NUMBER_ data point per second. I *might* be able to get by with _NUMBER_ minute intervals, but this seems a bit coarse. I can't seem to find any archived historical data like this.",request,datasets,sb06hr
102,"Hey everybody,
Looking for a US mortgage default/foreclosure rate dataset by US county broken down by year from _NUMBER_ through _NUMBER_. 
Any help in locating this dataset would be greatly appreciated.",request,datasets,s5nwxb
103,"I need the dataset for AI training. Preferably, each song should be more than _NUMBER_ minutes long.",request,datasets,sd73gi
104,I need Electronic components identification and function dataset to be used in relational database,request,datasets,rrqj1k
105,"I'm working on a project that needs data of air conditioners' electricity consumption under various conditions. 
So far, I found a dataset on Kaggle _URL_ for Chillers and it's great. But I couldn't find any for normal split air conditioners or VRF systems. 
Any ideas where I can find some data? Thanks in advance.",request,datasets,rwev88
106,"Hi there, 
I am a total newbie and this is the first time, I am trying to create a Business Intelligence and Data Visualization portfolio. The portfolio will consist of _NUMBER_-_NUMBER_ projects and it will be hosted on a website. The project pages will then be linked to LinkedIn. The goal is to then apply to freelancing websites like Upwork.com.
I am looking for datasets which I can use to create porftolio projects where I can show off my skills. I am looking for a website where I can find such datasets where I can choose one from. If someone here has a dataset I can use that would be great as well.
If I had to narrow down my interest, I would say that my primary interest is providing small businesses business intelligence services as a freelancer. So a dataset of a small business where I can create a dashboard along with a business intelligence report is the best fit.",request,datasets,ryecov
107,"I'm searching for a huge quantity of well-written sentences , that preferably don't have factual information such as dates, names or events. So potentially they could be merely descriptive, and if possible, available in languages such as French, Spanish, German or similar.
Thanks in advance.",request,datasets,rv957a
108,"Need sample data for timekeeping. Specifically time in, time out, on/off lunch, durations. 
TIA",request,datasets,rwrwtc
109,"As the title says, I'm searching for a dataset that has the digitized line items from receipts. When I search for receipt datasets I am only able to find images for training OCR models but I only need the actual text from the line items. This would be the same data one would get from using an OCR to scan the receipt but all I need is the line item text along with its price.
TL:DR 
I need the results from using an OCR not the data to train an OCR from scratch",request,datasets,rxjrg1
110,"I know the NCDC has a pretty robust selection for hourly and monthly data, but are there datasets with more frequent readings out there? The only option I've found is here _URL_ but the locations are pretty limited.",request,datasets,sdj6vs
111,FSOCO or any other Formula Student dataset with FSG/FSE styled cones for driverless,request,datasets,s55itk
112,"I'm preparing for a PoC to create energy building mangement system for industrial buildings. I'm looking for a time-series dataset of production facility / factory. Ideally, data from the building would be divided into sections / floors, where each section / floor has it's own measurements taken. 
I searched open-source datasets available online but haven't found anything of that nature. Does anyone know such publicly available dataset?",request,datasets,rtntc6
113,"Hey, I am doing a statistics course for my MSC and I am wondering if you guys could point me in the direction of species abundance data for at least _NUMBER_ sample sites, and the abiotic data for each of the sample sites. I greatly appreciate any feedback you guys have!",request,datasets,shl078
114,Anyone know where I might find a dataset for wheat yields by province in Afghanistan for the past _NUMBER_ years? Bonus if other grain crops are also included. Thanks.,request,datasets,sekvh4
115,"Hello,
I'm looking for a dataset with main insights already found but badly visualised, does someone have one ?
Thanks a lot",request,datasets,s3bf3m
116,I'm currently pursuing Master's in computer Science and engineering and my area of research is Technologic Innovation in Hybrid Education and learning. Now to test my hypothesis I require student dataset of students who are part of Hybrid Education based model. Any idea where can I get this kind of dataset or how can I proceed to get the dataset?,request,datasets,rte16w
117,"Hey guys!
I was searching for the ozone layer data for the pst few days and I couldn't find any open source reliable data that have the data updated after COVID!
Most of them stopped before it and no updates after that!
Could any of you point me to a good place to start my search because I think I'm searching the wrong way here.",request,datasets,sdt6f2
118,"I’m looking for data on sunlight levels by U.S. states, preferably also by county. I’ve only been able to find data from >_NUMBER_ years ago so I’m not sure if I’m looking in the right places for this data",request,datasets,s6rkod
119,"I'm working on research for datamining spatial datasets, and I've been butting my head against a wall due to a lack of good data. What are some good publicly available spatial datasets? I would prefer large datasets, and either datasets with multiple variables or  multiple datasets covering the same area.",request,datasets,sgr03o
120,"Hi,
I'm looking for a place where I can download for free as many datasets as possible about pretty much any subject imaginable. Preferably datasets that come in CSV or Excel files and that can be downloaded in bulk, instead of one by one... Any suggestions?
Thanks in advance",request,datasets,rzod4w
121,"I am looking for a dataset that has mapping from job titles to skill keywords required for the job. I have already explored ONET so, looking for something else.",request,datasets,rs10qz
122,Hi. Where can i find _NUMBER_ years worth of data for housing prices in canada by province?,request,datasets,s7yf5r
123,"The data set must have:
* _NUMBER_ or more predictors, and
* _NUMBER_ response variables to be predicted
* The response variables must have some sort of shared dynamics
Not sure what to be looking for, really! Kaggle has a lot of data sets but I'm not sure if any would fit the aforementioned requirements. I'd appreciate it if someone could suggest data sets.
Some topics that sound interesting: climate change, sales forecasting, weather forecasting,.. though I'm not sure where to find these data sets and if they fit the _NUMBER_ requirements above.",request,datasets,sggoos
124,"I'm working on an app for a client of mine and we need an updated list of prescription drug prices. I have contacted GoodRx but they won't work with ventures that don't currently have a user base. 
My client is a doctor so I'm not sure if we could use that to get access to some data.
Any help or info to point me in the right direction is appreciated.",request,datasets,sbm5ex
125,"I need for a project to make a wrong statistical analysis of a dataset that looks like its conclusions are true . I mean, for example use a simpson paradoxe or a wrong interpretation of p-value to fake prove something. Have you got any fun ideas ?",request,datasets,s3tnbk
126,I am looking for a font recognition dataset in Arabic to recognize the font of a given text image. Is there is any dataset for this task,request,datasets,s4uhwg
127,"Does anyone have a suggestion , as I am looking for a data set of venture capital firms and accelerators ? specially ones that work with minority entrepreneurs ... thanks",request,datasets,rvb350
128,"Including store names, etc...
Lots of info here, but not at address level: _URL_
I've found this: _URL_ but there's lots of missing data...
Another incomplete dataset: _URL_",request,datasets,rwruo1
129,"Need help creating a data set I’ll be using. analogies below to help avoid industry specific jargon usage:
I have a group of friends who all have different food preferences and budgets . I need to create a dataset for their preferences so that if on a given night I say I’d like to go get Italian and pay $_NUMBER_ I would be able to pivot the data and see whose budget I’m within and who likes Italian food and who dislikes Italians food.
The problem is I can’t figure out how to organize the preference section of the data set. While min and max price are easily added as a single cell in a spread sheet for each person. Some of the individuals like _NUMBER_ foods and dislike _NUMBER_. What is the best way to organize the data so I can easily analyze the data set to figure out which individuals I would and would not like to ask to go to dinner with.
Any videos or help would be greatly appreciated
Thanks for any help that comes my way",request,datasets,rx8aua
130,"Hi Everyone. 
I’m new to data analysis and was wondering if someone could give me some tips for a specific problem. 
I have a small dataset of venture funds that invested in different companies. I’d like to find out which funds are most similar to each other and which companies were invested in by funds who are similar to each other. 
I’m not sure how exactly to get this information. I believe I’m trying to find a qualitative standard deviation? 
The closest thing I could find online is the following: _URL_
Unfortunately I’m not sure if this is what I’m looking for and how I would even apply it to my dataset. 
Please let me know if this issue is similar to something you’ve dealt with before. Thank you!",question,datasets,ruf6k6
131,"At my old job, it seemed like I'd have a new project with a new dataset every few weeks. The hardest part of my job was understanding the data not completing the project. 
Last year, I built a data catalog using the no-code platform bubble and shared it here. We ended up with quite a few people testing it out and using it on personal projects. In the last _NUMBER_-months, I took the original platform I built and leveraged some open-source platforms like Amundsen to rebuild a modern data catalog focused on making data documentation transparent, collaborative, and straightforward for anyone or company. 
We have a sandbox environment with dummy data that we're looking for user feedback on. If anyone is interested in giving it a spin, please let me know! We're planning to release a public version for anyone to use early next year.
 
Happy New Year, and I appreciate anyone willing to give it a try.",question,datasets,rrblgb
132,"Hey everyone,
I am wondering if anybody might know of a dataset that contains non fatal car accidents incidents for each U.S. states. I know FARs is a good source but all those accidents resulted in a fatality, and I am wanting to do some analysis on non fatal accidents. 
Any suggestions would be greatly appreciated.",question,datasets,s936qg
133,Just wanted to know what are the best tools available in the market for web scraping including both the free and the paid ones.,question,datasets,rr71t4
134,"Hello everyone, I am new to NLP and i am currently working on training a custom NER model. 
Can anyone give me advice on how to set it up so that any new data that is being scraped gets annotated automatically and stored in the database?
I am using SimpleTransformers library for the training part
Thank You very much in advance.",question,datasets,rsnqps
135,"I am currently looking for county-level economic data that contain information like the unemployment rate, income, GDP growth, etc. Any suggestions would be greatly appreciated.",question,datasets,sekogt
136,"Hi, I need recent Instagram dataset for mi University project, Someone can to help me?",question,datasets,s03zmg
137,"im looking for a dataset that can give me insight on where car accident impact happens most. ie. are they head-on, rear-ends, side-swipes, etc.
i know Tesla is collecting a bunch of data about this but I don’t think it’s available to us.",question,datasets,rvys9b
138,"I am taking an IT class right now and I'm trying to figure out what data normalization is in mircrosoft access in SIMPLE terms 
What is the difference between 1st, 2nd, and 3rd normal form?",question,datasets,sbvuyz
139,Would greatly appreciate if anyone could help me out in how to map any of the ORBIS company IDs  to an industry classifier .,question,datasets,s5o845
140,"I am looking for a public domain dataset with astronomical images of stars. They doesn't have to be fancy or high resolution, I just need original photos of stars and the name for each star for a computer science project. Can you suggest me such a resource?",question,datasets,saqm4f
141,"Preferably from some scientific or technical background.
Thought of the UCI Reuters Newsgroup dataset. But it is too well-known and onlay available in a strange XML format.",question,datasets,ryagsp
142,"I have a graph that shows average IMDB movie rating vs. runtime. It shows that movies that are _NUMBER_ minutes long are generally rated lower than shorter and longer movies.
I have some questions I would like help with:
* Is this relationship helpful for movie producers to use?
* What factors could be influencing this result?
* What sort of actions should be taken for producers to improve how movies are received by audiences?
* What sort of analytical approach should be tried to dissect this result?
I would like to get your thoughts on how you approach this problem.
Graph: _URL_ _URL_",question,datasets,sf9zf2
143,"Hi there,
I am grabbing the table below from data.census.gov _URL_ 
 Census - Table Results _URL_ 
I'm noticing a lot of my data in ""meaning of sex code"" and ""meaning of race code"" equals ""Total"". I'm struggling to understand what this actually means. Does anybody on here have experience with this survey and could guide me in the right direction?",question,datasets,s3xuep
144,"Hello everyone,
I am searching for covid _NUMBER_ datasets. I have found some sources which had what seemed very granular and rich datasets . However, and after analysing many of them, it always turns out that a lot of data is missing
Can you guys share the most complete dataset you have handled so far ?
Thanks in advance",question,datasets,s0gycg
145,"I'm hoping it's acceptable to ask this question here.
My organization has a dataset of about _NUMBER_ records. We'd like to build a system where the data can be updated by multiple  parties, and replicated among all subscribers. We're concerned about data loss, data precision loss, and mid-air collisions, etc.
Aside from a centralized storage/API for the data, are there any established solutions for distributed storage and replication among multiple subscribers that would account for versioning, loss prevention, and replication?
I hope this question makes sense.",question,datasets,s1vbc5
146,"Hi! I aim to make an statistics with the results of natural and anthropogenic risk factors and develop a hierarchy of risk factors to identify the places where each risk factor poses low, medium and high threats. Which statistical test would you recommend me to do? Thank you!",question,datasets,sa8vzp
147,"Hi 
 Where could i found a dataset for cereal disease forcasting ?
Ps: satellite imagery 
Thanks",question,datasets,s96ca7
148,Hi! I am trying to figure out if there is any data provider that maybe I could use to find targeting details that a specific company uses for their ads?,question,datasets,s1997c
149,Have there been any other studies done that are similar to the GSS?,question,datasets,sec7gc
150,"I've recently started using scrapy and selenium to build my own datasets, but I've been having trouble finding websites that actually report or contain the raw data. Most websites provide just a generalized statistic of the data, which I would rather not use. 
One good website that I found was _URL_ _URL_ but I haven't found any others like it. If anyone knows of good websites I'd greatly appreciate it if you shared them.",question,datasets,rqintr
151,"Hi all, I'm sorry if this isn't the right sub for this.
I downloaded the PRC Data Breach Chronology _URL_ .csv dataset which I'm planning to use for my Master's Thesis. The dataset features a column named ""Description of Event"" in which, as the name suggests, a text description of each incident is provided; unfortunately, part of these descriptions are not properly delimited by quotation marks, which means that every time there's a comma in an unquoted text description the column breaks, which in turn makes the dataset unusable for my purpose. This happens whether I open the file in Excel or in RStudio.
I'm fairly new to data analysis and have really little experience in handling csv files so I can't come up with a viable solution other than manually removing all the corrupted rows using Excel , and I can't seem to find any solution via web search.
Is there any other way to solve this issue?Thank you all in advance!
EDIT: here is a screenshot of the problem I get for more clarity. _URL_ _URL_",question,datasets,rs62t3
152,"Is there any research out there that shows the average age people with lower income have kids, or that shows the average income of people who have kids under a certain age, like _NUMBER_ ? Are there any resources for finding something like this out? I imagine I could pour over census data but I wouldn't even know where to start with that. 
Any help would be appreciated, thanks in advance.",question,datasets,seose3
153,"How to find monthly google searches for a specific keyword I have seen people say to use google keywords planner but that needs an AD Campaign I am looking for something else which is free
I just want to make something like the most popular anime from the last _NUMBER_ years for just a fun side project, the video on youtube which you can find by searching ""Most popular anime from _NUMBER_-_NUMBER_"" has instructions on how he did it in the description but I am unable to find it Thanks",question,datasets,rrt8xd
154,"Hello friends,
I need help. I am currently using Europeam Values Study data set  and i did crosstab for two variables - country code and political party support.
The problem is that i have been given all the countries and all the political parties.
I would like to sort varibles in a way that i see only a specific country and the support for the political parties only in that country.
Thank you im advance",question,datasets,s4iglw
155,"Hi all,
I am looking for a dataset containing phone calls recordings, preferably in English. Does anyone know of something public? 
Thanks!!!",question,datasets,rr1ip4
156,"Hello everyone,
as per the title i am trying to find some useful datasets from the automakers containing data such as production levels , money spent on research, different costs and so on.
Does anyone where i could find such dataset? 
Thanks for the help!",question,datasets,sbu2bt
157,"Will I ever need to know networking, information security, java, or web development in analytics? I have a professor who told me that you never know if they could come in handy or not, but I would prefer not to take them if they aren't going to help me out in the future. 
\`",question,datasets,sf1qqx
158,"Let's say there is a poll that _NUMBER_ people voted on. The
topic and choices with votes in parentheses are as
following:
Which brand of TV is better?
_NUMBER_. Samsung 
_NUMBER_. Sony 
_NUMBER_. Same thing 
How would you interpret the data?
_NUMBER_. ""Samsung"" would be the answer bc it had the most votes
OR
_NUMBER_. ""Same thing"" be the answer be it encompasses the two
individual options in a singular choice and the two
individual options  are fairly evenly distributed
Side question: 
How does choice _NUMBER_ affect votes? Couldn't I
have drawn the conclusion they are about the same just from having choices _NUMBER_ and _NUMBER_ polled?",question,datasets,s9nc3y
159,I have details about patients in a csv. Each row has data about a separate visit. How can I group all the visits by patient so that all their medical history is available from that? I want to use the dataset for an ML model to see if the model can predict a disease before the doctor diagnoses it. But I'm confused about how the input data should look if each row had details about a separate visit and the number of visits differ by patient. Have you seen any examples of what I'm trying to do?,question,datasets,sgc7or
160,Do you think there is a market for building off the shelf computer vision datasets? If so what areas are the most in need?,question,datasets,s37rdn
161,"i am coding a budgeting applet that takes data from a users account activity. so far, only .csvs i have found online have super vague transaction titles such as “purchase” “insurance” and i was wondering if anyone had something more specific? such as individual store names or spending categories.",question,datasets,s0thrl
162,"I've got a good custom scrapper in python for the rental market in my area but I've been procrastinating on finishing the project. I need to validate every post manually and compare it to similar posts to flag reposts 
Is my only option to write a flask app if I want a nice visual data browser to go in and validate all my data? I already relate all similar posts during the scrapping so it just needs to display _NUMBER_ post at a time from the list that have a false in the manually checked col. I need to be able to edit all the fields and view the saved pictures and it needs to display the list of similar posts somewhere on the screen with just the relation confirmed col editable.
Right now the stapler saves to sqllite and the pictures col is a relative path/name but that can be changed if needed.",question,datasets,rrzxs8
163,"Hi community :)
I don't know if this is the right subreddit, please excuse me, I'm new in the hole data game. 
I'm searching for some cost of capital data for my bachelors paper. 
Do you know a website with some data of companies from _NUMBER_ to _NUMBER_ or longer. 
It's really hard to find something, but may I have better luck with good old reddit advise. :)",question,datasets,rw0qiz
164,Does anybody know of a publicly available and easily accessible data source that has demographic information  on NBA players as well as their on-court statistics? I know basketball reference is a really great source for player stats but I'm not sure it has the demographic information  that I might need.,question,datasets,s04039
165,"Hi! I've started a  course in data analysis, and the final assessment is a project requiring ""real world data"". I'm honestly not sure where to start looking for what I want .
Is there a FAQ/list of popular data sources? I don't necessarily need it to be free, but I'm not a millionaire either, so go easy on me :)
Thanks!",question,datasets,shbzwe
166,"Is there a gov source  to get the list of businesses registered ? Looking for U.K, U.S and Australia.",question,datasets,rvwtn5
167,"So my vacation plans fell through, I was supposed to travel but because of COVID, I've had to cancel. Which means I have _NUMBER_ weeks to do absolutely nothing. I want to occupy myself. I'm not much of an idea guy, but am a seasoned engineer.
I want to work on building either an API or a dataset. Any field, the only requirement is that is it be useful to someone or some people. I'm not asking to be paid either, completely free.
So if you have any ideas on what might be worthwhile or useful to work on.. or even just lots of fun, please let me know. I have way too much time on my hands. Thanks.",question,datasets,rzl8ye
168,"I am trying to run reports from this data source: Census - Table Results _URL_ 
This is data to tell the race and sex of business owners in a certain region. My struggle is that Race and Sex columns include a lot of data results that are ""total"". What does this mean? Anybody have any experience with this or census data in general?",question,datasets,s3y6du
169,"First time poster in this sub, and I'm only an amateur with data compared to many.
But, want I want to do is figure out the partisan lean for each of the new state house districts in Texas. This type of analysis is easily available for the federal districts at sites like _NUMBER_ , but I want to do it at the state house level.
Then, as a second step, it'd be great to have the correct data to draw my own state congressional districts and derive partisan leans . But, this would be a much larger project for the future.
Anyway, I'm struggling to figure out where to even start. I could get precinct data from the MIT data lab, but state districts break up even precincts, so they wouldn't be totally accurate.
Thanks in advance for any thoughts or help!",question,datasets,rxmkvq
170,"I need dataset with multiple tables to do query optimization analysis and write article about it, including datset characteristics and step-by-step approach. Can't find one. 
Thanks.",question,datasets,rs4wol
171,"Hi guys,
I am doing the review questions from Thompson: foundations of behavioral statistics, chapter _NUMBER_, question _NUMBER_. I cannot seem to conceptualize the correct answer. Would appreciate any help as I need to understand this review for the final exam at the end of term :)
""For a given dataset, how will using mean substitution for missing values impact the coefficient of skewness for data that were initially skewed? How will using mean substitution for missing values impact the coefficient of kurtosis? How does mean substitution differentially impact post-substitution means, SD, skewness, and kurtosis?""
Thank you!",question,datasets,s8w054
172,"I am following this great tutorial to learn how to create my own **sentiment analysis** pipeline using Python **SpaCy** package: _URL_ _URL_
This tutorial uses the ""Large Movie Review Dataset"" for training & test sets. Where do I find more datasets for different applications?
I need to analyze the sentiment of texts that users highlight in a **diabetes** brochure and website, to see if I can predict from the highlighted text whether a user is a diabetes patient or only just diagnosed or a friend or family member of a patient, etc...
What dataset should I use instead of the movies dataset used in this tutorial?
Thanks",question,datasets,sdt0jx
173,"I spent over a month creating a hand cropped and labeled dataset with _NUMBER_ images and _NUMBER_+ train labels from an 80s Anime. Ultimately my use case had too much error to even be close to useable at that scale , but maybe all of that work could be useful to someone for something simpler .
My question is how strongly does Reddit  defend Fair Use? The images are from a copyrighted work, but would likely be classified as fair use by any rational person. .
Also related, if the site won't be overly skiddish, does anyone have good file hosting recommendations for the total file size of _NUMBER_.7GB  and _NUMBER_.6GB ?",question,datasets,se9yie
174,"Just thought I'd share this Goodreads dataset here _URL_ It took me quite a lot of internet sleuthing to find an interesting, complete and large dataset to practice machine learning and more specifically recommender systems.
This data was originally pulled from Goodreads in _NUMBER_ by Zygmunt Zajac . It contains detailed metadata information for **_NUMBER_ books** , as well as _NUMBER_ million individual numerical ratings collected from _NUMBER_ users. There is no demographic information available for users, but the different files included in the release form an interesting basis for a recommender system.
I have released an expansion pack of sorts for this dataset, that adds book descriptions, genres and other features, enabling the use of various NLP strategies. **See here for the augmented dataset. _URL_ Cheers.",resource,datasets,sa9vuu
175,"Hi there, i currently developing ai chatbot, but i finetuning on clear data, my chatbot doesn't support nsfw topics, so i am searching for nsfw chats. If u have some parsed chats of humans it could be usefull too!",resource,datasets,ru7owv
176,"The dataset provides information on games that have been prohibited by various countries for a variety of reasons, resulting in governments making political decisions and establishing strict regulations against games that involve violence or violate religious or cultural feelings. This is a list of video games that have been prohibited or outlawed by various countries throughout the world. Governments that have outlawed video games have been condemned for increasing digital piracy, reducing commercial prospects, and infringing on people’s rights.
Click here to visit the dataset _URL_",discussion,datasets,s1d4om
177,"I collected news articles over the past _NUMBER_ years. Currently I have about _NUMBER_ million datapoints.
I played with it, did a lot of aggregations but now I would appreaciate a new set of eyes to look over the dataset and brainstorm possible visualisation ideas.
The Dataset is build like this:
||**url**|**title**|**author**|**pub\_date**|**categories\_json**|**publisher**|**lang**|**title\_clean**|**body\_clean**|
|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|
|**\_id**||||||||||
|**_NUMBER_**|_URL_ _URL_ Street parties inquiry: Opposition MPs...||_NUMBER_-_NUMBER_-_NUMBER_ _NUMBER_:_NUMBER_:_NUMBER_+_NUMBER_:_NUMBER_|\|bbc|en|\[downing, street, parties, opposition, mps, fi...|\|
|**_NUMBER_**|_URL_ _URL_ Look Up: Mixed reviews for Leonardo DiCa...||_NUMBER_-_NUMBER_-_NUMBER_ _NUMBER_:_NUMBER_:_NUMBER_+_NUMBER_:_NUMBER_|\|bbc|en|\|\[jennifer, lawrence, leonardo, dicaprio, film,...|
|**_NUMBER_**|_URL_ _URL_ a genocide taking place in Europe?|RT|_NUMBER_-_NUMBER_-_NUMBER_ _NUMBER_:_NUMBER_:_NUMBER_+_NUMBER_:_NUMBER_|\|rt|en|\|\[backdrop, war, scare, ukraine, statement, pre...|
|**_NUMBER_**|_URL_ _URL_ Maradona: Diego Maradona's younger brothe...||_NUMBER_-_NUMBER_-_NUMBER_ _NUMBER_:_NUMBER_:_NUMBER_+_NUMBER_:_NUMBER_|\|bbc|en|\|\[hugo, maradona, brother, legend, diego, heart...|
|**_NUMBER_**|_URL_ _URL_ Senate overwhelmingly passes massive defenc...||_NUMBER_-_NUMBER_-_NUMBER_ _NUMBER_:_NUMBER_:_NUMBER_+_NUMBER_:_NUMBER_|\|aljazeera|en|\|\[legislation, president, joe, biden, approval,...|
title\_clean and body\_clean are tokenized by spacy.
body is excluded for this reddit post because its too long, but its html code.",discussion,datasets,sg9lv8
178,"Carried on from Second Discussion Thread _URL_
> Carried on from Original Thread _URL_
> 
> > You have probably seen most of these, but I thought I'd share anyway:
> 
> > **Spreadsheets and Datasets:**
> > 
> > * _URL_ _URL_
> > * John Hopkins University Github _URL_ confirmed case numbers.
> > * Google Sheets From DXY.cn _URL_ 
> > * Kaggle Dataset _URL_
> > * Strain Data _URL_ repo
> > * _URL_ _URL_ 
> > * ECDC _URL_ 
> > 
> > **Other Good sources:**
> > 
> > * BNO _URL_ Seems to have latest number w/ sources. 
> > * What we can find out on a Bioinformatics Level _URL_
> > * DXY.cn Chinese online community for Medical Professionals _URL_ \*translate page.
> > * John Hopkins University Live Map _URL_
> > * Mutations _URL_ 
> > * Protein Data Bank File _URL_
> > * Early Transmission Dynamics _URL_ Provides statistics on the early cases, median age, gender etc.
> > 
> > **\IMPORTANT UPDATE:** *From February 12th the definition of confirmed cases has changed in Hubei, and now includes those who have been clinically diagnosed. Previously China's confirmed cases only included those tested for* 
> > 
> > **There have been a bunch of great comments with links to further resources below!** 
> > \
- COVID-_NUMBER_ Mobility Data Aggregator](_URL_ [^ _URL_
- County level mask mandate data set](_URL_ [^ _URL_ 
- NYT county level cases and mask usage](_URL_ [^ _URL_
- Please check the comments of the previous threads for more datasets.",discussion,datasets,n3ph2d
179,"I recently graduated from undergrad, where I double majored in economics and psychology, and have lately been considering going to grad school for statistics.
I'm very interested in many social issues, namely poverty and crime, and how those issues intersect with things like race and gender. Ideally, I'd pursue a career that would allow me to help tackle these issues on a systemic level, but I'm gradually coming to the realization that that kind of work typically isn't rewarded with a stable paycheck and benefits.
I am, however, really interested in the statistical facts and figures that inform my understanding of such social issues. Stuff you'd find at the BLS or Pew Research or the US Census Bureau. I also love learning about those surprising/intriguing statistics you'd find on an AskReddit thread , and about common mistakes made when creating/interpreting statistics .
I generally just like to think of statistics as means of better of understanding the world and the people in it, and that's kind of calming to me? I know that there are many problems with inaccurate and misleading statistics, but I'd like to think that with the proper education, I'd learn to avoid some of those pitfalls. I guess I just like painting pictures with my data.
So given my interests and past education, would it be worth it to continue considering a career in statistics? Or do I have a wrong idea of what the field would be like?",Career,statistics,sc6854
180,"Hi,
At work i am trying to correlate fraud with a few other variables . Dont really what I should use:
Scenerio:
I have about 100k Users, out of which 30k have been proven fraud.
I was trying to identify the patterns among the identified fraud users and compare them to the non fraud base. 
Eg: 
_NUMBER_) avg order value : _NUMBER_$ - Fraud user / _NUMBER_$ - NF
_NUMBER_) avg time spent on website : _NUMBER_ minutes - FU / _NUMBER_ minutes - NF
_NUMBER_) Avg categories shopped: _NUMBER_- FU / _NUMBER_ NF
Now i see a distinct pattern in the _NUMBER_ aforementioned , how do i correctly correlate them and use them as a measure of identifying a fraud user?
what i was doing:
 Assigning random weightages to the above _NUMBER_ kpis 
eg: if avg order value <= _NUMBER_ then _NUMBER_ weightage , if its above then higher weightage and so on.
Question:
_NUMBER_) what im doing, is this the right approach? if so, by what logic do i assign weightages instead of randomly?
_NUMBER_) If its not the correct logic, how do i go about using these _NUMBER_ or any given kpis/ variables and using them to identify a fraud user?
p.s: Example above is hypothetical",Career,statistics,se4z1a
181,"Dear poeple who know more about statistics than me, I‘m trying to figure out which factor extraction method is appropriate for my data reduction. I have a set of mixed variables, consisting of questionnaire scores and t-standardized values stemming from neuropsychological tests, both are supposed to measure the same or related constructs. I would like to reduce data because i suspect that the questionnaires and tests measure different constructs, which would mean they load on different factors. I read that PCA is inappropriate for psychological questions because it ignores the unique variance. Which extraction method should i use?
Thanks in advance!",Question,statistics,sd9f6x
182,"Hey all, so we get weekly survey data from a vendor on stock levels of parts, but sometimes they don't get responses by the end of the week so they will backfill the last known value. For example
""TRUE"" Weekly Stock Data
* Week _NUMBER_: _NUMBER_
* Week _NUMBER_: _NUMBER_
* Week _NUMBER_: _NUMBER_
Reported Weekly Stock Data
* Week _NUMBER_: _NUMBER_
* Week _NUMBER_: _NUMBER_ 
* Week _NUMBER_: _NUMBER_
Obviously this causes issues when trying to predict weekly stock movements using multivariate regression. Not only that but it isn't consistent which week is ""off"" and it is a rolling weekly time series . But they do say they have _NUMBER_% of the data within a _NUMBER_ week period. 
The data they send is ""as-is"" and they can't report how many survey responses were backfilled vs reported. So no chance of additional color from the vendor.
One of our data scientists said best practice is to do a centered _NUMBER_-week moving average as a preprocessing step , so just wanted to see if this is best practice or is there something else we're missing here that we should be looking into?",Question,statistics,sfq32v
183,"Had this question arise in one of my classes this week. From my understanding, both will actually produce the same or very similar coefficient estimates . 
Barring violations of distributional assumptions, I was told that linear regression is not preferable because of the potential for E to extrapolate outside of the interval bound by _NUMBER_ and _NUMBER_, which makes sense in the case of a continuous predictor. But I do not see how that is possible if the only variables in the model are exclusively _NUMBER_ or _NUMBER_? 
Perhaps I am missing something here - any insight would be much appreciated!
Thanks.",Question,statistics,sf02b2
184,"Hello, I’m a statistics undergrad and mathematics minor. I took a linear algebra course last semester, which was quite interesting but I think it was considered a first course in LA. A second course is typically more abstract and mathematical and not really geared towards certain applications of statistics, and I only have one spot in my math minor so I’d rather fill that with real analysis if I plan on going to grad school. 
A typical book in r/math people refer to is axlers Linear Algebra done right, but upon going through some of this I realized that some of it is not really applicable to statistics and data analysis. The rigor is fine, but some of the concepts just seem more for mathematicians than statisticians. It made me wonder if there was a type of linear algebra which was more “statistics” focused, and if so, if there was a book any of you recommend? I would appreciate if you could speak to this and recommend any books or classes that resembles the type of linear algebra class I’m talking about.",Question,statistics,sgbr0y
185,"I’m a CS and math/stats major and my interests lie in machine learning  and AI. I’ve found that when it comes to AI and ML, stats departments tend to focus more so on theory  and CS departments focus on both theory and applications, although not so much on the mathematics. 
I was originally considering a statistics PhD since statistics is absolutely fascinating to me, but now that I think about it, it really doesn’t matter if I do stats vs CS since my topic of study would be the same regardless of what I choose . So that got me wondering about some of the other factors that differentiate a stats and a CS PhD, especially in terms of the employability and the demand for these PhDs since my end goal is research in industry. 
My impression is that both are highly employable and in demand, and will continue to be for the foreseeable future. They both open doors to very lucrative careers. However, it seems that historically statistics PhDs have generally tended to open more doors, since they are also heavily recruited by big pharma, quant finance firms, banks, and business analytics, in addition to big tech. On the other hand, it seems obvious that a CS PhD would be preferred over a statistics PhD in the world of tech. But an interesting thing to note is that CS is becoming increasingly statistics heavy, especially with the rise of big data and fields like data science, AI, and ML. That is to say a computer scientist nowadays, especially one who specializes in a related topic, is a lot more likely to be proficient if not an expert in statistics than ever before. However, a statistics PhD will generally not have a lot of the CS related skills that a CS Phd would have, or at least at the same level. 
So that means that a lot of CS PhDs nowadays  would have most if not all the skills a stats PhD would have but with the addition of being an expert in technologies and their applications , making them seem to be the better choice. 
So my question is: will a CS PhD  be a lot more valuable than a statistics PhD in the very near future, if they aren’t already? Shouldn’t that mean that a CS PhD could also qualify for all the other opportunities that are available to a stats PhD, but with the addition of lucrative tech related opportunities? Also, it seems that research in CS departments and computer science as a whole is a lot more dynamic and broad than research in statistics which to be honest seems to be extremely narrow in its scope, being primarily concerned with refining preexisting statistical theories and models and not so much with formulating novel theories and ideas. 
Sorry for the long post, would appreciate any input though! Also, I understand there will be some response bias, seeing as this is the statistics subreddit.",Question,statistics,sfbn58
186,"Hi all,
I wanted to ask for help in devising an approach to evaluate a model of mine. I have created a Logistic Regression model, and extracted the probabilities for every observation, without a classification based on a cutoff. A certain part of these observations  will be transferred to a different handling regime . After a cutoff in time I will have to evaluate.
We have no hard data on whether the intervention works, just that it should for some degree, even if not perfectly.
We know that even in the top _NUMBER_% the rate of events is around _NUMBER_% , so I wouldn't look at the top _NUMBER_% probability cutoff as a cutoff in itself. 
Because of these, I started conceptualizing probability residuals. After the allotted time ends, differently handled observation will fall into values of _NUMBER_ and _NUMBER_. I could subtract the probability of the event from these and work with these residuals, looking at mae, mse, rmse , benchmarking it to historical data before handling. A major problem with this is that an aggregate error is good and bad at the same time. Bad as in of course a high error is bad, but also good, as it shows that the intervention pulled high probability observations away from the event.
Are there any resources or ideas you could share with me?
Thank you.",Question,statistics,senwbn
187,"I would like to test my data set to see if having an underlying health condition is more likely to readmit you to the hospital. For example if you have high blood pressure or diabetes and compare it to a yes or no if you were readmitted while having one of these conditions. My overall question I want to answer is "" Are patients with underlying health conditions more likely to be readmitted to the hospital?"" Im leaning towards chi-square. Any suggestions?",Question,statistics,shr0l6
188,"Hello, 
I've been asked to figure out a measure for performance and figure out a target that people need to hit. 
This is around a helpdesk that uses _NUMBER_ methods of communication, a phone system and a chat system. 
The data that I think I need to find this answer is listed below. 
Month 
Name 
Total Phone Calls 
Total Chats 
Total overall contacts  
Total Days Worked 
Overall contacts per days worked 
 
First point of contact percentage . 
I'm a reporting guy, but my manager is tasking us with learning more around statistics and figuring out things like this, but I'm unsure on where to start. 
 
Apologies if the format is incorrect here! Happy to edit whatever is necessary, and thanks in advance for the help.",Question,statistics,schx6d
189,"We mainly work on retrospective studies and the data was retrieved from the patients database. I work with a physician who has more research experience than me. The statistical analysis plan was written by another statistician before me so I am just applying her plan. 
They want to compare the outcomes and characteristics of two populations: one with disease and another without that disease. The plan says it should be student t test or Mann Whitney test for continous variables and chi square for categorical.
I asked him if he can think of confounding factors like age or sex or race or medications.. Etc to adjust linear model or logistic models. He refused and he just wants to publish as long as it is significant. 
Now my question is, is this OK to do? Wouldn't the results be biased? Why do medical papers use only univariate tests for retrospective studies? This isn't experimental study. Retrospective studies are full of confoundings.",Question,statistics,set26r
190,"I recently came across an interesting problem that I wasn't sure how to handle. The problem is:
There is an error value that is defined as the sum of many different sources of data:
Error = Σ
Where there are hundreds of n's
The data sources can either be positive or negative . Also, the scale of each data source can vary by multiple orders of magnitude. In an ideal world all the data sources should balance out to _NUMBER_, but that's never the case as there's always a problem with some data sources, sometimes minor sometimes major. I want to try and find problems that are causing errors. 
I also have _NUMBER_ measurements that are collected via different methods for each data source. One of the measurements is ""less"" accurate, I'll call this variable LAM for Less Accurate Measurement. While the other measurement is more accurate, which I'll call MAM for More Accurate Measurement. Now when I say more or less accurate, they're both still pretty good, so it's not like it's bad data. And I still use the LAM because it's faster and useful for the problem of diagnosing issues, which this whole post is about. 
So here's the problem: In order to figure out which data sources are contributing to the error, I've been plotting the LAM and MAM on the same graphs for each data source and manually looking for places where they don't match up or correlate with each other. And if I notice such an instance, I compare it to the Error at that time. And if they match up I investigate that data source for any potential problems. I'd like to make this more reliable by using some statistics instead of just manually searching the data. 
My idea to do this was to take the difference between the LAM and MAM that I'll call ΔM for difference in measurement. Then I would find the correlation factor between ΔM and Error over a certain time period and if the factor is high  I'd investigate that data source. 
That's my plan anyway, but I'm not sure if that would work. One concern I have is that the Error and ΔM are often on wildly different scales so I don't know if the correlation factor is correct to use. Also, since the Error contains the data for all the data sources. I don't know if it makes statistical sense to compare _NUMBER_ data source to the sum of all the data sources. 
If anyone could let me know if this idea would work, or if you know of a better method of trying to accomplish this task, I would much appreciate it.
Edit: Also for clarification purposes the Error is calculated using the MAM for each data source since it is more accurate. Unless the MAM is missing for some reason, then the LAM is used in its place. This is one of the issues that can occur causing errors but finding missing measurements is easy enough. It's finding measurements that are incorrect that's harder.",Question,statistics,sewb8r
191,"Let's say I have a population that is _NUMBER_% class A and _NUMBER_% class B.
There's another subgroup in this population with two levels, _NUMBER_ and _NUMBER_. Sub group _NUMBER_ is proportional to the overall population in that it is _NUMBER_% class A and _NUMBER_% class B. However, sub group _NUMBER_ is _NUMBER_% class A and _NUMBER_% class B.
Since subgroup _NUMBER_ is responsible for a disproportionate amount of class B relative to the overall population, what formula would allow me to estimate the actual number of students that subgroup _NUMBER_ adds to the class overall B population?",Question,statistics,se1ar4
192,"I've been studying linear contrasts and it feels like I have missed a meeting on what they actually do. I get that each row of treatment coefficients should equal _NUMBER_, and I somewhat understand that they get the variables to _NUMBER_ or _NUMBER_ or _NUMBER_, or that _NUMBER_ can be balanced by _NUMBER_ _NUMBER_ _NUMBER_ _NUMBER_ by isolating a variable but it's confusing how they do that. So with reciprocals the _NUMBER_/_NUMBER_ times _NUMBER_/_NUMBER_ = _NUMBER_, but how are they getting them to zero? Is this like a blind crossover in medical trials where the dog gets a piece of a hotdog with a pill inside with null effect vs a piece of a hotdog with a new pill that might rabies?
The second part of this is asking is a statistical test of goodness that the rows add up to zero, else it's a bad fit and they are not parallel thus not collinear?
This isn't a homework question, I'm not understanding the concept of its calculation or how it's used. So I've also posted a similar question in the calculation?
_NUMBER_. How do they get the coefficient of a variable to be _NUMBER_
_NUMBER_. What real-world use is a linear contrast good for
_NUMBER_. Is it just a 'stats professor' wanting me to know how to do something the old school way that computers do for us with code in a matrix calculation and he is prepping me with 'you need to know how this works by hand'?
This isn't a homework question, I'm not understanding the concept of its calculation or how its used. So I've also posted a similar question in this thread at _URL_ r/datascience for the application part with photos of the slides I'm referencing since I can't seem to post photos here.",Question,statistics,sgkgm3
193,"I would like to ask for some help with regards to interpreting and describing the results from a linear regression analysis.
My goal is to describe the relation between independent  variable X1 and dependent  variable Y. I performed a simple linear regression and found that the regression equation derived from the model did not predict the occurrence of my outcome variable better than an intercept only model (F3.359, p=_NUMBER_).
X1 B=_NUMBER_ p=_NUMBER_
Can is still report the unstandardized B value of my IV ?
I subsequently performed a multiple regression analysis to analyse the effect of X1 on Y corrected for possible confounders X2-X10 but encountered the same problem (F1.678, p=_NUMBER_). 
X1 B=_NUMBER_ p=_NUMBER_
How can I best report theses results? Should I just say I conducted both a simple and a multivariate linear regression analysis, which both showed variable X1 not to be a significant predictor of Y? And which values should I use to substantiate this?",Question,statistics,sdwhhy
194,"I would like to know the full equation to calculate predicted values given the coefficients of an ANCOVA where the continuous predictors where standardized with z-scores.
My equation is essentially: Y = B0 + B1X1 + B2X2 + B3X3 + B4X4 + B5X2X4
However, variables _NUMBER_, _NUMBER_, and _NUMBER_ are continuous predictors that are standardized with z-scores . Variable _NUMBER_ is my categorical predictor, and the last term is an interaction term between variables _NUMBER_ and _NUMBER_. 
Given a regular multiple linear regression with no interaction terms but predictors are standardized, 
B0 = B0 - SUM(Bi)
BiXi = Xi \* 
How would the formula be modified to accommodate a categorical predictor and an interaction term? I'm specifically looking for help on the intercept - calculating the rest of the formula is pretty straight forward. There's no mean or SD for a categorical predictor, so would I just subtract the coefficient? And how does that work when the categorical predictor is part of the interaction term?",Question,statistics,sctjxm
195,I am performing a test retest study on a treatment and control group for a test with _NUMBER_ questions. I want to test if the percentage improvement for the treatment group is greater than the control group. So I'm looking to do a paired test on proportions. I think McNemars test would be what I want to run  but wanted to see what others thought.,Question,statistics,shh603
196,"I'm using the software DAGITTY to map causal inference and the model tells me that my variable is a biasing path  but I was also wondering if it could also be a mediator?
I've had a look online but haven't been able to get a decent explanation
Any help would be greatly appreciated, thank you",Question,statistics,seqwnw
197,"I know the basic idea that a t-test is used to compare if two sample means come from the same population or not, but beyond that I'm not sure how a t value comes into play, underlying assumptions, etc.
Checking books and/or blogs, I sometimes find it confusing or different, wondering if anyone recommends a chapter on it. I'm okay with hand-wavy explanation, just want to ""get it"".",Question,statistics,sg8l8x
198,"I have collected data for the MASEM but have been unable to actually do anything with it. My University does not have SPSS, so I have tried R  and webMASEM online . At this point, I'm wondering if I could run it in Jamovi or if there is another program I can use. Any advice is appreciated.",Question,statistics,sdak9h
199,"Let's say that you have some data from a population which has the characteristic of being grouped in very regular clusters, but the location of the clusters is essentially random.
As an example, I'm imagining something like looking at random hairs on the coat of a cheetah, looking for black ones.
Let's say that you suspect that the data has some kind of clusters, but not how big or how dense they are. Is there any sort of test that will help you guess how ""large*"" such clusters are likely to be, under the assumption that, if they exist, they are all of an approximately regular size? 
*For some definition of size that includes some notion of distance and means a quantifiable boundary exists at some distance from the center of the cluster.
Basically I just want to know if there's a statistical way to recognize n-dimensional cheetah spots.",Question,statistics,shn7dp
200,"Hello, I was browsing the search tool on the subreddit to see how useful differential equations is in statistics. I found that an overwhelming majority of posts said how differential equations is mostly useless unless it’s for time series, in which stochastic differential equations is important. Since time series is something I find quite interesting , but suppose in the future I wanted to, is stochastic differential equations necessary to understand time series at a grad level?",Question,statistics,sewkx7
201,"Hi all, I'm a physicist so please bear with my ""loose"" stats knowledge and poor terminology usage :) I'm also unaware if this is a replication of some well-known statistics? This is related to my studies but it is not homework.
\*\*Question:\*\* Am I right to conclude that this estimator displays statistical bias when the sample size is small? And is there any clear reason for why this is the case?
\*\*Background:\*\* The general idea is as follows:
_NUMBER_. Start with an unknown sample size of N\_true events
_NUMBER_. Each event has two independent rolls of some true probability p\_true
_NUMBER_. We sample all events where both or just one of the two rolls passes 
_NUMBER_. Use the sampled events to estimate p\_estimate
There are a few ways the expression used for this estimation can be derived , but the simplest is to count the number of sampled events with _NUMBER_ passes, and the number with _NUMBER_ pass.
\* N2 = N\_true \*  \* 
\* N1 = _NUMBER_ \* N\_true \*  \*  \
And simple algebra is used to cancel N\_true and express  in terms of our sampled events N1 and N2:
\*  = /
\*\*Simulation:\*\* This is a generally well-standardized formula and it does work very well assuming the sample size is large O for N1 and N2. But after performing some toy Monte Carlo studies it appears this is offset if the available statistics is notably smaller. I've detailed a few plots in this github notebook: _URL_ _URL_
\* Plots of  - : Shows asymmetry when N\_true is low, with the average delta falling below zero, implying a bias.
\* Plots of Avg( - ) as a function of N\_true, this bias appears to approach zero as N\_true increases, essentially becoming unbiased for large statistics.
\*\*Thoughts:\*\* Assuming this is all correct my conclusion would just be a warning to avoid the low statistics regime, but I'd like to gain a handle on any reasoning for this feature. Qualitatively the only idea I have is that this estimate has no knowledge of events where both trials fail, , but I can't understand the translation to any quantitative explanation for the bias? Or is it not easy to explain this and I should just take these results as gospel?",Question,statistics,sc4llm
202,Let’s say you have a _NUMBER_  x _NUMBER_  repeated measures ANOVA interaction effect: The difference between X and Y is bigger for A than B. Can you calculate the effect size for the first simple effect  and second simple effect  and then compare the size of the two simple effects using a z test?,Question,statistics,sftjyj
203,"I have measured brain signals with the same EEG on the same person, but with different applications.
One measurement came from the computer and the other one from the app.
I want to know if there is a statistical difference between the data.
Now I am unsure if the dependent t-test is the right way to go.
Could anyone help me ?
Thank you for your time.
Edit:
The end goal is to show, that there is no statistical difference between the computer measurement and my application.
Basically to confirm that my app is working as intended.
I have one column for the time  and one for the Voltage .",Question,statistics,sfgg0v
204,"I’m a Poli Sci major and Stats minor, interest in quantitative research for public policy. Last semester I took a quantitative Poli Sci class where I was introduced to directed acyclic graphs. Today in Multivariate Stats the professor talked about path diagrams and it looked exactly the same. Are these just different names for the same thing?",Question,statistics,sdejd3
205,"I’m working in a warehouse/distribution center environment.
One of the assignments I had, was to build a manpower planner in which, if we know the total volume in lines per day by work area, given a certain “standard”, we can estimate the number of labor hours we need per day.
The reason that I say standard in quotes is because it’s not a true labor standard that is managed by a system and is based on multiple variables, rather what I call a “reasonable expectancy”. 
Very rough math example, if someone goes to pick _NUMBER_ item off a shelf and does it in _NUMBER_ minutes and travels _NUMBER_ feet round trip, then the pick lines per hour is about _NUMBER_ lph and they can also travel _NUMBER_ feet per hour. If in each same trip, they pick _NUMBER_ items instead of _NUMBER_, then their lines per hour goes up to _NUMBER_ lph.
So again, I’m trying to find a way to set these productivity reasonable expectancy rates in a fair way that captures the average day. Is this done via confidence intervals or some other methods?",Question,statistics,sgu6nq
206,"I know this is a stupid question but I really confused on which sample size would be right. I have to find the sample size from a data set that has _NUMBER_ related groups in one study, the number of people studied is given. Lets say there's group A and group B, would the sample size for group A be  or ",Question,statistics,sf0pzg
207,"So I am still trying to understand how to calculate very apt sample sizes for experiments and I am shocked by the amount of studies that doesn't report effect sizes. By convention, we all agree that a power of _NUMBER_ is the minimum and I have no contentions for that. However, effect sizes are practically non existent in any studies I am looking at. Someone said to me reporting it isn't really the norm. 
BTW my field is biotechnology so we are also burdened by the pressure of not wasting animal/reagent/materials for research. I just want to determine sample sizes that are statistically valid and at the same time something that would be logistically sound. Can't subject a total of _NUMBER_ mice to a lot of treatments or do a total of _NUMBER_ potted plants for biofertilizer experiments just to achieve good effect size and power isn't it?",Question,statistics,sc55kw
208,"It seems like it is much more commonly recommended to do statistics in graduate studies while opting for a math undergraduate degree instead. I've heard various justifications for this, from ""undergrad stats isn't taught well"" to ""you'll understand it better if you do math first because stats is easier to pick up in your own"". Is there any truth to this?",Question,statistics,sg5r0r
209,"I've done this before but just for one player. I download their game logs, find the mean, stddev and z score. Then I use the z score to find odds to score more than x. 
I can't wrap my head around how to do this for _NUMBER_ players to combine though.
The numbers if you need:
Player _NUMBER_: avg: _NUMBER_ points, stddev _NUMBER_, n = _NUMBER_
Player _NUMBER_: avg: _NUMBER_ points, stddev _NUMBER_, n = _NUMBER_
Looking for them to combine for _NUMBER_+ points
I ended up removing the first _NUMBER_ games from player _NUMBER_'s data set and then added their points and used the same method, but I know there is a better way.
I've taken a basic stats class, willing to read and watch videos to learn more.
Thanks!
e: I just found this _URL_ regarding PMF, but I'm having some trouble understanding the notation
They play for different teams, and basketball points have a Gaussian Distribution",Question,statistics,se7vcz
210,"Nervous about messing up something basic so wanted to check here:
I'm supposed to be listing all the Means and SDs from a GLM computed in SPSS. What I'm wondering, is, did my mentor actually mean *means and std error*, NOT the SD 
Correct?
Sorry if this is dumb.
Edit: he did in fact mean std",Question,statistics,seaio6
211,"In marketing, I'm often asked the question ""How many additional new users do I need to see a stat sig lift from this marketing campaign?"" This directly affects how much the client is planning to spend.
Is there a formula/methodology to reach an answer?
I imagine that it depends heavily on the previous baseline . It may also depend on how much ""noise"" typically occurs . Thanks!",Question,statistics,scpqwt
212,"\
Hi there. I've been modelling count data using zero-truncated Poisson regression\*. My IV is the number of drugs detected in a person's blood when they have a drug-related death . DVs include year of death, age group, sex . Here _URL_ is one of the predicted effect plots for interaction of age group and year.
I am struggling to understand how some of the predicted values are less than _NUMBER_ . Please could someone help me understand if this suggests a problem with either my approach to modelling or my interpretation of the results?
\* For info I am running a glmm in R, where family = truncated Poisson, link = log.",Question,statistics,sezuky
213,"Hi. First, sorry for my english. Second, my problem: I am studying asymptotic theory and in some examples we use a sequence of normal random variables, like ~N. I can't imagine how look like this sequence because all the X's has the same distribution, with the same mean and the same variance, so when n->∞ whats happened to them? Doesn't Xi have the same distribution as Xj if its parameters do not depend on n? 
I hope that the question is understandable. Thanks all",Question,statistics,scuy88
214,"Hi, I have to complete a mini project for my masters. I want to do a social network analysis on rstudio in relation to liverfluke cases in Ireland using historical data from farms throughout the region that has x amount of infections that can shine light on the severity of the issue and current risk. I have that data available that ranged back _NUMBER_ years.I would like advice on the best way to go about this
I want a representation of the severity of the issue but the worry is, do I also need data on animal movement between farms or can I still do a SNA in absence of this data and would it be worth it or even enough to do an SNA. If it is required I would prob do a questionnaire around the farms to better understand their movements between other farms? Would I be right in doing that?
Any advice would be much appreciated. Thank you for reading my post :)",Question,statistics,sf8ypw
215,"I got into a conversation with a coworker, he was doing napkin math and showed that average purchase value and average conversion rate, together, give us the expected revenue. And my response wasnot so fast, the expectation of a joint distribution is not guaranteed to be equal to the product of the marginal expectations. He didn't have any idea what I was talking about, so two questions...
_NUMBER_. Am I right or confused? I don't want to lecture this guy if my understanding isn't principled
_NUMBER_. If I'm correct, is there a more intuitive way to explain this?
`E != E * E`",Question,statistics,sf0jfu
216,"I think we've all seen examples before where there is a medical test which has a specificity of _NUMBER_% and sensitivity of _NUMBER_% for a disease which _NUMBER_% of people have. The probability that somebody who test positive is P = P*P / P = /(+)
Meaning less than _NUMBER_% of the time a positive test is a true positive. The example then goes on to show how the probability of two positive tests is less likely as the probability of X false positive tests in a row for the general population should be _NUMBER_^X, so that for two tests in a row
We now have /(+) or a _NUMBER_% chance that the result is a true positive...
However doesn't this type of re-sampling only apply to the population as a whole, not an individual? For instance, if you take a COVID antigen test and get a false positive OR a false negative, there will be some people for whom this is a systemic, repeatable error .
When these values get reported for tests is that taken into account in any way, or do the systemic errors get bundled into the bulk population rate?",Question,statistics,sh6tqb
217,"For example, I came across a post in r/nba saying that a player scored _NUMBER_ points, with _NUMBER_ in the fourth quarter. It struck me as something you might expect rather than noteworthy. So two things, how would you calculate this, and am I wrong? I’m going off the assumption that each point could be allocated to a random quarter with a _NUMBER_/_NUMBER_ probability. Thank you!",Question,statistics,sfk52r
218,"Hi,
I need a research article that uses panel data to describe some business or financial phenomenon. I would like the article to include the source of data and a well-written econometric method. I need to trace a method and data, and I want to try to implement it in Python.
Please share links to specific articles.",Question,statistics,sfflhm
219,Does anyone have an idea how to run a power analysis for a multilevel three-way interaction or knows any thumb rules for the N?,Question,statistics,sd4j5n
220,"I hope the following makes sense.
We are a business-to-business organization. Some of our customers have signed up for a membership to get a discount and some have not. Those that have signed up have contract dates that start at differing dates and the whole market is growing, this makes it hard to assess the impact of these memberships. That said, we still need to evaluate the impact of the memberships on sales numbers .
One solution is for me to take any given member and compare its x\_year growth rate to the market growth rate for the same period and iterate through for all members. This will create _NUMBER_ distributions that I can test. The issue is that this kind of data manipulation is challenging therefore, I would like to know if there is a single statistical test, I can run across the whole dataset that accounts for market growth, the growth of a member age of the membership contract?
Thanks in advance",Question,statistics,sew8fw
221,"Hello. 
I have an angle sensor which needs to be calibrated, and a standard sensor which gives accurate results  
What are the techniques used in order to use the accurate readings to calibrate the non accurate sensor? 
I think some curve fitting / linear regression, but don't know if this is the right way and how to do it well . 
What are the techniques used for this type of calibration?",Question,statistics,sh5bk7
222,"Hi all, first post here.
I am working on a collection of regression models, and I'm inspecting the residuals to see if there is anything out of the ordinary like missing variables. My variables have been standardised  to be able to compare them, etc. I plotted the distribution and would like to say something about the variance of the error. All the residual plots look approximately normally distributed with zero mean. Is it valid to take the standard deviation of these plots? For example, one of the residual distributions has an std of \~_NUMBER_, would it be valid to say that say _NUMBER_% of the residuals  fall within _NUMBER_  standard deviations of the original variable?
I hope I have worded it correctly. Thanks in advance.",Question,statistics,seodxm
223,"I’m aiming for a PhD in statistics further down the road. I’ll be finishing my bachelor’s in math, stats, and CS  soon and will have taken several advanced classes which are helpful for graduate studies in statistics . However, I want to work a bit before committing to a PhD and save up some money. So my plan is to work 4ish years while pursuing a masters in statistics part time and knocking out almost all coursework  required by stats PhD candidates. 
In the end, my hope is that I’ll be in a place where I’ll know what topic I want to research , what advisor I want to work with, and have much better time management skills. As such, I’m wondering if its likely that I’ll be able to finish my PhD in three years, assuming everything goes at least some what smoothly. 
Also, I’m guessing my chances would increase significantly if I pursue a PhD from the same university that I do my masters from, so that I am familiar with the graduate faculty, the school’s requirements, and advisors. Is that correct?",Question,statistics,sc5gpu
224,"We're trying to figure out the percent chance of getting this pick correct:
You have _NUMBER_ total choices, _NUMBER_ correct answer, you get _NUMBER_ guesses. After each guess, you know it's either right or wrong and can eliminate that choice for the next guess.... We have some saying it's _NUMBER_%, some _NUMBER_% and the rest all over the place.. we're all over thinking it and none of us know how to properly figure it out. 
 
Thank you in advance!",Question,statistics,sbr1fe
225,"I want to see which days of the week are the busiest and least busy days for a specific subreddit. Such a tool would probably look at stats like: 
* votes per day
* posts per day
* comments per day
It would aggregate across similar days of the week week-over-week. Does anyone know if such a tool exists?",Question,statistics,sgjalr
226,"Hi all, 
I want to learn time series and I don't know where to start and so I am looking for your recommendations. My undergrad is in pure math where I took pretty much everything under the sun including but not limited to, measure theory, functional analysis, topology, algebraic topology, probability theory and mathematical statistics, PDEs and Nonlinear Dynamics, and more, and I finished my master's degree in Stats at a reputable school in May of '_NUMBER_ where I took elective coursework in Statistical Computing, Stochastic Processes, Bayesian Statistics, and Statistical Inference on Graphs. My real main point here is, don't be shy about including a reference or book that is math heavy -- I can take it haha, though I'd prefer to not have to see more graduate analysis again if I'm honest as it's been a few years and that knowledge is dusty. 
I feel bad because Time Series always seemed so interesting to me and was just one of the subjects that always got away from me. I'd love to honestly learn it and learn it well. I'd prefer to learn how to do TS in R as that's my main language I like but it seems like knowing them in Python is increasingly becoming commonplace. I honestly just assumed there was more and better support for Time Series in R as well. I'd love to hear what you all have to say. I really don't mind starting at square _NUMBER_ if that's what is needed. I don't know everything and just want to learn the subject well. I know it is commonly used in quant finance or whatever but I want to really see and explore a bunch of applications as well as the theory. I have plenty of time. I'm not trying to learn it for work or for any urgent reason. I just want to be a student again and struggle with the material haha. 
Thanks!",Question,statistics,sfvt73
227,"H: household income has no effect on levels of bias crime
Pearson correlation coefficient = _NUMBER_
P-value = _NUMBER_
Interpretation:
Pearson correlation coefficient is _NUMBER_ which indicates a somewhat weak positive correlation between the median household income and average bias crimes per 100k.
Assuming a significance level of _NUMBER_, the P-value for this regression is _NUMBER_ which is statistically significant.
It implies that there is less than _NUMBER_% chance that the null  is correct. This means we can accept that household income has an effect on levels of bias crime.
h2: urbanisation has a significant effect on levels of bias crimes
Pearson correlation coefficient = _NUMBER_
P-value = _NUMBER_
Interpretation: Pearson correlation coefficient is _NUMBER_ which indicates a weak positive correlation between the levels of urbanisation and average bias crimes per 100k.
Assuming a significance level of _NUMBER_, the P-value for this regression is _NUMBER_ which is statistically insignificant.
It implies that there is more than _NUMBER_% chance that the null  is correct. This means we can not accept that levels of urbanisation have an effect on levels of bias crime.",Question,statistics,sfe7t5
228,"I work in applied statistics and I find that the people I speak with either think that AUC is a good metric when dealing with a classifier trained on imbalanced data, or think that it's far too optimistic in these cases and should not be used.
Anecdotally I've noticed that if there are very very few samples in the minority class the AUC can be misleading, but for large datasets I haven't noticed much of a problem .
Is AUC generally a good metric in this case? Why or why not? If not, what is a superior metric ? I am looking for a metric that does not require picking such a threshold, and instead measures performance based on the output probabilities.",Question,statistics,shb5dd
229,"This is a different example than the problem I’m trying to solve but very similar. 
Is there a way to test whether the same top N car brands, say, perform the best in different tests by different providers? The hypothesis is that the top N brands “influence” the different test providers to make their brand come up on top in their tests. The idea is to test whether such a bias exists, so if the top N slots in each test is the same  then we could corroborate that with external information to confirm the hypothesis. 
How do I test for such consistency or stability in the top N items of different lists? Does Kruskal Wallis H test makes sense here? 
Thanks for your help.",Question,statistics,sghkok
230,"Hi, I am planning to self study statistics . So far my introduction to the topic comes only from Statistical inference book by casella and berger. I do have background in real analysis, measure theoretic probability and linear algebra. However, I have no clue what topics and books I can pickup to learn advanced material. My interest is on the theoretical side. Examples include but not limited to decision theory, information theory, higher dimensional statistics etc. It would be great if one can classify the branches of statistics and put references  accordingly",Question,statistics,sg4tiq
231,"Confession: I'm in way over my head, so any help you could provide would be greatly appreciated. I'm working on a project that I need to be able to provide some statistical analysis for my organization. Which is out of my league!! And we don't really have anyone in the organization that can handle this kind of thing. 
Here's the problem I'm trying to characterize statistically
_NUMBER_. _NUMBER_ parts were purchased
_NUMBER_. A sample size of _NUMBER_ was taken and tested 
_NUMBER_. There were _NUMBER_ failures
Some questions I want to be able to answer...
_NUMBER_. What is the probability that the entire lot only had _NUMBER_ bad products, and magically those _NUMBER_ were the ones selected
_NUMBER_. What if an additional _NUMBER_ more were tested, and all _NUMBER_ passed successfully, meaning there were _NUMBER_ failures
_NUMBER_. What is the probability that any one part in the lot of _NUMBER_ is a good part. In other words, if I random pick a part, what's the probability that it will work?
And really any other questions you think would be helpful. Or if these questions are stupid. Again, I'm in over my head.
Thanks for any help!",Question,statistics,sev4ht
232,"I’ve heard several people mention that an industry PhD might align best with my interests. I want to pursue a PhD and do research but in industry . I don’t really have any interest in research in university-academia but more so in very applied techniques, theories, and technologies. 
So my question is what exactly is an industry PhD? How does it work? Will it be associated with a university, a  company, or both? Will it still be funded?
I searched these questions I was only able to find one program by Northeastern University. Are there any others in the US, especially in statistics or even CS?",Question,statistics,se9xa9
233,"People skills aside, I'm wondering what content and knowledge you think would set a master's student up well for success in industry, assuming that this is a terminal degree. Are there certain things you'd recommend studying, certain skills to pick up and learn along the way, good projects/brain work that would help expand my ability to be a competent statistician?
Personally, I'm in a very applied Biostatistics program that largely eschews proofs for work with data, so I'm taking some time on my own to get at least a basic understanding of the mathematical underpinnings of things and to remind myself *why* any of this works. I'm not necessarily doing every proof for everything I come across, but I'm running myself through the theory until I feel I've got a good grasp of where the model I'm working with comes from and what its limitations are. Any similar things I should be doing?
Also, internships, do I need them?",Question,statistics,sgwlf4
234,"For example, I have a factorial design experiment on two variables, amount of water A  and temperature B . Since the differences between them are very huge, some textbooks code them as \. For example, A would have values of _NUMBER_ for 5L and _NUMBER_ for _NUMBER_ L and for B you have _NUMBER_ for _NUMBER_ C and _NUMBER_ for _NUMBER_ C. Is this really necessary? Because my linear models would obviously be different for coded vs. non-coded, but IDK what is the right thing to do in the context of publishing papers and experimental design. Should I make them coded or not?",Question,statistics,sefwue
235,"As a researcher, should we require the telephone number to be included in the respondent's identity?
Meanwhile the research context uses random sampling. Therefore, the surveyor is a person who has only known the respondent for a short time, but the respondent must provide his/her telephone number. The reason is because researchers need this number to validate the data.
But when surveyors went to the field, many respondents didn't want to give their phone numbers.
So, if we look at the regulation of personal data protection, should the researcher ask for the respondent's phone number? or asking for a phone number is actually not allowed?
Sorry, if this question out of context for stats _EMOJI_",Question,statistics,se0fsg
236,"Hey guys, I did my bachelor's in business  and have always been interested in data analysis. I work in Product Management now and have been learning to code in Python for some time now. But eventually, I realized, regardless of how much coding I learn, I need to have theoretical knowl as well.
So I decided to do a master's in Applied Stats.
My stat basics is not very strong and the courses I'll be taking in my first semester are: Applied Regressions and Applied Time Series Analysis.
The question: what should I learn before my classes start? What are some good resources  which can help me learn these? 
Thank you very much for your time. 
Detailed Course Content if anyone's interested:
AST _NUMBER_: Applied Regressions
Credit-_NUMBER_
_NUMBER_. Simple Regression Models: Review
_NUMBER_. Multiple Regression Models and Estimation
Matrix Notation and Literacy
Hyper plane extension to simple linear model
Interaction models
Basic estimation and inference for multiple regression
Related Application
_NUMBER_. General Linear F test and Sequential SS
Reduced and Full models
F test for general linear hypotheses
Effects of a variable controlled for other predictors
Sequential SS
Partial correlation
Related Application
_NUMBER_. Multicollinearity between X variables
Effect on standard deviations of coefficients
Problems interpreting effects of individual variables
Apparent conflicts between overall F test and individual variable t tests
Benefits of designed experiments
Related Application
_NUMBER_. Polynomial Regression Models
_NUMBER_. Categorical Predictor Variables
Dummy Variable Regression
Interpretation of models containing indicator/Dummy variables
Piecewise regression
Related Application
_NUMBER_. More Diagnostic Measures and Remedial Measures for Lack of Fit
Variance Inflation Factors
Ridge Regression
Deleted Residuals
Influence statistics -Hat matrix, Cook's D and related measures
Related Application
_NUMBER_. Examining All Possible Regressions
R2, MSE , Cp
Stepwise algorithms
Related Application
_NUMBER_. Nonlinear Regression
Logistic and Poisson regression models
Probit Model, Tobit Model
Related Application
AST _NUMBER_: Applied Time Series Analysis
_NUMBER_. Introduction: Examples, simple descriptive techniques, trend, seasonality, the correlogram. White noise  ,Transformation to stationarity, Stationary Time series with practical examples
_NUMBER_. Probability models for time series: stationarity. Moving average , Autoregressive , ARMA, ARIMA, SARIMA models with applications to economics, engineering and biomedical sciences
_NUMBER_. Estimating the autocorrelation function and fitting ARIMA models.
_NUMBER_. Forecasting: Exponential smoothing, Forecasting from ARIMA models.
_NUMBER_. Stationary multivariate models: Stationary multivariate models with application to real life data. Dynamic simultaneous equations models, Vector autoregression  models, Granger causality, Impulse response functions, Variance decompositions, Structural VAR models.
_NUMBER_. Nonstationary Multivariate models: Nonstationary Multivariate models with examples. Spurious regression, Cointegration, Granger representation theorem, Vector error correction models , Structural VAR models with cointegration, testing for cointegration, estimating the cointegrating rank, estimating cointegrating vectors.
_NUMBER_. Stationary processes in the frequency domain: The spectral density function, the periodogram, spectral analysis with Empirical aspects of spectral analysis
_NUMBER_. State-space models: Dynamic linear models and the Kalman filter with applications of filter",Question,statistics,sh3y7x
237,"I have a dataset with _NUMBER_ types of variables. One is years of clinical trials publications , other clinical trials publication numbers for each year, and the last one dropout numbers for each publication. Shapiro-Wilk shows no normality. How can I decide to the number of split for each variable into classes before the chi-square test? Any help? in general is there any algorithm to help number of bins of data for ranked  data?",Question,statistics,sg2xng
238,"It sounds like several graduate classes from the math department would be very valuable to a statistics PhD, including graduate analysis, measure theory, probability, numerical analysis, optimization, and some tangentially related classes like stochastic processes, theory of functions, combinatorics, and ergodic theory. So how common is it to get a masters, a graduate certificate, or a PhD minor in math while pursuing a PhD in statistics?
Curious because it’d be nice to get to study some more math at the graduate level while doing my PhD in stats.",Question,statistics,sczo4e
239,"If I have some collection of data about some people, and I want to use it to predict, say, their favorite genre of TV show out of three choices, what would be a good statistical method to use? Google suggests something called nominal logistic regression, which I know nothing about, but I'm willing to look into it if it's really a good idea. Seems like there should be something more basic, that I'm missing, though.",Question,statistics,seakbw
240,"Hi,
I have two parameters originating from a simple OLS model. I'd like to determine the percent increase  between the parameters .
Calculating this from the raw parameters is of course easy, but in doing this I lose the uncertainty. My question: can I determine the distribution of percentage increase/decrease between the parameters by repeatedly sampling and calculating it from the parameter distributions?
For example, my two parameters from the OLS are _NUMBER_ and _NUMBER_, with standard errors of _NUMBER_ and _NUMBER_ respectively.
With the assumption that the parameters have a Gaussian distribution, my idea is to do this:
```
tibble(
 param_a = rnorm,
 param_b = rnorm,
 percent = ( / param_a) * _NUMBER_
) %>%
pull %>%
quantile(probs = c)
```
Yielding a _NUMBER_% interval of:
```
_NUMBER_% _NUMBER_%
_NUMBER_ _NUMBER_
```
Is this valid, or am I off the mark?",Question,statistics,sehzun
241,"Hi,
We are struggling with accidentally distributed IT operations across the whole company.
I have found a rule that says the following, buy I can’t find it any more.
Here’s what I remember:
- If a worker completes a unit of work in _NUMBER_ hour under _NUMBER_% load, =_NUMBER_
-than it will complete the same unit in _NUMBER_ hours if the load is _NUMBER_% as (_NUMBER_/_NUMBER_/=_NUMBER_
Where do I read more on this please?",Question,statistics,sca0ch
242,"I have a dataset that has samples taken over a period of time. I'm trying to see if each sample takes any random value under the distribution the data has formed, or if the value a sample takes is dependent upon the samples prior to it. Essentially, I am trying to find if there are patterns/trends in the order of the data. Am I correct that this is what the Mann-Kendall Test for Montonic Trend does? If not, how would you recommend for me to run this test?",Question,statistics,sgpd7s
243,"Can someone explain on intuitive level why 'The Naive Forecast' makes sense? Toy examples would make more sense!
@ EDIT
Think I found a satisfying explanation: 
_URL_ _URL_",Question,statistics,sca8nl
244,"I'm a maths teacher with very little stats background . However, I am supervising a students project with is heavily stats based. I am stuck on one step. The student is integrating XLfdXL from k to infinity. Their solution is
E phi\
I'm not sure how the CDF of the normalised dist. is included, or why the VarXN would be in the numerator if it is.
Any help would be much appreciated",Question,statistics,set1p2
245,"I have a longitudinal dataset, and I only used the MAKE command in VARSTOCASES to create long-format variables from the wide-format variables I originally needed for an analysis. There are over _NUMBER_ longitudinal variables and I didn't have the time to use MAKE for them all.
Now my advisor wants me to include another variable. This is a survival analysis and I entered survival event data manually in long format after using VARSTOCASES. 
Is there any syntax I can use to quickly add a long-form variable from existing wide data? Or do I have to go back to wide and then long again?
One reason it would suck to do the latter is that half of the population was only tracked for _NUMBER_ time points and the others for _NUMBER_. So I had to manually delete the _NUMBER_ blank rows for the group with _NUMBER_ time points last time.
Thank you for any ideas!",Question,statistics,sdjuq3
246,"I'm trying to add a safeguard to a device I'm building. The primary function utilizes a set of sensors to determine when a component should be turned on or off
This time should be approximately the same *most* of the time. However, there is some natural variation in how long it takes and a general upwards trend overtime as the component wears out and operates more slowly. It is also affected by things like reservoir capacity.
Currently, I log this data for each time the device runs. The idea is to observe the system carefully for the first day. Then once basic sensor function has been verified manually, data will be collected and mostly be of free of anomalies unless a sensor fails .
Each time the component runs, I'd like to calculate lower and upper boundaries for the run times beyond which an event can be considered anomalous ",Question,statistics,scvt70
247,"Ok. Repost. Me and my partner are arguing. Can someone give us the equation. 
If I’m in the 98th percentile for height for my age and gender, what is the probability that one person in a group of _NUMBER_ people in my age and gender group is taller than me?
I said there is over _NUMBER_% chance that someone at this party of all one gender and age bracket  will be taller than me. My partner says “that is not how stats works _EMOJI_.”
Isn’t it though? The birthday paradox or something _EMOJI_",Question,statistics,sgbkjd
248,"Hello! I am a computer science student and also an avid trading card game player, and I am looking for help with a statistics-related project I have been working on.
My latest project for my computer science class has involved me creating a program that would allow me to enter a population size, a number of card A, and a number of card B , and then it will deliver the odds of seeing card A and not B, B and not A, the odds of seeing neither, and finally, the odds of seeing at least one of each.
According to the people on this comment thread _URL_ to calculate the odds of seeing at least one of each of those cards, I would need to subtract the total of the three other probabilities  from _NUMBER_. When I plug the numbers provided in that thread into my program, I get the same numbers as them, but when I try different numbers, such as a pop size of _NUMBER_, a card A number of _NUMBER_ and a card B number of _NUMBER_, it gives me a negative number .
Does that not mean that there has to be something wrong with my program? Or do I just take the absolute value of that final result .
Thank you!",Question,statistics,sevws5
249,"I have data  that is log-normally distributed. 
I have been told that incorporating the measurement uncertainty of each result into the model would be difficult as the measurement uncertainty is normally distributed.
As the error distribution is symetrical but the overall model is lognormal, the lower range has more impact than the higher.
_NUMBER_. Is this true?
_NUMBER_. How can I incorporate the uncertainty into the overall model? What should I go and learn to do this? Bayesian hierarchical model?",Question,statistics,sceymp
250,"In greater detail: given a standard _NUMBER_ card deck, where Aces are high, there is a missing _NUMBER_ card, leaving _NUMBER_ cards.
The game is literally a random draw game, heads up with _NUMBER_ players. The one who draws the higher card wins . After discovering that the deck is missing one card , the loser is claiming that the probabilities were skewed. However, I'm arguing  that the odds were the same given that it was an _NUMBER_.
Any clarification here? TIA.",Question,statistics,sftpgi
251,"**My list:**
\- US Public companies with associated tickers 
\- All public companies on that list are segmented into _NUMBER_ separate industries
\- Of note, I'm using more of a LinkedIn market segmentation 
\- Using google sheets to pull past pricing by date to get averages for each industry
**Current data point I'm tracking:**
\- Weekly Average % growth/decline for each of the _NUMBER_ industries
\- Example showing % growth above/below _NUMBER_% _URL_",Question,statistics,se8grc
252,"I am working on a discreet model  where I want to do a sensitivity analysis of the stdev while keeping the mean the same. It looks quite a lot like a poisson distribution in its current form but I cant use that for simulation since the mean/var are tied together. Same with Binomial. 
Anyone have a recommendation for a discreet model to use for that?",Question,statistics,se4yh9
253,"If there are _NUMBER_ weddings in the same month , what is the probability they are all on different weekends? How do you calculate that?",Question,statistics,sdlib3
254,"I would like to invite you all to the Sloan conference about price regimes and fractional calculus, organized with Ecole Polytechnique and the University of California at Irvine, held February _NUMBER_, _NUMBER_, at _NUMBER_:_NUMBER_ Paris time  online.
In recent years, non-Markovian modeling has received a lot of attention in pure and applied financial modeling. It is of fundamental and practical interest to understand how such non-Markovian, or fractional behavior, as observed on a macroscale, can arise as a result of stated models on the microscale. Such analysis bears similarities, but also differences, to the analysis of physical systems from a scaling limit perspective. This conference will present some aspects of the process of understanding rough macroscale behavior as a result of certain microscopic dynamics.
Our speakers will be Jean-Philippe Bouchaud](_URL_ (_URL_ and [Christa Cuchiero _URL_
You can register here: _URL_ _URL_
I think this will be extremely interesting on what is to come in the next years in financial modeling. Hope to see you there!",Research,statistics,seqgh9
255,"Initial image _URL_
# TL;DR? Just skip to the statistics below .
**Part I. Introduction:**
_NUMBER_. Many people say things like how, in standard chess, **white has a big advantage or there are too many draws**, that these are supposedly problems and then that 9LX supposedly solves these problems. Personally, while I subjectively prefer 9LX to standard, I literally/remotely **don't really care** about white's advantage or draws in that I don't really see them as problems. Afaik, Bobby Fischer didn't invent 9LX with any such hopes about white's advantage or draws. Similarly, my preference has nothing to do with white's advantage or draws.
_NUMBER_. However, some say as an argument against 9LX that **white has a bigger advantage** compared to standard chess. Consequently, there are some ideas that when playing 9LX players should have to play both colours, like what was done in the inaugural  FIDE 9LX world championship _URL_
_NUMBER_. I think it could be **theoretically true, but practically**? Well, that white supposedly has a bigger advantage contradicts my own experience _URL_ that white vs black makes **considerably less** of a difference to me when I play 9LX. Okay so besides experience, what do the numbers say?
_NUMBER_. Check out this Q&A on chess stackexchange _URL_ that shows that **for engines **
* in standard, white has _NUMBER_% advantage against black: /_NUMBER_=_NUMBER_, but
* in 9LX, white has only _NUMBER_% advantage against black: /_NUMBER_=_NUMBER_
* 
**To even begin** to talk about that white has more of a practical advantage, I think we should have some statistics that show there is a **higher** winning percentage change between white win and black win in 9LX as compared to standard.  **But actually** **'it's the reverse'**](_URL_ (See** (_URL_ maybe. **These are actually the precise statistics you see in the** (_URL_ But I think this further supports my case: I could have higher advantage as white in standard compared to 9LX even though on average my blitz standard opponents are stronger (see the 'thing _NUMBER_' (_URL_ and response [here _URL_ than my blitz 9LX opponents.
**Part III. Now let's get to the statistics:**
Acronyms:
* WWO = white vs black win only percentage difference
* WWD: white vs black win-or-draw percentage difference
9LX blitz  _URL_
* white: _NUMBER_/_NUMBER_/_NUMBER_
* black: _NUMBER_/_NUMBER_/_NUMBER_
* WWO: /_NUMBER_=_NUMBER_\~_NUMBER_%
* WWD: /_NUMBER_=_NUMBER_\~_NUMBER_%
standard blitz  _URL_
* white: _NUMBER_/_NUMBER_/_NUMBER_
* black: _NUMBER_/_NUMBER_/_NUMBER_
* WWO: /_NUMBER_=_NUMBER_\~_NUMBER_%
* WWD: /_NUMBER_=_NUMBER_=_NUMBER_%
9LX blitz  _URL_
* white: _NUMBER_/_NUMBER_/_NUMBER_
* black: _NUMBER_/_NUMBER_/_NUMBER_
* WWO: /_NUMBER_=_NUMBER_\~_NUMBER_%
* WWD: /_NUMBER_=_NUMBER_\~_NUMBER_%
standard blitz  _URL_
* white: _NUMBER_/_NUMBER_/_NUMBER_
* black: _NUMBER_/_NUMBER_/_NUMBER_
* WWO: /_NUMBER_=_NUMBER_\~_NUMBER_%
* WWD: /_NUMBER_=_NUMBER_\~_NUMBER_%
**Conclusion**:
In terms of these statistics from my games, white's advantage is lower in 9LX compared to standard.
This can be seen in that WWO  is lower for 9LX compared to standard. This is true for either the unconditional case  or the case conditioned on both sides castling . We can see that in either case the new WWO is less than half of the original WWO.
Similar applies to WWD instead of WWO.
* **Bonus**: In my statistics, the draw rate  in each colour is lower in 9LX as compared to standard.
Actually even in the engine case _URL_ in the introduction the draw rate is lower.",Research,statistics,scgf5o
256,"Hello guys,
I am running an experiment with a split plot design in agronomy. My primary factor is fertilisation and the secondary is crop species. One of my studies will only compare two out of _NUMBER_ crops. So some of these crops are side by side in the same primary plot and others not. My question is if I still run the statistical analysis as a split plot design or not  . Thanks in advance!",Research,statistics,sdtbp4
257,"Hi All,
So I am trying to build an excel formula or to solve the below excel issue -
Column A to J contain basketball players and each row represents a basketball game, so columns A to J shows how many points each player has scored per game. Column K contains the number of points scored by the whole team, and the sum of columns A to J do not total the number in K.
How do I work out which combination of players points best correlates the number of points scored by the whole team? So the answer could be the players in columns B, D and F for example, or it could be just the scores from one player. And it cannot be manual in case I want to add a larger number of players later.
Edit: Columns A to J are not the full team only a selection.",Research,statistics,sggutd
258,"_URL_ _URL_
TLDR
* Describes an algorithm for Bayesian ridge regression where the hyperparameter controlling the strength of the ridge regularization is parameterized to be approximately data translated  and fully integrated over.
* The github repository _URL_ provides an implementation, a small example demonstrating usage is available at [_URL_ and here's a comparison to more traditional conjugate prior algorithms for Bayesian linear regression: [_URL_ _URL_
Let me know what you think. Thanks.",Software,statistics,sh9llv
259,"I am currently learning and using R, which I thoroughly enjoy thanks to its many packages.
Nonetheless, I was wondering whether Julia could one day become in-demand skill? R will probably always dominated purely statistical applications, but do you see potential in Julia for DS more generally?",Software,statistics,sde682
260,"Hello smart people,
I have to make a basic survey that evaluates the lab technician instructors at my institution. The questions will generally follow the format of, “How effective is this lab technicians on a scale _NUMBER_–_NUMBER_?”
I am familiar with RedCap and have made a few basic surveys for fun. My issue is, however, that when I export the results to an Excel file, the results are too raw and unpalatable for my managers. They would prefer colorful graphs and more visual representations of the data.
I know RedCap can present the data with bar charts, it even allows us to download them. However, how can I generate a report a PDF/file that won’t require me to spend a lot of time downloading each graph from RedCap and then copying them onto a Microsoft Word document, then arranging and tediously formatting images of the graphs? Can I partner RedCap with a different program and expedite this process?
Thank you for taking the time in assisting me.",Software,statistics,scudzg
261,"Hello!
I'm a graduate student in the humanities, and I would like to gain a grasp of statistics, until the level of being able to meaningfully interpret and cite findings of studies with statistics methodologies/sections . At the moment, I am not looking to conduct statistical research myself.
Essentially, understanding correlations, probability tables, graphs, charts, etc., that show up in many useful papers.
I have thought to apply myself to this for a few months, and any tips or resources for this type of learning would be much appreciated. Thank you!",Education,statistics,sdifz4
262,"Hi all,
I am considering policy data masters programs and wanted to ask the following:
_NUMBER_. What are my chances of getting into a Columbia QMSS / CMU MSPPM type program  given my below background/lack of post-college policy work?
_NUMBER_. For those who have attended, how are the post-grad opportunities? Would you say these programs are worth the investment?
My background:
\- _NUMBER_ at a top _NUMBER_ US university 
\- worked as a paralegal after undergrad in preparation for law school, but had a change of heart and no longer want to go to law school
\- enjoy mix of qualitative/quantitative research  Would like to be compensated decently but not my primary motivator
Any insight or advice would be very much appreciated. Thanks!",Education,statistics,sg0elm
263,"Hi all, I'm a senior in undergrad studying Statistics, and I'm just trying to gauge my chances in any advanced statistics programs . I have good grades, _NUMBER_+ gpa, although my curriculum is almost exclusively in data science, CS or otherwise relatively low level statistics for a graduating senior . I have no published research in this field either, although I've tried reaching out to my professors yet somehow my uni has absolutely no relevant research going on and I don't even know how to go about starting my own project, let alone being sure if I have the competency given my background. The majority of my experience is in Financial Services, but it's not even stats related, like Analyst roles at Investment Banks. Not sure if I'd like to do that for the rest of my career, so I'd like to hear from you all if I'm even remotely competitive for grad school MS programs given my _NUMBER_.) Lackluster curriculum in terms of Math _NUMBER_.) No research _NUMBER_.) Lack of direct work experience in stats/data science. Appreciate the help guys :)",Education,statistics,sc2qf0
264,"Hey All,
I'm currently a post-bacc student working and taking courses in calculus and linear algebra to prepare for a Statistics/Applied Math graduate program, and I'm far enough in the process to start building a rough list of programs. I've found that you could credibly put programs into two categories, State Schools that won't break the bank \*as much\* vs Private Institutions that are noticeably more expensive. 
If I have this somewhat correctly, the consensus regarding State schools if that they're a bit more generous with funding and grants  but private institutions certainly have a name brand  and are in more economically competitive areas you could say 
My question is, do these private/more expensive institutions end up paying for themselves in the medium or long run, at least in a general sense, or does it depend more on the student? Is there any of type of consensus view here, or is it too varied to say?
Happy to expand on any of the above thoughts, and thank you all!",Education,statistics,se5134
265,Let me know!,Education,statistics,sfz0nd
266,"I’m a 3rd-year undergrad majoring in Statistical Computing who feels set on wanting to pursue a Master’s Degree in Statistics, Probability, or Data Science. I may even want to do a PhD later on, but right now I have no idea what particular thing I would want to focus on/research . 
I’ve done lots of searching through this and other subreddits  as I’ve been contemplating what I want to do in and after grad school. I even changed my mind from wanting to be an actuary to taking an interest in data science . 
One thing I’ve been struggling with when looking at various Master’s programs is the coursework, or apparent lack thereof. I am _not_ saying that it looks easy to me, but there’s just...so little. Most of the programs I’ve looked at  have programs that can be completed in one year, in which one would take _NUMBER_ courses, _NUMBER_ or _NUMBER_ of which are predesignated . But then when I look at the listings for grad school courses, they have like _NUMBER_ classes, and _NUMBER_ of them look really interesting to me :( 
Now, I know that these _NUMBER_ extra classes must make a pretty big difference, given how useful an MS is in the industry . However, in my head it just seems like it can’t be _that_ much, since there are so many other cool things that I wouldn’t even learn about. And I want to learn them!
This kind of bleeds into my reason for not considering a PhD right now. I can’t decide at all what is most interesting to me, and am instead trying to learn everything, which I know is a mistake . 
So my questions are:
_NUMBER_. Any advice on how to choose my courses? I think my preferences will become clearer through my coursework and  internships over the next year and a half, but it feels so sad to have to “limit” myself 
_NUMBER_. Is it feasible/wise/common to get multiple Master’s Degrees ? Especially if they can be done in a year, I think it may not be a horrible idea . If not, will I learn a lot of stuff from the classes I don't/didn't take in my career ? Or is it just not that important?
_NUMBER_. Any advice on how to choose a program? Using UofT as my basis again, my choices would be Applied Statistics, Probability, and Data Science. I really don’t want to choose Data Science , because I want to do as much coursework as possible, and I am skeptical about how well-paying the internship at the end will be, given that it's tied to the university 
I hope I don’t sound presumptuous or anything :( To be honest, I’m not even confident that I’ll get into grad school , nor that it will be easy if I do. 
Thank you for reading all of this, and I would really appreciate any guidance!",Education,statistics,sdbkbt
267,"I was talking to a friend recently about stats, and p-values came up in the conversation. He has no formal training in methods/statistics and asked me to explain a p-value to him in the most easy to understand way possible. I was stumped lol. Of course I know what p-values mean , but I couldn't simplify it. The textbooks don't explain them well either.
*How would you explain a p-value in a very simple and intuitive way to a non-statistician? Like, so simple that my beloved mother could understand.*",Discussion,statistics,sfbea0
268,"In a essay about the meaning of life, the famous scientist Schrodinger once said ""*Physical laws rest on atomic statistics and are therefore only approximate*"" .
At the same time, I have always wondered about the origins of different laws that are used in the natural sciences and engineering disciplines. For instance, consider the following:
* **Young's Modulus**: Young's Modulus  - dating back to the year _NUMBER_ - is a mechanical property that measures the tensile or compressive stiffness of a solid material when the force is applied lengthwise. It quantifies the relationship between tensile/compressive stress  and axial strain  in the linear elastic region of a material and is determined using the formula.
Mathematically, Young's Modulus can be expressed as : **E = sigma / epsilon**
Even to this day, we can continue to see the ""approximate and data based nature"" of this ""theoretical concept"": _URL_ _URL_
**I have always wondered how the formula for Young's Modulus was determined at the time.** If I were to speculate, using today's terminology - I would guess that:
* A ""parametric form specifying the relationship between the entities of interest""  was decided, possibly based on some ""**prior** hunches"". For instance, **E1 = sigma / epsilon** vs. **E2 = sigma + epsilon**, vs. **E3= sigma + epsilon + some\_constant\_c** , **E4 = sigma\^_NUMBER_ / epsilon** etc.
* Next, data was probably collected through a series of experiments in which measured the resulting ""stiffness""  for different values of ""stress""  and ""strain"" .
* Finally, different candidate ""models"" for Young's Modulus were compared against the data collected from these experiments. The ""model"" which performed the ""best""  was likely selected as an early form of Young's Modulus.
Thus, can some please tell me - **is it unreasonable to believe that the ""idea of regression"" might have long predated its formal inception, and  many theoretical laws in science and engineering have their roots in this early ""experimental regression idea"" instead of ""pure theoretical abstraction""? Were such scientists responsible for these scientific laws actually performing some early version of Machine Learning and Cross Validation?**
Thanks!
**Sources:**
* _URL_ _URL_",Discussion,statistics,shpery
269,"Statistically speaking you have _NUMBER_ in _NUMBER_ of being attacked by a shark. And _NUMBER_ in _NUMBER_ chance of dying in a plane crash. 
It's not the statistics that matter, it's in the event of a shark attack, or a plane crash you have, generally speaking, a _NUMBER_% chance of being horribly mamed, or killed. There is almost no way you can defend yourself from a shark. You just can't get away from it. If you happen to be in a plane crash, you are most assuredly going to die. Statistically speaking you are more safe traveling by air than by car, but at least in a car you have crumple zones, seat belts, air bags, anit-lock brakes and evasive maneuvers. 
It's not an unfounded fear. You lose just about all control and defense in those situations. In those cases you have almost a better chance of winning the lottery, but if you were in those situations you have almost _NUMBER_% chance of surviving.",Discussion,statistics,sfo6u2
270,I just graduated with an undergrad in statistics and so far the only thing I see with just a bachelor's is being a Data Analyst. I have mixed feelings about this field because it seems that even people with commerce or business administration degrees can do this job. Any thoughts?,Discussion,statistics,sfi5qp
271,"Let me elaborate.
First of all, the goal of statistical analysis oftentimes is to solve inverse problems .
This means that we expect that there is a data generating process that has a particular functional form. However, this is not always the case , however the models still have good performance . Since we have no guarantees of particular functions that might be driving these processes, an explanation in the context of functions is a bit problematic. 
An alternative approach that I think is worthy of exploration is that the statistical tools are not leading to an identification of the models, but we are in fact creating the models by using these statistical tools, which is in fact data compression . This has a nice interpretation from the perspective of under and overfitting. When we underfit, we are in fact creating a lossy compression where we cannot go back to the original data anymore, but this should not be a surprise given that underfitted models usually have a small number of parameters only. On the other hand, overfitted models have a huge number of parameters, hence naturally this leads to lossless compression.
This does not mean that statistical models are not useful, but means that they are not able to lead to identification of the data generating processes.",Discussion,statistics,sfg04x
272,"from here:
_URL_ _URL_
>a probability at _NUMBER_ or _NUMBER_ is degenerate and will never change
can't quite find the comment thread anymore, but i did take a screenshot
_URL_ _URL_",Discussion,statistics,schofa
273,"Trying to improve my general mental map of the subject. For me it would be information geometry on the math stats side, some stuff in extreme value theory, and nonparametric kernel/spline/wavelet based methods. 
Where are we diving the deepest?",Discussion,statistics,shgpkc
274,"I have heard the following argument being made regarding Neural Networks:
* A Neural Network is a composition of several Activation Functions
* Sigmoid Activation Functions are Non-Convex Functions
* The composition of Non-Convex Functions can produce a Non-Convex Function
* Thus, Loss Functions for Neural Networks that contain several Sigmoid Activation Functions can be Non-Convex
**My Question:**  Can the same argument be extended to lack of Convexity of Loss Functions of Neural Networks containing several ""RELU Activation Functions"" ?
* On it's own, the ReLU function is said to be Convex.
* Mathematically, we can show that compositions of Convex Functions can only produce a Convex Function.
However, Neural Networks that contain compositions of  ReLU Activation functions make it **unclear to me how a Loss Functions that contains  ""RELU Activation Functions"" would a Non-Convex.**
Can someone please comment on this? **If compositions of Convex Functions can only produce Convex Functions - does this mean that the Loss Function of a Neural Network containing only containing ReLU Activation Functions can never be Non-Convex?**
Thanks!
**Note:** Using some informal logic, I do not think that the Loss Functions of Neural Networks containing RELU Activation Functions are generally Convex. This is because RELU  Activation Functions are generally some of the most common types of activation functions being used - yet the same difficulties concerning mon-convex optimization still remain. Thus, I would like to think that Neural Networks with RELU Activation Functions are still generally non-convex.",Discussion,statistics,segduk
275,"Big data or statistics help by companies research is being hoarded by big companies. Interesting episode I thought I’d share that discusses what big data is and why so many business are hoarding their information. 
_URL_
Description copy and pasted below:
Big data is a big deal! Today, I was glad to welcome Viktor Mayer-Schonbergeroday on the show to discuss how impactful data information and security is, along with how our mental frames change the world.
Bio: Viktor Mayer-Schonberger is the Professor of Internet Governance and Regulation at Oxford. His research focuses on the role of information in a networked economy. Earlier he spent ten years on the faculty of Harvard's Kennedy School of Government.
He has published eleven books, including the international bestseller ""Big Data"", ""Learning with Big Data"", and the awards-winning ""Delete: The Virtue of Forgetting in the Digital Age"" with Princeton University Press. He is the author of over a hundred articles and book chapters on the economics and governance of information.
In _NUMBER_ he founded Ikarus Software, a company focusing on data security and developed the Virus Utilities, which became the best-selling Austrian software product. He was voted Top-_NUMBER_ Software Entrepreneur in Austria in _NUMBER_ and Person of the Year for the State of Salzburg in _NUMBER_. He has chaired the Rueschlikon Conference on Information Policy in the New Economy and in _NUMBER_ he received a World Technology Award in the law category for his work.",Discussion,statistics,sd6ijh
276,basically the title like ML and AI jobs are dominated by masters and PhD degree holders ... wondering if it is the same here ?,Career,datascience,shp55f
277,"I'm considering a switch from a position as a data scientist to a platform engineering job. Currently at work I do data pull automation with PySpark and Airflow, but I do little actual analysis or science with it. I'm still pretty happy overall because I'm learning new things and I really love coding. 
This love of coding led me to find a job at the same company as a platform engineer. It sounds much closer to software engineering, and sounds like it is ""backend data science"", so to speak. It also has machine learning as a ""nice to have"" requirement. I would like to do more ML, so this would be great if the job actually needs it. 
These are my questions:
_NUMBER_. What exactly is platform engineering?
_NUMBER_. Would you consider the step from data scientist to platform engineer a lateral move?
_NUMBER_. Will experience as a platform engineer help me with my data science career in the future?
_NUMBER_. Will it be difficult to get a job as a data scientist again with ""Platform Engineer"" as my most recent title?",Career,datascience,sedyxj
278,"Hoping for the $_NUMBER_ of the more experienced.
I work as a data science manager now with a group that's focused on predictive modeling and analytics. I've been reached out to by a recruiter for a Director role in a Business Intelligence group. The BI group is self-described 'immature' and the current work seems to be largely around turning reports into real time dashboarding. They also want to move into ""ML and AI"" which is clearly not a short term thing and I'm confident they don't even understand what that means.
Is it a terrible idea to take a step from a data science manager and move into a group that is doing mostly BI work and trying to build out their predictive function? Or is it the career kiss of death to go into a BI group? Will I be stuck there and never be able to get out of it?
There is a money bump, of course, and also additional headcount. That part I'm interested in but I worry about being yet another BI manager.",Career,datascience,sezt2d
279,"Non-native English speaker here. 
I was thinking about getting into the non-profit sector. Maybe first doing a few freelance projects for non-profits and then joining one as a fulltime remote, or fulltime onsite employee. 
I have a BS in Economics, and an MBA. So I am trying to get into ""Data Science Lite"". Something like Business Intelligence, Data Visualization. You know things that I can transition into. But not things that would be nearly impossible to get into without a CS degree like Machine Learning Engineer, Data Engineer and such. 
I am looking for roles that have a good work-life balance, work flexibility, availability of _NUMBER_% remote work and freelance gigs. 
Can someone tell me, what roles are open to me and I should aim for if I want to join the non-profit sector?",Career,datascience,shogk0
280,"Hello,
I am currently a Sr. Data Scientist at a medium sized company  and my boss will promote to “Line Manager”. 
Now the good news is that I have some freedom in proposing a new title, and I would like your advise here.
Some background info:
_NUMBER_. We do very little DS overall. I tried to bring to life some models , and have succeeded in that, but there is virtually no DS culture in the organization.
_NUMBER_. Since we do very little DS, in the last _NUMBER_ years I found myself very busy with Data Engineering and Business Intelligence related topics, rather than pure DS. Myself and two colleagues of mine had to build everything ground up. Literally everything: from frontend development to data warehousing, to data science and Airflow automation .
_NUMBER_. If things don’t progress in the next two years , I’lll be moving on, as I have little faith in the overall organization . On a good note, I am paid about _NUMBER_% more than industry average.
Now, I have just hired a Data Engineer to join my new team and hopefully I’ll be given the budget to hire more people within the year . It will be a team with mixed skill set, not only purely DS. The next topic is which title to pick after my promotion.
Options are:
- Data science manager 
- Data science team lead
- Data science architect 
- Head of data science 
- Something else that you can recommend?
I fear that “Data science manager” is one of those title that sounds senior but it isn’t. What do you reckon? And what would you pick, considering that I might be leaving the company if things don’t take a better turn?
Thanks and sorry for the long post",Career,datascience,sffts4
281,Do you guys think graph theory and network systems are broadly crucial to a data science career?,Career,datascience,se7sks
282,"Hey folks. I’ve been working as a data scientist for the past _NUMBER_ years in the Canadian fintech industry, on top of my regular, or traditional if you may, data science/analysis or machine learning projects, I got more and more involved with building automated ETL pipelines and cloud infrastructure and ML API development. I started a few weeks ago to apply for some data/cloud engineer roles kinda just wanted to see what’s out there.... Now surprisingly and with a bit luck I guess, I received an offer for data engineer with a consulting firm... I want to pick your brains to see do people usually move from data science to data engineering or the other way around?
A bit background about me: I only have a BS degree in engineering with a minor in statistics  not trying to show off or anything and prob not important but the reason I am bringing this up is the majority of my data scientist colleagues have a master or Phd in stats/engineering/physics, I used to feel that I was just lucky to be working among them. Not saying data engineers are anything less, but I expect data engineers are more engineers and not necessary need a Phd for that matter which is why I never did a master or even interested in going back for one... honestly I was once been told if something is not working blame data engineer not the data scientist.... Lol imaging it prob would be a lot more stressful in that role...
So just want to know has anyone done this switch or the other way around and do you think you’ve made the correct move? What would you consider before making any decisions?
Thanks in advance :)",Career,datascience,sdomzm
283,"I am hopelessly lost in my new role at work. I'm an experienced analyst but find myself now working in a data lab doing financial forecasts, all of our work is in SQL. I'm not very good at SQL and have read some books but ultimately have no real understanding of it.
Some background is that I'm not formally trained in data science or statistics or anything, I'm feeling pretty intimidated by everything I'm being faced with now, Olap Cubes, snowflake, cognos, et cetera.....I can't make heads or tails of it. Is there any way I should systematically go about understanding all of this so I can presumably do.....anything.
What I'm experienced in is Excel, with some R and some SQL, but basically beginner level, I know typically people here will say this is just imposter syndrome but in this case I legit just don't know much. I am not in the same bucket as people who have MS and PhDs in this stuff. 
Has anyone else been in this position? I took this as a consulting role but it very much feels like what they actually meant was data science. Most of the work I've been exposed to thus far is running SQL scripts that give us some forecasts based on strict models but there isn't anything about how close this number is or how confident we are in it.....it just plops out. I've ran one script thus far and still have no idea how I'd even explain what it did. It seems like my team inherited scripts from _NUMBER_+ years ago and we just modify them as new customers are added.
So yeah, any advice would be appreciated, all my previous roles moving up the seniority chain sounded big and ended up always being generic spreadsheets, so I'm woefully, woefully ignorant of **real data science**. I believe I *can* learn this stuff but I've been given minimal to ineffective training and I simply don't know where to start. I have various DS books or SQL books I have in physical and online form but with no clear direction I'm feeling confounded on where to begin or what sequence I should take to become competent.",Career,datascience,sh6iqa
284,"I’m trying to get involved with some additional professional data science work outside of my _NUMBER_-_NUMBER_ as a principal data scientist.
I’m looking for board positions, advisory boards or alike, but I’m having troubles finding something. I’m fine with unpaid work.
I’m in the Bay Area currently. If anyone have some suggestions on how to make this happen, I would very much welcome it!",Career,datascience,sf6egy
285,"I recently finished an MBA with an emphasis in data analytics, and while I'm not a math genius by any stretch, I'd like to apply what I learned to my job in comms/marketing. Unfortunately, every supervisor I discuss this with has very little interest, which I find strange considering how much analytics is used in marketing and comms. 
Is there a way to sell this that I'm missing? I'd like to use, for example, historical social media data to determine what works, what doesn't, best time to post, and other basic measurements to connect this information to sales.",Career,datascience,seyxnl
286,"I'm wondering if anyone can share an experience of moving from an IC  - to an Executive level position ? 
As I progress in my career, I'm finding I'm much less interested in continuing to write papers and do research, and more interested in being involved in upper level decision making. I'm already on track to transition into a management position, but from there it seems like an impossible gap to make Director or VP.
I'm looking for some anecdotes of people who've moved from an IC to an Executive role, either passing through management or not. Did you find you needed to move between companies? Did you need to get an MBA? Is this actually impossible and I'm stuck in IC/technical management land?",Career,datascience,sed0ne
287,"I’m working on getting into tech and interested in data analysis/science. However, someone mentioned that I would be a good fit for a Junior Cloud Practitioner role. However, I can’t really find info on what this role is and what one does. Any info would be greatly appreciated!",Career,datascience,seboii
288,"Hey all
I just defended my PhD in biomed engineering, w/ a focus on medical imaging. I'm starting to look for full-time Data Science roles, and am looking for some career advice.
Towards the tail end of my PhD, I was interning at a startup biotech company involved in tissue engineering, imaging systems, etc. Originally, I was hired on as a ""Data Science Intern"", but the day-to-day work was / has been / is still primarily related to software engineering / testing / etc., and building up their software platform. The general idea was that this company was planning to ""build up"" their Data Science team -- once I defended my PhD, and when / if I was hired on full-time, I would be their first ""official"" Data Scientist.
In the mean time, while they've been putting a job offer together, I've also been applying for other Data Scientist roles at more established biotech companies that have actual Data Science teams . I'm pretty far in the interview process with one, and am scheduled to start interviews with one more. Ironically, given what people have been saying about it being an employee's job market right now due to Covid, interviews have been hard to come by.
**\*I don't have any offers yet\***, but I'm not sure how to weigh size of company, size of DS team, etc. against the prospect of being able to essentially ""start"" the team at another company. I want to be able to learn certain skills, like team management, various software tools, and industry best-practices, and I'm concerned I won't get that at a startup of this size. But I'm also drawn to the idea of building up something from scratch and think that would be a valuable resume builder, and could also boost my career.
**tldr**: when faced with the choice\* of working as a Data Scientist for a small or a bigger company, what concerns do you have? What things do you value? Why?",Career,datascience,sf5a4x
289,"I graduated from a master program some years ago. Let's pretend the degree program was called ""MS, Data Studies."" Two years later, the school changed the name of the program to  ""MS, Data Science.""
The program itself has not changed. I can see the same curricula and specialties on the website. But obviously the new program name is way the hell better than the old one.
If you're me, do you use the new name when talking to people and on your resume? Or do you stick with the old one so that the program name matches your grad year?
Thanks for any tips, DS friends.",Career,datascience,sdh6e4
290,"I am currently working as a data scientist for a logistics partner for a Tier _NUMBER_ tech company. I am expecting job offer from a tier _NUMBER_ financial services company for the position of business intelligence.
Would choosing business intelligence be a good idea or a bad idea? How is the future path looking for me if I accept this offer.
I understand that business intelligence is descriptive analytics while the data science role is predictive. 
I enjoy both but what would pay me more in future and what positions can i transition to from the new role.
Any guidance is appreciated, thanks!
A little background on me:
I’m a recent graduate with masters in computer science, 
_NUMBER_ years of data science experience, led teams in all positions.
_NUMBER_ publications, _NUMBER_ focused towards financial sector, other towards statistical analysis in geology.",Career,datascience,sdsocz
291,"I’m considering starting a project in Dash and I’m just curious how it compares to Shiny, from a functionality, scripting and usability perspective. I learned Shiny last summer and the learning curve was mildly steep, but once I understood the fundamentals, became pretty intuitive. 
I really liked the functionality of the front end dashboard-like interface, and am wondering how Dash stacks up, or if there is another Python equivalent that would be better.
TIA.",Tooling,datascience,sfj1gl
292,"Across my career as DS, I've come across differing opinions on Tableau. To be honest, I hate it but it seems enterprises and some people love it and swore by it; maybe due to its aggressive marketing and almost turnkey approach on dashboarding.
I also can't believe the license costs. It's like an invitation to having a sunk cost mentality when your management decided to purchase Tableau for a year.
As a user, I hate that it is not intuitive like other dashboarding tools. You have to jump through many settings and even code yourself just to implement a visual that only requires a single click in other tools.
There is also a lack of serious competitors that isn't cloud-locked . I also find no open-source alternatives that rivals the visual fidelity and ""enterprise""-readiness of Tableau. I've tried Superset, Metabase, and Grafana but they are not at the level of Tableau yet in my opinion. 
What's your take on Tableau? Interested to hear your thoughts on this.",Tooling,datascience,sgrsdy
293,"I'm trying to import a file named in.dat into python which is an int16 file, however the file is giving some values of greater than _NUMBER_ which seems odd since int16 can only house numbers _NUMBER_ to _NUMBER_. Some of the values are correct!
import numpy as np 
f = open 
a = np.fromfile 
print
The output returns:
\
I have tried testing this in other programming languages e.g. c and MATLAB but obtain the same results.",Projects,datascience,shexex
294,"Hi Reddit,
Sorry if this is not the right place to ask. 
Im doing a MSc data analytics with zero programming background, and am required to create an agent based model using Netlogo. 
I’ve been looking at several examples, but they seem quite contrived in the coding, and not suitable for my level of skill. 
By any chance, do you have any ideas for an agent based model I could create that it’s not too complicated?",Projects,datascience,serc2u
295,"I need some help with a unique job posting.
I am working on posting a job for a  firm that does sports handicapping.
Currently all handicapping is done via manual process and excel sheets. They are VERY successful but want to try and modernize and automate their workflow using whatever tools are available. I'm assuming some sort of Machine Learning or something.
We're talking web scraping, data warehousing, modeling, analysis, you name it.
My question is, does this seem like the sort of thing that is doable by one person? I realize that's probably hard to answer without knowing all of the details.
I feel like the job should be titled ""Machine Learning Engineer"" but I'm not certain how accurate that actually is.
Also, is a site like Indeed sufficient for this? Or should I post it elsewhere?
Thanks for any help anyone is willing / able to provide.",Projects,datascience,se2m7g
296,"Basically I have daily climate data with temperature  and I want a generalized way to predict this dataset into the future . 
In essence, I want to be able to take any temperature series for any lat/long from this dataset and quickly make a prediction x days into the future. 
In the past, time wasn't of the essence, so I used SARIMA, which provided very accurate forecasting, but now, because of the new objective I have, the runtime of SARIMA is far too prohibitive. 
In reality, doing something like taking the last _NUMBER_ _NUMBER_-day SMAs, each _NUMBER_\*_NUMBER_, _NUMBER_\*_NUMBER_, ..., _NUMBER_\*_NUMBER_ into the past within the dataset and averaging them out would be a suitable way to do quick somewhat accurate prediction given that its simply temperature data, but I am perhaps looking for something a bit more ""datascience""y and generalized. 
Thank you for pointing me in the right direction or any help",Projects,datascience,sd7qqu
297,"Hey guys! I just finished my first data science project and would love to get some feedback on it! If you have some free time I would appreciate it!
This project was the first of a Udacity course I am currently taking and thankfully they let you have free range over what data you could use and how you use it.
I want to get a job as a data scientist  so I tried to make this as professional as possible. The rest of my Github is a mess so don't flame me on it. 
_URL_ _URL_
Thanks in advance!",Projects,datascience,se0v7k
298,"I am on R _NUMBER_.2 and have been looking everywhere to try and solve why I am getting a “Java.sql.SQLRecoverableException: The network Adaptor could not establish the connection ” error when I run dbConnect. The following code i am using is below:
jdbcDriver <- JDBC(driverClass = “oracle.jdbc.OracleDriver”, classPath =  ##okay
jdbcConnect <- dbConnect
My user name and scheme are the same. Other blogs or post on stackoverflow are connecting usually to my SQL and the one post I found on LinkedIn is from _NUMBER_. I’ve reached out to our IT team that typically manages issues connecting to Oracle Database using TOAD and they said they do not support Orcale R which leads me to think they are thinking of the wrong software or I am not fully grasping the limitations we have. Any help would be awesome.",Projects,datascience,sh95c9
299,"For my portfolio of projects that I will be showcasing to employers, should they be in a Jupyter Notebooks format or should they be .py files?
I originally did them in jupyter notebook, but I don't want to come off as unprofessional. Is there a preference/industry standard?",Projects,datascience,shedzp
300,"Might be a dumb question...but in my current data analytics class, I’ve been assigned a final project worth _NUMBER_% of my grade. It involves choosing a dataset, analyzing it using R or Stata , creating multiple figures, and writing an _NUMBER_-_NUMBER_ page report on it. Would this be something I can include on my resume if I want to get into the data science/analytics field ? Or is it not significant enough to include?",Projects,datascience,shpkj8
301,"I got a job task on relations analysis and community discovery and I don't have any experience about it. 
Basically we have a set of people related by different kind of relations to other people. Some of those relations are relevant, some aren't depending on another particular relation. We want to test and predict its existence. Better if we can discover communities sharing that particular relation.
I can model each people pair as an A/B test with a set of features, but I suppose graph theory can help. 
Do you know of any useful online resources on the subject?
TIA",Projects,datascience,sfv89p
302,"This is a stats and data science cross-over question, so I hope I'm doing this crosspost correctly.
I've been studying linear contrasts and it feels like I have missed a meeting on what they actually do. I get that each row of treatment coefficients should equal _NUMBER_, and I somewhat understand that they get the variables to _NUMBER_ or _NUMBER_ or _NUMBER_, or that _NUMBER_ can be balanced by _NUMBER_ _NUMBER_ _NUMBER_ _NUMBER_ by isolating a variable but it's confusing how they do that. So with reciprocals the _NUMBER_/_NUMBER_ times _NUMBER_/_NUMBER_ = _NUMBER_, but how are they getting them to zero? Is this like a blind crossover in medical trials where the dog gets a piece of a hotdog with a pill inside with null effect vs a piece of a hotdog with a new pill that might rabies?
So hotdog with medicine = _NUMBER_, hotdog with sugar pill with essentially no effect = _NUMBER_? Then where do the _NUMBER_ come from?
The second part of this is asking is a statistical test of goodness that the rows add up to zero, else it's a bad fit and they are not parallel thus not collinear? Then if they are not linear that means the two sets of data indicate the data is not parallel to each other and not exactly collinear so one might be gradually more important than the other. So Y=mx1+2mx2+mx3+b should be more like Y=mx1+3mx2+_NUMBER_.1mx3+b when mx3>_NUMBER_ ?
The third bigger question is that does this transformation of integer variables in a linear equation to a grid become computable like a matrix calculation?
_NUMBER_. How do they get the coefficient of a variable to be _NUMBER_
_NUMBER_. What real-world use is a linear contrast good for
_NUMBER_. Is it just a 'stats professor' wanting me to know how to do something the old school way that computers do for us with code in a matrix calculation and he is prepping me with 'you need to know how this works by hand'?
This isn't a homework question, I'm not understanding the concept of its calculation or how it's used. So I've also posted a similar question in this thread in _URL_ r/Stats for the application part without photos of the below slides I'm referencing since I can't seem to post photos there.
Page _NUMBER_ _URL_
page _NUMBER_ _URL_
page _NUMBER_ _URL_
page _NUMBER_ _URL_
page _NUMBER_ _URL_",Education,datascience,sgjxj7
303,"I've noticed this is particularly the case for my main browser, chrome, whilst microsoft edge seems to be working fine. My guess is maybe it's some extension I have, but I'm not entirely sure.
Has anyone else had this problem and fixed it before?",Education,datascience,sgjqli
304,"I’m in charge of helping train a group of new Statisticians and Data Scientists. Their skills are all over the place, so we’re trying to host a few broad data science and statistics trainings for all of them to attend. I’m open to boot camps, MOOCs, university courses, etc. All that matters is that we get high quality training that is available for multiple people. Any suggestions?",Education,datascience,sdfijr
305,"I'm learning data science after working as an engineer for _NUMBER_+ years. I used to take probability in college and struggled through it. So happy I didn't have to use probability or stats for my previous jobs... until I learn you need quite a bite of stats and probability to be a data scientist.
I'm re-learning probability now and found that most of the examples were of dice or cards drawing; same thing that caused me to struggle in college then. I never played any dice game and seldom played card games. I know of them but it does take me a minute to map the scenarios in my brain. I would have to imagine 'what does this mean?', 'is it possible?', 'what if it's not?', and had to try to see other cases if it's not true . By the time the first scenario made sense to me, the lecture had gone through _NUMBER_ more examples of just  dice or card games. Ugh!! 
The irony of this is: data science look at real life events and make prediction/correlations of every day situation... aka NON dice or card games  My point is everyone struggled through stats class having to make sense of something they don't do or use daily. THEN, flip around trying to make sense of the world using concepts they have a hard time understanding because the examples are not correlate well with real life . 
We grow up and see lots of events happening and the chances of something succeed or fail given something else . Not many of us grow up playing card games days after days so our brain don't get used to see these examples being given in prob/stats classes. Yet for professors, they had to probably struggle through the same thing through _NUMBER_ yrs undergrad, researched and explain in these dice or card games through _NUMBER_ more years in PhD; so they teach what they are familiar with. That's not the same for the rest of us, especially non card/dice players. 
One upside of using cards and dices is that you have concrete numbers to calculate with. From my limited data science exposure, all models have a certain degree of uncertainty / error. Then why teach something 'absolute'  in class, when real life scenarios involves many unknown factors that contribute to the error rate of the model . 
My request is if you teach others how to learn concepts in prob/stats or data science, please use daily examples that, say, someone who has gone through at least _NUMBER_+ years of life could relate too. The brain learns best if it can connect new concepts with what it already knows . Yes you might have to really think through what examples make sense for the theory you want to teach. Please don't regurgitate what was taught to you before if you yourself had a hard time initially. 
Teaching is an art and a special skill. Many thanks to those who can turn on the light bulbs in others instead of completely break that fragile light bulbs from young minds.",Education,datascience,shfli3
306,"I came to DS from Academia, and now I am in my second role as a data scientist. This role is different from anything I had done before because it is more business focus
The question is, does anyone has a good recommendation on resources about introductory topics in Economics? I want to get a better hold of the basic concepts and language.",Education,datascience,sehsr2
307,"Disclosure I'm a beginner...
I'm a little confused with these two terms, in the case of decision trees we use the same methods of bagging and boosting for both regularizing and ensembling, so I want to know if when someone regularize the model is also ensembling it? Or what would be the difference between the two concepts and how the same methods are applied to those two different situations?",Education,datascience,sgmwc5
308,"Having read a couple of posts on here lately, there seems to be criticism in how statistics is taught at the undergraduate level.
I currently work full-time as a data analyst, while completing the undergrad statistics curriculum at a local university part-time. I pretty much have all the prerequisites to start the actual statistics and probability courses. From my conversations with fellow classmates and looking through previous course notes, there is a **huge** emphasis on computation in the 2nd and 3rd year courses.
Oddly enough, many of the 4th year courses in mathematical statistics and probability are cross-listed with their graduate level counterpart. Probably because they're more proof-based.
_NUMBER_. Is this/why is this ... rite of passage normal? 
_NUMBER_. Is there anything I should be doing? 
_NUMBER_. Part of me feels I will be wasting my time.
Edit: When I say ""computation"", I don't mean programming, but rather ""memorize formula, plug in numbers, get output"" akin to high school mathematics.",Education,datascience,scvph7
309,"I am entering data science with a data analyst skillset. I know that building data pipelines is a prerequisite to conducting analysis, but feel out of my depth with this because my degree has mainly focussed on analytics and statistics. 
When I have been exposed to using a virtual server with CLI to run Apache Airflow , I feel that I lack understanding of how systems like this are structured and that I don’t know what I don’t know about this topic.
I have coupe of questions about this:
_NUMBER_. What are the key concepts I should be familiar with to understand computer networks and systems related to data engineering?
_NUMBER_. To what extent should I have technical expertise with these systems in a data science environment, despite not aiming to be a data engineer?
For context, I am proficient in python, R, SQL + rdbms, powerbi, tableau, stats+ml.",Education,datascience,sg25er
310,"I belong to a South Asian country, and one of my biggest problems here is that there just isn't good data infrastructure. How does one do data science when there isn't even any data?
Could anyone tell me what sort of data degree I should choose if I want to make more impact in a country where good quality data is sparse? Should I become a Business Analytics expert and help companies set up ETL? Or should I be a data engineer?",Education,datascience,sfh30w
311,"I tryed many free courses, but none of them makes me to want to pursue further. I think CodeAcademy is one of the best made and I liked that study enviroment. But some online reviews says that CodeAcademy is not good at all, and before seeing that I wanted to buy the pro subscription. What you guys think? Is CodeAcademy worth to buy? Do you have another source to learn where is similar to CodeAcademy?
Thanks!",Education,datascience,shq41m
312,"I am looking for a book that talks specifically about the problems encountered when dealing with massive datasets and the strategies/algorithms/tools developed to _NUMBER_. feasibly process the dataset and _NUMBER_. manage or organize your time effectively as a data scientist
 I am looking for things like a discussion of data processing engines , the purpose of creating data pipelines , etc.
I looked on the DS Book Megathread _URL_ but from the titles, I did not seem to find quite what I am looking for.",Education,datascience,shf2yc
313,"To me I am more interested in method/algorithm development. I am in DS but getting really tired of tabular data, tidyverse, ggplot, data wrangling/cleaning, p values, lm/glm/sklearn, constantly redoing analyses and visualizations and other ad hoc stuff. Its kind of all the same and I want something more innovative. I also don’t really have any interest in building software/pipelines. 
Stuff in DL, graphical models, Bayesian/probabilistic programming, unstructured data like imaging, audio etc is really interesting and I want to do that but it seems impossible to break into that are without a PhD. Experience counts for nothing with such stuff.
I regret not realizing that the hardcore statistical/method dev DS needed a PhD. Feel like I wasted time with an MS stat as I don’t want to just be doing tabular data ad hoc stuff and visualization and p values and AUC etc. Nor am I interested in management or software dev.
Anyone else feel this way and what are you doing now? I applied to some PhD programs but don’t feel confident about getting in. I don’t have Real Analysis for stat/biostat PhD programs nor do I have hardcore DSA courses for CS programs. I also was a B+ student in my MS math stat courses. Haven’t heard back at all yet. 
Research scientist roles seem like the only place where the topics I mentioned are used, but all RS virtually needs a PhD and multiple publications in ICML, NeurIPS, etc. Im in my late 20s and it seems I’m far too late and lack the fundamental math+CS prereqs to ever get in even though I did stat MS. ",Education,datascience,seac59
314,"Hello.
I’m interested in switching careers between these _NUMBER_: data analytics, data engineering, and data science. 
They seem to be quite close to one another. I took it as DA takes current/historical info, and shares it, DE builds the systems for data collection and DS seem to look for trends going forward? 
I was hoping you could share one thing you love and one thing you hate so that I can get a different perspective of what to expect if I pursue this. :)
My goal is to get a bachelors in Data Analytics and Data Management. I would be building up a portfolio during this time, and then hopefully going for a MS in Data Science. 
Thank you!",Education,datascience,sej1no
315,"Sorry if this is the wrong place to post, but are the use of control variables a data processing technique or a statistical method?",Education,datascience,sfj5nf
316,"Newer to programming and am curious if people think cleaning data is easier in R or Python, rather than Excel? I am pretty savvy in excel and am thinking there is no way it’s easier to clean data outside excel unless you have an enormous data set that just doesn’t fit in excel. Thoughts?",Discussion,datascience,shgydi
317,"We are communicators of uncovered knowledge derived from data. And the only way we succeed in our job is when we communicate well.
If your goal is to research and develop AI and machine learning models, then go get a PhD in math or compsci. I can’t really think of any other way to do it. 
But if your goal is to research and develop solutions to problems that we all face every day, then you will have a better chance of landing a job in practically any industry around the world. I’m not saying that this is easy, but we as data scientists should think in terms of how well can we use our skills in math, stats, and coding/software to help people and businesses. Once I figured this out, it helped me get over my imposter syndrome and actually feel confident in my ability to work as a data scientist.",Discussion,datascience,sewc1z
318,"Hey everyone, I wasn’t sure where to post this, but this just eating away at me and I’m not sure if it’s because I don’t understand, or if this really isn’t how it’s supposed to work. 
I started this job a few months back, at a National lab setting . We’re in the process of writing up a paper for publication. 
A coworker decided to lead the paper and take first author. Well, said coworker actually has done nothing except create the initial outline, which consisted only of placing section headers. I filled in the majority of the content, wrangled other people into some, and in general engaged a lot with paper planning brainstorms. 
The coworker who should be leading has been been very hands off has enthusiastically declared they’re uninterested in writing at this time, that we should run with it, and he’ll serve as the overseer. And, as far as I know, still in the first author role. They’ve not mentioned stepping down from that. 
Is that normal? My experience has been first author goes to who did the majority of the work, both for the project and the paper, neither of which this coworker has done. 
I’m doing what my boss is asking of me, but I’m hesitant to do more for this paper that seemingly someone who has been very uninvolved with will stamp their name in front and submit as their own publication. 
Do I have a misunderstanding of how publications work? I want to know if I’d be out of line in not continuing to do a lot of heavy lifting on this paper.",Discussion,datascience,sdhtbq
319,"Has anyone noticed this? I've noticed this before, and now taking a course for a company compliance requirement. It happens to be on NLTK in Python but that doesn't matter. 
What I'm seeing is the green check mark after one minute, even for five or ten minute videos. This says to me that you could watch a minor fraction of each video  to say you completed it. 
By comparison, when I've taken Udemy courses I recall that it got marked as complete when you were like _NUMBER_-_NUMBER_ seconds from the end. 
Is it just me or is that problematic? So people just want to say they learned something without actually learning it?",Discussion,datascience,shdqmx
320,"Hi, The count sort is only feasible for some inputs of 'k' that are not abnormally large since the space complexity increases arbitrarily. Can it be implemented using a sparse Matrix to counter that drawback.
I've tried but can't seem to get it work, can someone provide some insight on this. Thanks",Discussion,datascience,shassl
321,"A general question about feature selection - For any model, is it correct to say an insignificant variable  in one model will very likely be insignificant in all others?",Discussion,datascience,sgm8qs
322,"Globe down of a bit of rabbit hole. In most places, this is defined as government data but in a few, it seems is not. Wold like people's thoughts on this.",Discussion,datascience,shesec
323,"Hello all, I was in my ugrad Bayesian stats class the other day when my professor started lecture off with an interesting discussion. He was talking about how nowadays there’s no frequentist vs Bayesian battle, and it’s more of a predictive vs explanatory modeling battle. Ie. Statisticians focus on interpretable modeling, Computer Scientists focus on predictive modeling. 
He went on to mention how he was at a talk and a computer scientist was saying how beta hat is dead, and no one cares about finding coefficients anymore, and the focus has shifted to y hat. Ie. Predictions > coefficients.
Funny enough, the professor said how he raised his hand and mentioned clinical trials, and the speaker had nothing to say.
It made me ponder, whether something like this is industry dependent, ie. Maybe in tech it’s more about predictions, not caring about interpretability, and maybe in biostatistics or other fields it’s the opposite.
Can anyone speak to the reality of what this computer scientist was saying?",Discussion,datascience,sfv7i8
324,"I'm in a graduate program for data science, and one of my instructors just started work as a data scientist for Facebook. The instructor is a super chill person, but I can't get past the fact that they *just started* working at Facebook. 
In context with all the other scandals, and now one of our own has come out so strongly against Facebook from the inside, how could anyone, especially data scientists, choose to work at Facebook? 
What's the rationale?",Discussion,datascience,sdzkex
325,"Whenever I find some tough spot in Python, C++ or even Matlab I can google my problem. In Excel I always fall in some SEOed page that gives a basic tutorial in something that barely matches my search. I wish excel stack overflow had more people",Discussion,datascience,sf1qrd
326,"Hi everyone,
My question is stated on the title. Thank you very much for your answers.
Have a nice weekend.",Discussion,datascience,sezcx5
327,I use a lot of Twitter and would love to follow people/accounts related to what we do.,Discussion,datascience,sg2daz
328,"I'm comparing performance of neural networks to linear regression for several problems - some linear, some not - to make a point to some non DS folks.
One thing I can do with NN is give an n-dimensional vector where each vector component is continuous and get back an n-dimensional vector that ranks the values . For example, input = \ -> output \
I want to say, there is no analogous way to do this with linear regression - meaning, I cant even do it badly / with low accuracy. Am I wrong?",Discussion,datascience,se6zik
329,"I believe that companies are waking up to the fact that the time of Data Engineers is better spent on creating assets and building pipelines, not maintaining a dimensional model or optimizing a SQL query. There are many cloud products  that have optimization algorithms that can outperform any human. I think that dimensional models like star schemas, are going the way of the cube. Soon to be relegated to outdated technology. Cloud compute has gotten so cheap, and cloud storage has gotten so cheap, that the cost/benefit analysis is now incredibly obvious: spend more on good engineers, spend more on cloud compute, and get more value per dollar.
I explain more about how Google is implementing this strategy effectively if you want to read more, but the bulk of my thoughts are above \^ Learn from Google's Data Engineers: Don't Optimize Your SQL _URL_
What are your thoughts? I know this is kind of controversial, because so many people are proud of their learnings in optimizing queries.",Discussion,datascience,se0o3u
330,"Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:
* Learning resources 
* Traditional education 
* Alternative education 
* Job search questions 
* Elementary questions 
While you wait for answers from the community, check out the FAQ](_URL_ and  pages on our wiki. You can also search for answers in [past weekly threads _URL_",Discussion,datascience,sg7rx3
331,"I'm trying to see if this is normal or just unique to my firm.
I've always worked at smaller firms as a somewhat ""Full Stack Data Scientist"" where I speak directly to stakeholders, do data analysis, feature engineering, model building, and deployment to the cloud . These models are never really super custom, generally just heavily feature engineered combined with open source classical models . I then setup and host internal dashboards for model metrics and inference exploration through open source tools like plotly Dash. I lead a team with this same model.
I enjoy getting to own the process from start to finish and the variety it brings. I love the feeling of going from problem generation to solving a problem on an ongoing basis. I get really bored doing just one thing for a while.
As we recently became public, there's been a huge push for engineering to get more involved in our processes, and they've created a team of ""machine learning engineers"" that only have front end website building experience. They've begun locking down our abilities to deploy anything, adding really strict processes, and making everything take ten times longer while having less accuracy .
Probably too much backstory... But is this normal for bigger firms? I'm thinking it might be time to look elsewhere but if I'm always going to be stuck in one area . I don't know what to do... I'm not super specialized in either area.",Discussion,datascience,sg1ewv
332,"I'm working on this project at work. We are building a dynamic microsimulation model to try to forecast how many different events our customers will have and therefore, off the back of that, we will be able to understand how many people we need to employ to action these events.
I have been tasked with understanding the probabilities of different customers having each of these events given their/given a combination of their characteristics.
I have opted for a decision tree approach to determine what combinations of characteristics are most important and therefore what the probabilities of having events are based on their characteristics. These probabilities will ultimately get built into the model.
However, the data that I am using to construct the trees is a bit confusing. We keep monthly data on customers. So if someone remains a customer for _NUMBER_ months we have _NUMBER_ frames of data and _NUMBER_ frame of data if they are a customer for just _NUMBER_ month.
Ideally I would just take _NUMBER_ month's data and put it into the trees. The issue is that the month of the year could also be an important variable in determining whether an event happened. So instead I am using all data from _NUMBER_ .
The question I am struggling to understand is how - if at all - do I sample this data to put into my trees? .
Do I just put in all the data? I think this is probably the best option but I am worried if I am biasing my trees because someone with _NUMBER_ month's data will get a lot more representation than someone with _NUMBER_ month's data...?
The other way I'm looking at it is, ultimately I want to be able to take someone's _NUMBER_ month data and determine whether they are likely to have a range of events based on their characteristics. So surely I want one random month's data per person? The issue is then sampling. If I just select a random month's data per person, I will actually bias my sample because I will be biased towards selecting someone's early month's data - so would have to be a stratified sample based on time as a customer.
Sorry a bit waffely, but based on what I am trying to get out of the trees, and the monthly data I have, how would you adapt the _NUMBER_ data I have before putting into the tree algorithm? I.e. if I used all the data and ran a tree - say the top node basically said _NUMBER_/_NUMBER_ customers had this event in _NUMBER_, could I also interpret that as if I select a random month's data from this year, there is a _NUMBER_/_NUMBER_ chance of them also having an event? Or is that wrong because I am using multiple months of data per person if I put all data into the trees?
Thanks!",Discussion,datascience,sg6kg4
333,"I read a comment on this subreddit that said something like ""I'd bank that _NUMBER_% of people that are in this sub do not utilize anything more complex than linear/logistic regression models."" I'm kinda wondering if there's truth to that.
I know DS is so broad, but I'm asking for specific people here to chime in :)",Discussion,datascience,sexpx3
334,"So you are a data scientist, with a nice job with enough work that isn't data cleaning/ Excel manipulation to keep it both real and interesting. Obviously you took classes in linear algebra, calculus what have you to get where you are now.
Is there any maths you didn't get to learn at school that you wish you could have got to? Is there any maths you're trying to learn right now because you think you need it, or just because you're interested?",Discussion,datascience,sfgdvs
335,"It’s becoming more and more common to have _NUMBER_-_NUMBER_ rounds of screening, coding test, case studies, and multiple rounds of panel interviews. Lots of ‘got you’ type of questions like ‘estimate the number of cows in the country’ because my ability to estimate farm life is relevant how? 
l had a company that even asked me to put together a PowerPoint presentation using actual company data and which point I said no after the recruiter told me the typical candidate spends at least a couple hours on it. I’ve found that it’s worse with midsize companies. Typically FAANGs have difficult interviews but at least they ask you relevant questions and don’t waste your time with endless rounds of take home 
assignments. 
When I got my first job at Amazon I actually only did a screening and some interviews with the team and that was it! Granted that was more than _NUMBER_ years ago but it still surprises me the amount of hoops these companies want us to jump through. I guess there are enough people willing to so these companies don’t really care. 
For me Ive just started saying no because I really don’t feel it’s worth the effort to pursue some of these jobs personally.",Discussion,datascience,seufwd
336,"tl;dr: New data science department, not a data scientist, how should I proceed?
I've been tasked with establishing a Data Science department at my mid-sized, fairly traditional company. We have an Analytics department that focuses on descriptive analysis and data viz. My background is in operations research, so while I have some technical/stats knowledge, there's a lot that I don't know about being a Data Scientist.
I'm planning for _NUMBER_ and I know I need to build my fundamentals in Python and re-educate myself on forecasting best-practices. If you were in a similar position, how would you go about building a department like this? What knowledge do you wish you had when first starting out?
Edit:
I should've been more clear on a couple points: I am the only Data Scientist for now, my success/failure will determine the future of the 'department'. I haven't been given a specific direction other than 'start with forecasting'.",Discussion,datascience,sfx8dn
337,"Through my career and over the past few years I’ve spent significant time developing an understanding of different models and use cases in Python. I’ve been working towards a Data Science certificate from MIT and have a Masters Degree in Data Analytics. What I’m struggling with is learning how models are deployed to scale through Spark/PySpark and the likes.
EDIT: Holy guacamole I didn’t expect this to blow up the way it has. Thank you so much everyone for your help I greatly appreciate it!",Discussion,datascience,sgdnw8
338,"As the title states, I'd love to know if anyone can point to publicly available datasets especially conducive to practicing feature engineering, a subset I want to understand more deeply through practice.
By ""especially conducive"", I mean datasets that produce noteworthy improvements in model performance when moving from base to engineered features.
Thank you!",Discussion,datascience,sgewbs
339,"I was just hired as the main “data person” at a smaller company. Sometimes I get urgent requests that I need to respond to right away. I really don’t want to mix my personal phone with work, but my company does not offer phones or even to pay for a plan. What recommendations do you have for having access to information that I need right away given this context? Is it too ridiculous to buy myself another phone? Just trying to gauge how insane that would be. Thanks in advance.",Discussion,datascience,sea3lj
340,"In traditional statistics, it is hammered into us that correlation DOES NOT mean causation.
But I feel like that doesn't apply in something like a Neural Network seeing as we don't care if the input variable is causing the output variable or is merely correlated with it.",Discussion,datascience,se9g7w
341,"I, a Lead DS at a large well-known company, received a recruitment email I now believe to be a scam. I have a good deal of work experience and am often contacted by recruiters, so I didn’t think twice about this email, except that normally I don’t respond - the email was for a senior role at Petco, which, as a dog lover, I thought, “this is the dream job”. I sent my resume and availability for a call. No response since then, and I’m beginning to think this was phishing, except, what’s the scam? Everything on my resume is on my LinkedIn and public knowledge. Should I be concerned I shared my resume? 
The recruiter didn’t include any contact info or their last name, and the email had links to Petco’s website and workday app site . The recruiter’s email is paulc@paulrecruiter.com . 
Email subject: Director of Inventory Analytics & Data Science Opportunity at Petco!
Email contents:
Hi pantherapardus,
I'm reaching out about an opportunity to join Petco, a category-defining health and wellness company focused on improving the lives of pets, pet parents and our own Petco partners. We are seeking a Director of Inventory Analytics&Data Science to join our team and thought you would be a great fit.
Since our founding in _NUMBER_, Petco has been trailblazing new standards in pet care, delivering comprehensive wellness solutions through our products and services, and creating communities that deepen the pet-pet parent bond.
If you’d like to learn more about this exciting opportunity, please let me know by sending over an updated copy of your resume and I'd be happy to set up a call to discuss further! Let me know if you have any questions, look forward to hearing from you!
Cheers, 
Paul
Guys, wtf is going on here? Or is this maybe legitimate and I’m just not qualified ?",Discussion,datascience,sdh7nh
342,"Hey want to show these datapoints
* Health/nutrition
* Communication
* Friends/Family/relationships
* Well-being/Happiness/positivity
* Openness
* Goals/motiviation
* Compassion/Empathy
* Career/money
* Health/fitness
* And maybe others
Each datapoint would maybe be from _NUMBER_-_NUMBER_, or _NUMBER_-_NUMBER_ or _NUMBER_-_NUMBER_ scale
Which of these graphs charts  do you think would be better to show/viz this? For visual clarity?
_URL_ _URL_
 - a site that lists/ranks graphs charts would've been so much better and more helpful
This site also has a bunch of graphs charts that arent really good to show data
Currently thinking of horizonal bar chart but wanna know what are other good options? For clarity
For 'infographic' if it helps you decide on which graphs/charts are better _URL_ _URL_",Discussion,datascience,seu1no
343,"A reasonable amount of my degree was 'advanced' Bayesian methods like LS-SVMs _URL_ gaussian processes and various kinds of graphical models: DAGs, Markov networks and their combinations. I've honestly never ever bothered using any of these in my career for two reasons:
_NUMBER_. Most of the material was just math and the little code was in MATLAB and just haven't committed to learning STAN or pymc3. \*
_NUMBER_. Main reason: Up til now I've always been in consulting and I will be again after the summer. Somehow I don't feel comfy leaving the customer  with something they just wouldn't understand. For this reason I mainly stick to stuff that exists either in sklearn, statsmodels, SparkML or Tensorflow.
Since I have a lot of time on my hands recently I'll go through my syllabus again and and properly learn how to properly use STAN. My main use case would be the research project I've been working on for a few months. Something like hierarchical modelling with shrinkage would work wonders here. The problem is I'm partnering with a company, I get to use their hardware to train the models using databricks but I have to deploy the best results. 
Assuming my research tells me some non sklearn / SM / Tensorflow model is really the best and most elegant solution. Should I bother with trying to explain how they work to both the data scientists that don't have a background in this.... and even worse, business? How do you experienced folks tackle this?
^",Discussion,datascience,sh39ni
344,"Hi all! I'm trying to forecast a daily time series that has many _NUMBER_ days. For example, forecasting a person's daily expenditure for every day for the next N days. Customer's expenditure as one would expect has many days with _NUMBER_ spend. 
Has anyone here dealt with a similar time series problem?
If yes, what methods would you recommend and also what metrics would be useful in this case to measure the success of a model. 
I'm relatively new to time series forecasting and any help is really appreciated. 
Thanks!",Discussion,datascience,sef8ny
345,"The hiring process for MLE and Data Scientists is broken. After _NUMBER_ applications in _NUMBER_ months, \~_NUMBER_ of them with referrals, most had custom cover letters attached and only _NUMBER_ interviews, _NUMBER_ of which were useless and waste of time mental reasoning assignments, and the _NUMBER_ at Apple, Microsoft and IBM research teams never updated after the first interview. Good mental health - See you never!! Insecurities - Hello!!
One day I was high as fuck, and that's when it Hit me this is not working, recruitment teams are not functioning well! If recruitment teams are actually working on such thin fake lines of distinctions where you give one selected person all the riches and the unselected candidates, probably good and maybe even better, who has just asking for feedback, and the HR can't make the damn interviewer cough up a couple of sentences over email about why you are not selected? At Least tell the person that they are rejected ffs!! Requesting you all in recruiters to do your job well, or please consider or be aware that AI is getting quite good at modeling human behavior, and if you don't provide value, please don't blame all those engineers who you frustrated so much later. Look around, every engineer - applied or research or software at any level hates job hunting, because they don't know what they are doing wrong. It's so ridiculous, I am laughing my ass off on it's simplicity. 
Well finally, I texted _NUMBER_ people on LinkedIn in frustration _NUMBER_ days ago, whose work I have been following and by the end of the third day I had _NUMBER_ mind blowing full and part time offers over which I am frankly overwhelmed by heavy imposter syndrome, but have decided to work very hard, so let's see...
Edit:
I am sorry if I was not clear here. The recruiters on LinkedIn don't reply, or on their emails, the conversations always get lost somewhere in b/w. The people I reached out to were either lab directors or CTOs. They replied ridiculously fast. The idea of the post was actually to promote people to reach out to their network, rather than relying on the system just like u/squirrel_of_fortune mentioned - there is luck involved for sure. So let's simply reach out to the people you want to work with, and this was news to me :)",Discussion,datascience,sfao77
346,With all the propaganda going on with Facebook the past few months do you think FB still has its same reputation when it comes to working there? If you work for them now will other companies see that as a red flag? I know in the past if you worked at FB you could transfer easily to other FAANG companies. Wondering if that’s not the case anymore...,Discussion,datascience,sd83sb
347,"Is anyone working with any organization that uses platform/software provided by Palantir?
How demanded is this? What would be it's growth in the next _NUMBER_ years in the field of IT?
I'm in a company which has this and we have no other option other than getting trained for this. I would like to know, how much of this would be useful. Hopefully it's not a niche software. It consists of every tool one can use in the field of data i.e engineering, visualization, AI etc.",Discussion,datascience,shn8qc
348,"I just caught an error where my absolute value function was nested in the wrong set of parentheses. After fixing it, I checked one analysis and the outcome had changed. Now I need to rerun a long list of analyses and inform any impacted stakeholders...",Discussion,datascience,shhhwi
349,"Hi All,
Is there any technique to find similarity between email ids. I have list of email Ids, and wanted to create cluster based on the organisation, and wanted to remove any duplicate emails if there are any.
Eg: a.b@domain.com & b.a1@domain.com ",Discussion,datascience,semq3h
350,Is there a reason to learn SQL instead of just learning dbplyr in R package?,Discussion,datascience,sesbe8
351,"Update: Added a new option, updated ranges as only _NUMBER_ options are allowed, made question specific to 'Data Scientist' with a specific range of experience.
View Poll _URL_",Discussion,datascience,sdzzoc
352,"I am in the middle of moving from Juypter to Colab cause I find it alot more easy to use and portable and enables cooperation.
I was wondering what you guys use for your data science projects?
Im more so talking about the smaller projects not the ones that would need a full IDE like VS/Spyder or Pycharm 
PS. As for my R DS friends we all know RStudio is pretty much key :)",Discussion,datascience,sfgugr
353,"_URL_ _URL_
I saw these graphs I really liked - does anyone know what software might have been used to make these graphs? E.g. R, Python, Julia?
Thanks!",Discussion,datascience,sgv7ip
354,"My goal is to find comments about stocks and create a model to classify the comment as good/bad/neutral  so that I can give an opinion for the stock. My issue is that I have some comments with multiple sentences. In some cases, all sentences are about one stock and in other cases, the sentences are about different stocks.
A simple solution to this could occur when I'm manually labeling the comments. I could group the sentences based on which stock they're about. The issue with this is that when I begin to apply the model to new comments, this won't work, as I won't be there to manually separate sentences by which stock they belong to.
Essentially I need a way to map the sentence in a comment to the appropriate subject of the sentence. Any ideas?",Discussion,datascience,sebvzx
355,_URL_ _URL_,Discussion,datascience,se40w0
356,"Hi everyone,
I am just curious how do you use time-series econometric models, let's say ARIMA, exponential smoothing, or TBATS, in real life? Have you ever achieved a successful model? What is the case?
I am asking mainly because of three reasons: 
_NUMBER_- The feature engineering aspect is somehow missing relative to machine learning.
_NUMBER_- The model structure might be very simple, especially in the case of the univariate time-series model.
_NUMBER_- Except for several cases, I always ended up with a very high confidence interval.
Also, I would love to learn if you have any experience regarding how performance of time-series econometric models are compared to xgbTree  with time-series features?
Thank you very much.",Discussion,datascience,sffyqy
357,"Hey everyone, I'm looking for small things that most business owners can do today to improve the quality of their data.
One example would be to replace ""free form"" fields from their CRM with ""closed-ended"" fields as much as possible.
Any other examples like this one?",Discussion,datascience,sesnu9
358,"It's been a few years since I took my courses in Data Science , and I want to refresh. I haven't worked on Kaggle before, but that seems to be the place to go for doing this work in ways I can demonstrate my skills to future employers / teams / etc.
I was thinking of going through The Elements _URL_ and applying the tools they go over to some simple datasets; e.g. MNIST, Titanic, etc. Then I could keep those notebooks for reference, and I think it might be a good refresher; I've got a lot of time before I hear back about PhD's for the fall.
What's your opinion? Does this sound like a good idea? If you'd like to add anything, or suppose any alternatives, I would very much appreciate it.",Discussion,datascience,sdsxah
359,"This is a question for those that freelance or do side jobs as Data Science contractors...Do most clients prefer that you work and deliver solutions inside their cloud environment or are you allowed to download their data and crunch it on your workstation?
I'm asking because I don't really understand why people are buying personal workstations unless they are just researching on the side. If you are actually doing production work for a client, wouldn't they have preferences on where you build your solution, data security risks, containers to use, etc?
As a side question, does anyone have experience with OpenMined/PySyft-spirited _URL_ workflows - where you could download encrypted versions of client data locally, crunch, and model without actually moving sensitive data out of your client's environment?
View Poll _URL_",Fun/Trivia,datascience,sgq1np
360,"I have a maybe stupid question. I'm training to become a data scientist, coming from chemistry and I have what is probably a combined one year of python coding  under my belt. I have now been invited to an interview as a software engineer.
The company looks quite interesting, also in that they hire mostly academics and they're willing to give beginners a go . They are known to offer jobs if beginners qualify in other parts of the interview, i.e. logic puzzles.
So far, so good. I was originally approached by a headhunter who then passed me on their colleague. The first time I talked to her before christmas about my qualifications, she was a bit hesitant when I explained my coding level. Fair enough.
The second time we talked in the new year I showed her the stuff I had been preparing and she reacted surprised - like not pleasantly surprised. I had a bad experience with a headhunter that tried to oversell my competence level before.
A few minutes into the preparation she got overly hyped about how many chapters I'd read in a book that she had recommended .
She doesn't have a coding background, so I should probably just ignore her reaction but ever since that, I have been doubting myself:
- one if my level is okay , and
- two if I really want to do software engineering.
The second one is not something anyone can help me with but I'd appreciate some pointers/orientation that would help me get my head out of the self-doubt loop it's currently stuck in.
Thanks a lot in advance and I hope you're all happy and healthy! _EMOJI_",Job Search,datascience,seu8xi
361,"did you go on to continue work in a field similar to the topic of your research? 
I am currently an undergrad ML researcher in remote sensing and satellite imagery and will be finishing  a little after graduation. I find this area really interesting, but I am unsure if I should try to branch out into a new field after finishing or try to continue w a firm doing related work. 
My only worry is that remote sensing and image processing is a bit of a niche area, and continuing may limit my exit opportunities down the road. But I also imagine it may be easier to sell myself to a related firm as opposed to for eg a consulting or insurance firm. 
I am curious to hear what your guys/gals experiences and thoughts.
**edit: why does this get downvoted lol",Job Search,datascience,se5z08
362,"I'm not particularly hopeful about my odds of hearing back at all because I found out about it pretty late and didn't really put my best foot forward. I'm curious to know how long it was before you received a response after submitting the application. I'm assuming I'll only hear back if I get an interview.
Bonus for those who got the internship: how many stages were the interviews and what was the timeline like after you initially heard back from Google?",Job Search,datascience,sg0c7s
363,"Looking for a way to find a bunch of data science startups. I'm trying to find a job to fill awkward gap between now and when I start grad school next fall. I've contacting startups is generally the easiest way to get the kinda interesting, remote, short term, relatively low barrier to entry mathematical modeling/AI jobs I'm looking for, but i've sort off tapped out the startups in my local area. What might be the best way to find some more?",Job Search,datascience,sd8dfn
364,"Hope it's ok to post job opportunities. Learn about LinkedIn's REACH Apprenticeship program; REACH is a technical apprenticeship program at LinkedIn that creates opportunity for every individual with the passion, potential, and drive to either develop or restart their career in engineering or technical program management. To learn more about REACH, check out our website here](_URL_ and watch this [video. _URL_
On February _NUMBER_, _NUMBER_ our applications for our June _NUMBER_ cohort will go live here _URL_ Please note that we release applications details early in order to give applicants sufficient time to prepare their application materials, and pull the application down within a week or two of posting.
**Additionally, there will be two information sessions.** **fill out this form** _URL_ **so we can send additional invitation details. The dates and times of our information sessions are as follows:**
* **REACH Info Session _NUMBER_: Monday, _NUMBER_/_NUMBER_ _NUMBER_:30pm PT**
* **REACH Info Session _NUMBER_: Thursday, _NUMBER_/_NUMBER_ _NUMBER_:00pm PT**",Job Search,datascience,shh0g7
365,"Is there anyone who could help me? I am an amateur in this area with poor knowledge of maths and a bit of knowledge in OO programming.
 I've got a few days to make an application using deep learning which has a dataset of images of a lone object in the center of it as input and the output estimation is supposed to be the density of an object in kg/ha.
 I am supposed to approximate the area/volume of an object with deep learning. 
 I measured the object weight manually so I suppose I don't need plenty of code to make proper CNN to solve this project? Are there any projects on this theme to recommend? I know there are but I need something similar. Mainly object detection is used for detecting more objects I guess.
 I suppose I just need to localize or detect the object and measure its height and weight? Any help is welcome. Thanks in advance",,deeplearning,sgbwfp
366,"I have just published my latest article in medium. It is a great attempt by Meta AI to develop a model which is able to perform well on three types of visual data . the model is based on Swin Transformer and Self-Attention. The results show that OMNIVORE has better performance than its state-of-the-art counterparts in computer vision. Please have a look and let me know if there is any point. 
_URL_ _URL_",,deeplearning,sgzh3a
367,"I'm currently working on my Masters' Thesis in Deep Learning, specifically Convolutional Neural Networks.
I need to perform multiple isolated ablation studies with different parameters to showcase how they might affect the ""algorithm"" I am proposing. The only problem is that typically when you are wanting to compare mechanisms, you would run them around _NUMBER_ times to get a mean and standard deviation. The problem is that I do not have the resources to run that many trials for each combination of hyperparameters. ***Do you think it is acceptable to run each trial once, or maybe three times each, and compare those results?***
For example, here are _NUMBER_ different sets of hyperparameters I'd like to test:
Train CNN Model for _NUMBER_ Epochs on **Cifar10** Dataset:
Learning Rate: **_NUMBER_** with BatchSize: _NUMBER_, _NUMBER_, _NUMBER_, and _NUMBER_
Learning Rate: **_NUMBER_** with BatchSize: _NUMBER_, _NUMBER_, _NUMBER_, and _NUMBER_
Learning Rate: **_NUMBER_** with BatchSize: _NUMBER_, _NUMBER_, _NUMBER_, and _NUMBER_
So in this ablation study I'm looking at the interaction between learning rate and batch size.
I'd also like to test another ablation study using the best learning rate and BatchSize achieved from the ablation study just mentioned in analyzing the correlation between the final validation accuracy of the Full Dataset and Half the dataset, _NUMBER_/_NUMBER_ of the dataset, and _NUMBER_/_NUMBER_ of the dataset.
As one can see, these ablation studies will take up a long amount of time, and I haven't even gotten to test my actual proposed ""algorithm""!
So in conclusion, do you think it is acceptable to just run each trial once to compare them in my report, or should I narrow my ablation studies so that I can run many independent trials?",,deeplearning,sfsn73
368,"Hi guys, I graduated in Mechanical Engineering in _NUMBER_ and spent the last _NUMBER_ years on deep learning and reinforcement learning. But I find it's hard to get an internship in deep learning. Any advice would be appreciated.",,deeplearning,s8zytg
369,"Like assuming I have a very complicated function, i want to use a MLP network to fit it. Then is there a theoretical prediction that at given parameter number what is the best performance or this fitting. Or for example, I want to train a neural network denoiser, for a given parameter number, is there a theoretical performance upper bound. Thank you. Is there any key words. or paper or books recommend.",,deeplearning,sgelql
370,"Dear all,
It is my pleasure to personally invite you to join the **Continual Learning workshop** we are organising this year at CVPR. You can find a more detailed **call for participation** below.
\----------------------------------------------
**CVPR _NUMBER_ Workshop on Continual Learning ** _URL_
The CVPR _NUMBER_ Workshop on Continual Learning  aims to gather researchers and engineers from academia and industry to discuss the latest advances in Continual Learning. In this one-day workshop, we will have regular paper presentations, invited speakers, and technical benchmark challenges to present the current state of the art, as well as the limitations and future directions for Continual Learning, arguably one of the most crucial milestones of AI. 
We invite Continual Learning contributions of any kind, not necessarily related to Computer Vision. Join one of the largest gatherings of Continual Learning in the world ! 
**How to participate?** 
_NUMBER_. Apply a continual learning related paper .
_NUMBER_. Submit to our well-tailored challenge tracks.
_NUMBER_. Save the date and join the workshop! 
**Important Dates ** 
* Workshop paper submission deadline: **March 9th _NUMBER_** 
* Notification to authors: March 28th _NUMBER_
* Camera-ready deadline: April 8th _NUMBER_
* Workshop date: June 20th _NUMBER_ 
**Invited Speakers** 
* Bing Liu 
* Irina Higgins 
* Sebastian Risi 
* Zeynep Akata 
* Tyler Hayes 
* Siddharth Swaroop 
**General Chairs** 
* Matthias De Lange 
* Vincenzo Lomonaco 
* Pau Rodriguez 
* David Vazquez 
* Antonio Carta 
* Gido Van de Ven 
* Dhireesha Kudithipudi 
* Irina Rish 
* Tinne Tuytelaars 
**Challenge Description** 
For this workshop, Meta will release a novel dataset featuring short video sessions taken from an egocentric point of view. The annotation level will allow us to define _NUMBER_ tracks in which you can participate: a more “classic” classification track, and two new detection tracks. Besides making detection in CL more accessible, very generous prizes will be awarded to the best solutions! Will you be the first to define the state-of-the art on this new benchmark? 
The latest info about the challenge is available at the official workshop website: 3rd CLVISION CVPR Workshop - Challenge _URL_ 
**Important Dates ** 
 
* Beginning of the pre-selection phase : _NUMBER_ March _NUMBER_
* Opening of submission tracks: Later in March _NUMBER_ 
* Pre-selection phase ends: _NUMBER_ May _NUMBER_
* Final ranking will be disclosed in the workshop: June _NUMBER_
**Challenge Chairs** 
* Lorenzo Pellegrini 
* Zhicheng Yan 
* Chenchen Zhu 
Check out the Official Website](_URL_ for more details and contact us for any question at [contvisionworkshop@gmail.com mailto:contvisionworkshop@gmail.com! 
\------------------------------------
Kind regards,
Vincenzo Lomonaco, 
and the CLVision board",,deeplearning,se2v8v
371,"I am trying to use Librosa to analyze few audio files that was collected on a motor and then create a model using ML to detect anomalies. Some audio files have so low decibels that the loudness came out as ""-inf"". I was wondering what folks did when this happens.
Thanks,",,deeplearning,sd9uqi
372,"In the new paper Laplace Redux — Effortless Bayesian Deep Learning, a research team from the University of Cambridge, University of Tubingen, ETH Zurich and DeepMind conducts extensive experiments demonstrating that the Laplace approximation  is a simple and cost-efficient yet competitive approximation method for inference in Bayesian deep learning. 
Here is a quick read: New Study Revisits Laplace Approximation, Validating It as an 'Effortless' Method for Bayesian Deep Learning. _URL_
The Laplace code is available on the project’s GitHub](_URL_ The paper *Laplace Redux — Effortless Bayesian Deep Learning* is on [arXiv _URL_",,deeplearning,scfvbu
373,"Hi,
I’m part of an art group from Switzerland currently studying at HSLU Design & Arts (_URL_ _URL_
The group consists of:
Karim Beji (_URL_ [_URL_ _URL_
Emanuel Bohnenblust (_URL_ _URL_
Lea Karabash (_URL_ _URL_
Yen Shih-hsuan (_URL_ [_URL_ _URL_
At the moment, we are working on a project on the topic if AI can augment the happiness of humans. To answer this question, we are mainly working with chatbots. The end result is going to be an exhibition at the end of March. 
For that exhibition, we want to conduct a trial in which people from over the world chat with a chatbot to find out if and how it augments the mood of the participants. 
We would give you access to a GPT-_NUMBER_  chatbot and ask you to a) record yourself through a webcam  while you are chatting and b) simultaneously screen record the chat window. 
In the exhibition we would have a) a book with all the chats and b) small videos with your faces  to assess your mood. 
We would have a Zoom meeting beforehand to discuss everything.
Looking forward to your message!",,deeplearning,sgyojm
374,"Hello,
I am looking for a good article or a github repo that explains the implementation of Deep Sort algorithm for object tracking in videos. 
I found this repo:
Deep Sort _URL_
But can someone point me to a better article that explains the implementation. 
Thank you",,deeplearning,s9dykl
375,"Hey! We're exploring project opportunities in GAN video generation. Could someone provide some resources regarding the same, which could help us narrow our search down and even see some good work previously done in this field.",,deeplearning,sc394z
376,"What kind of AI do you think this self-learning machine was? Turing test is toast now btw. But, Turing himself remains an important figure, not just for his science, but also his bravery on other fronts.",,deeplearning,sc5ev7
377,"New AI Weekly Update on Henry AI Labs for January 31st, _NUMBER_!
- OpenAI Embeddings: what was done, issues with evaluation and cost, ideas for improvement
- Training LMs to follow instructions from human feedback
- Natural Language Descriptions of Deep Visual Features 
- GreaseLM 
- Synchromesh 
- and more! 
_URL_",,deeplearning,sh3kk1
378,"hi, I was looking for some DL models for action recognition . I could find many models for video but i found very few and not so good models for still images in respect to action recognition. 
Can you please suggest me some good/SOTA models for action recognition in still images which has a github code . Models for both image and video will also work .",,deeplearning,se793t
379,"I am trying to implement transformers in pytorch from scratch. If we feed into the decoder block what the transformer had previously generated. In my understanding the output of the decoder block should be of dimension
  
The Ty is the len of inp to the decoder. Do we avg it? bc we want it to only generate one word at a time, right? Why is the output of the decoder dependent on the inp length to the decoder?
So if we have a completion-model task, we would take a window of n words and feed some words to the encoder and let the decoder predict the next word. After it predicts during inference we feed the decoder the text, the model has generated so far. What do we input in the decoder at the beginning, because we can't use SOS token as it isn't the start of sentence?
I was following this tutorial for implementing transformers from scratch; _URL_ _URL_",,deeplearning,scj82w
380,I want to generate road network for a not existing city using road networks of existing cities. What architecture should I use?,,deeplearning,sbmavh
381,"I'm trying to tune a model, but it keeps predicting the average output value instead of predicting something like the actual output movement, because predicting the average also decrease MAE . The same problem is there with out regression metrics like RMSE. 
It's nonlinear regression so I can't use R2 as a metric.
Is there another metric or something else I can do to get the model to stop predicting the average?",,deeplearning,sam0x4
382,"I am stuck on it for few days. Can someone help with it ,",,deeplearning,s990a9
383,"For real-time applications, #**DeepStream** might be the best tool. DeepStream is built on top of the open-source multimedia framework, #**GStreamer**. Even though DeepStream has numerous advantages such as multi-streaming, compatibility with #**Nvidia** GPUs, scalability, and speed, many developers may have trouble working with its Python bindings. #Neuralet has developed and deployed several computer vision applications using DeepStream in the past year.
To explain **DeepStream Python bindings** and various functions, we have decided to write a series of articles. These articles attempt to clarify concepts by exemplifying them. Find out more about the DeepStream pipeline and its important elements and functions in the first two parts of this Neuralet article series.
_URL_ _URL_
Stay tuned for the next part, where we will show an actual use case of DeepStream Plugins.",,deeplearning,sdj80q
384,"Hello everybody, 
I am trying to find out if object recognition can be precise to detect specific object in web environement. 
 
Can I make difference between objects of same type like a ""Fender guitar"" compare to a ""Gibson guitar"" ? Or in another context, would I be able to detect a specific sculpture or painting? 
Can I train model over one specific object to be detected ? 
Do you have references to better understand if it's possible and how? 
Thanks for help \_",,deeplearning,sawd5q
385,"So, I'm working on a project where I have, say, two satellite images of the same place. One has a \*much\* better resolution than the other one. What I'm supposed to do is generate an image with a resolution similar to the better one. That part I have got figured out. I'll be using a GAN .
However, the issue is, all these images are of different dimensions, i.e. even the higher resolution images are not EXACTLY the same size. I mentioned this to my supervisor, and mentioned these solutions I had in my mind:
_NUMBER_. Crop the images to the same size. We'd lose very little information. Some images are 90x115, and some are 95x120.
_NUMBER_. Pad the images to make the dimensions same.
She told me that both methods are unsuitable, and I should use an Encoder-Decoder to make the input the same size. I am _NUMBER_% blank on how to approach this. I have tried searching a bunch of things to make sense of this somehow, but I still got nothing. I don't even know what I'm supposed to be looking for.
It would be very helpful if anyone has any suggestions on where I should start, or just a general direction to point me to, so I know how to approach the problem.",,deeplearning,sbt3xn
386,"Imagenet was trained on 224x224x3 images, and it seems like most of deep learning has tailored its research on input images of approximately this size.
I work in the medical domain, and frequently see biopsy scans on the order of 6000x30000x3  which I would like to explore with deep learning. Is there anything stopping me from running resnet on a dataset of these files? I have gpus with 40gb vram available. 
Edit: This is for image **classification** purposes. I have one label per giant image. Crops/downsampling is not an option as the necessary features are located at the extreemely high resolution level, and may only be expressed on a small portion of the input image.",,deeplearning,sdgzfq
387,"\- Excited to share our latest survey paper on the applications of Transformer models in Medical Imaging by covering more than _NUMBER_ papers and a diverse set of applications including segmentation, detection, classification, registration, reconstruction, and clinical report generation.
\- For paper and related github repository please check _URL_ _URL_
_URL_",,deeplearning,scp67d
388,"I have just published a new article in the medium that is associated with computer vision. It is a considerable attempt to reduce computational costs and consequently time consumption. In this research, random patches of the image were removed to make training simpler with efficiency which makes it scaleable in computer vision. It includes an asymmetric encoder-decoder architecture (Also, I have added Python code for both main subsets of architecture ). This pre-training model is really practical in real-world computer vision. Hope you give it a thorough reading and consider it in your own projects. If my articles sound interesting to you, you can follow me on medium. :) 
_URL_ _URL_",,deeplearning,sbk9v8
389,"What do you do after mastering image editing? One possible answer is to move on to video editing, a significantly more challenging task due to the inherent lack of temporal coherency in existing inversion and editing methods. Nevertheless, Rotem Tzaban and the team at The Blavatnik School of Computer Science and Tel Aviv University show that a StyleGAN is all you need. Well, a StyleGAN and several insightful tweaks to the frame-by-frame inversion and editing pipeline to obtain a method that produces temporally consistent high-quality edited videos, and yes, that includes CLIP-guided editing. With the overview part out of the way, let’s dive into the details.
Full summary: _URL_ _URL_
Blog post: _URL_ _URL_
Stitch it in Time _URL_
arxiv](_URL_ / (_URL_ and follow me on [Twitter _URL_ for weekly AI paper summaries!",,deeplearning,sda7c1
390,"I wrote a step-wise tutorial to demonstrate the steps required to deploy an ML model using AWS Lambda, GitHub Actions, AWS API Gateway and using Streamlit to access the model API through a UI.
Check out the blog here - _URL_ _URL_",,deeplearning,say24p
391,"Hello 
So I am trying to train Faster RCNN from tensorflow model garden and I want to monitor some evaluation metrics while the model is training. I saw some tutorials/documentation and to evaluate model while training requires me to run two processes  simultaneously which is not possible in Colab as I can't run two cells parallelly in Colab. I don't have a powerful machine to run it locally.
Is there a way to run two cells parallelly in Colab? or Should I first finish the training job and then evaluate metrics for different checkpoints?",,deeplearning,s95iv5
392,"Hello, assume I am building a ResNet Convolutional Neural Network and that I have only _NUMBER_ layers to use.
Would it be better to build the architecture by using _NUMBER_ layers per block?
So _NUMBER_ layers for block A with _NUMBER_ filters, then _NUMBER_ for block B with _NUMBER_ filters, ..., and then the final _NUMBER_ layers for block D with _NUMBER_ filters.
OR
Would it be better to build the architecture by decreasing the number of layers per block?
So _NUMBER_ layers for block A with _NUMBER_ filters, then _NUMBER_ for block B with _NUMBER_ filters, then _NUMBER_ for block C with _NUMBER_ filters, and then _NUMBER_ for block D with _NUMBER_ filters?
I'm trying to keep the number of trainable parameters small so the latter option seems more reasonable but I'm wondering if the accuracy will decrease.
Any suggestions or intuitions?",,deeplearning,sh6is5
393,"Any feedback on the Gigabyte G5 KC-5ES1130SH Intel Core i5-10500H/16GB/512GB SSD/RTX _NUMBER_/_NUMBER_""? Which other alternatives? around _NUMBER_€
What about running UBUNTU in it? Any feedback?",,deeplearning,seue6w
394,"I am trying to create an AI name generator that will take in input from users answering a series of questions to narrow down to a name that they may like. I am not trying to just use a list of existing names. The AI will ask questions on different aspects of a name  and create a name based on the input. I plan to use a decision tree algorithm similar to the app Akinator.
What I need help with is knowing what questions/aspects would be good input to generate a realistic yet unique name. I had an idea for part of it to work like the app Tinder where the user would swipe right or left on names that they like or dislike and over time the AI could distinguish a pattern in the names that were liked, and this data combined with other input from questions would give the AI enough data to create something that will sound good to the user.",,deeplearning,sbrkzu
395,Go get some sleep,,deeplearning,seu8fx
396,"Hi y'all!
I have a question about using my rig filled with 3090s to train DL models ""on a budget"" .
I've been training mostly classifiers  and fp16/tf32 precisions . I've very much hit the PCIe bottleneck and it does scale ok to some extent but it is visible in nvidia-smi that the GPUs are waiting for each other to transfer data.
Would connecting _NUMBER_ of GPUs  via NVLink would speed up training? I suppose mostly the gradient sync / averaging is a bottleneck part for me. I see a problem would still persist when sending gradients to all other GPUs which are not nv-linked and the speed would be the same. I'm having a debate with data scientists if this will work.
I need to have some evidence that it might work  to suggest burning a money to buy at least _NUMBER_ of them . For starters I would use them in our 4x or 3x3090 rigs, but we also own _NUMBER_+ ones .
I've used latest pytorch and condas. Ubuntu _NUMBER_.
nvidia-smi looks like this :
nvidia-smi _URL_
and topology:
_URL_
The system has 2TB RAM @ 3000MT/s. CPU is at \~_NUMBER_% usage. iotop shows _NUMBER_% utilization after single epoch since the dataset is cached in RAM after it.
Thanks in advance!",,deeplearning,sdedyo
397,"Please suggest a tool that can help visualize DNN and extract computation at each layers 
I liked ‘Netscape CNN Analyzer’, but it works only for Caffe. I’m seeking a similar tool that works with TensorFlow or PyTorch",,deeplearning,saargo
398,"I have two laptops , and it would be really nice if I could train a model simultaneously over both of them. Any way to do this?",,deeplearning,sgfd10
399,"does it matter if I use PyTorch or TensorFlow ? or even use none to learn deep learning? i want to learn deep learning but the framework aspect confusing me 
",,deeplearning,sga8ny
400,"Hello,
i have a problem and am looking for an intelligent solution in the following scenario:
I have let's say _NUMBER_ labels trained:
\-Circle
\-Square
\-Triangle
\-Line
Now, i have like _NUMBER_ images for each label, which should be really sufficient for solid classification.
In my software, i can set a minimal probability that the classification output.
If it's like _NUMBER_, then a label is only accepted if p\_class > _NUMBER_.
But what happens now if there are for example _NUMBER_ labels within an image? For example, my classification image inherits a circle and a line? Then my classification probabilities drop to \~_NUMBER_, which means that my software says there is no structure at all, which is of course a completely
undesired result.
How can i deal with this issue? ",,deeplearning,sdvhdc
401,"With the current state of the art GAN implementations, could you either ""accidentally"" or possibly via search/optimisation using facial recognition feedback, create a face that is recognisably the same person as from the training data? I did a quick Google search and nothing popped up. Has anyone published on this topic? Happy to read rather than have you summarise it for me!",,deeplearning,sb4nl4
402,"I want to start a transformer-based OCR project and after reading about Perceiver IO around when the paper came out, I thought it would make a likely candidate for the task. 
I’m not too experienced on the decoder side of transformers — Primarily I work with BERT based models. Would Perceiver IO be capable of performing region proposal in its decoder? Or will I need a RPN? 
I would envision the input to be plain images, and the output to be bounding boxes with detected characters / text. Perhaps predicting the text may require a separate network / head. 
I wanted to get some guidance from the community on the feasibility of this idea, and possibly where to start on the decoder-side of the model. Thanks in advance!",,deeplearning,she7uw
403,View Poll _URL_,,deeplearning,sg1dtg
404,"Can someone share Industrial level projects with the problem statement and approach?
Really looking forward to learn how to build production level projects from the github repos.",,deeplearning,se3njh
405,"Hey ! , I need some guidance to use Pyannote for diarizarion. Anyone who has worked on it before? 
From where do I start since there's so much to understand.",,deeplearning,scaflg
406,"What metric should I use in case of of binary object detection If my test contains some images with no label. If I use mAP, or F1-Score binary, it counts image with no label as _NUMBER_ even if model predicts it correctly.",,deeplearning,sefhev
407,"\- Excited to share our latest survey paper on the applications of Transformer models in Medical Imaging by covering more than _NUMBER_ papers and a diverse set of applications including segmentation, detection, classification, registration, reconstruction, and clinical report generation.
\- For paper and related github repository please check _URL_ _URL_
_URL_",,deeplearning,scp0hi
408,"Hello guys,
I am an undergrad computer science student and I have to select a topic for my undergrad final year project please suggest me something. I am passionate about ML and DL and had done numerous projects in the past Related to DL and ML .",,deeplearning,sc4ear
409,"I am in my final year and assigned the project to work on rnn pruning. However, I am not sure how should I go about it. 
Can anyone give any idea about which sub-topics in RNN Pruning can I work on ?
Or can help me to get an idea on what research questions to solve in this?
Should i start with some algorithm and try to imrpove on it. Or how should i go about?",,deeplearning,sac89w
410,"Hi,
My use case is anomaly detection on industrial parts images.
Some parts have visual defaults, some have not. However, we don't know what default we will find on parts.
I would to explore the current state of art solutions to see what is possible, what is not possible, what needs some research to be carried on.
**Level _NUMBER_: the basic anomaly detection model**
The most basic model is to detect anomalies. It is a binary problem. Either the parts have a default, or these are Ok. If we suspect that there is a default, we send an alert to the human operator.
**Level _NUMBER_: the continuously learning anomaly detection model**
This model is using the feedback from a human operator to continuously learn what is a default and what is not a default.
**Level _NUMBER_: the continuously learning anomaly detection model that performs defect classification**
This model workflow is the following:
_NUMBER_. It detects an anomaly
_NUMBER_. If the anomaly is already known and the uncertainty is low, it classifies it.
_NUMBER_. If not, he asks the human operator: ""Here is the anomaly on the image, is it something we already know? I can suggest that it may be a class ""corrosion mark""  or an ""oil mark"" . Or is it a new anomaly? If so, please enter a name for the new class.
_NUMBER_. It learns
**My question**
What would you suggest as ""state of art"" tools for each level? If this exists...
Thanks",,deeplearning,setba1
411,"I am currently solving a problem that has multiple outputs. For example, let’s say it classifies spam/ham and urgent/not urgent. However, these two classes are not balanced within the dataset. 
How would one go about balancing such a dataset so that the number of spam and ham instances are similar and the number of urgent and not urgent instances are similar?",,deeplearning,sacbzt
412,"I am currently working on an operation that requires unscrambling messages.
For example, consider the sentence:
***I like coding in Python and R*** 
It now reads:
***like I python coding in R and*** 
I need to find a way of unscrambling these messages.
I thought about tokenising and tagging the words  in each message and then somehow generating a syntactically correct sentence using this information.
Does anyone know of an NLP library in Python that can perform this task? I know NLTK can tokenise and tag a sentence, but is there a function to generate sentences?
Thanks in advance.",,deeplearning,sb3noz
413,"Hi everyone,
I am now working on the Deep Learning-based entity matching for the streaming data project. My job is to create an incremental deep learning model for the classification of entity pairs.
A stream of entity pairs is given to the model. With no prior knowledge of the incoming data, the model needs to train itself using the incoming streaming entity pairs . My objective here is to determine how much time and how many data points are necessary for the model to converge to the optimal point.
Could someone please assist me in finding a solution? I'm not sure how to address this issue.",,deeplearning,s9y02o
414,"I have one task like converting _NUMBER_ 3D gltf files into 2D images with python or any deep learning concept it is possible? If yes, please someone guide me, I am a beginner in 3D. Thanks",,deeplearning,sgx2oq
415,"Hi everyone!
There is an “X” of the year award in pretty much every industry ever, and ranking things is fun, which is reason enough for us to hold the first annual Casual GAN Papers Awards for the year _NUMBER_!
This isn’t going to be a simple top-_NUMBER_ list, since pretty much all of the papers I covered this year are the cream of the crop in what they do, as judged by yours truly and my imaginary council of distinguished ML experts! The purpose of this post is simply to celebrate the amazing achievements in machine learning research over the last year and highlight some of the larger trends that I have noticed while analyzing the papers I read every week.
_URL_ _URL_
Subscribe to Casual GAN Papers](_URL_ and follow me on [Twitter _URL_ for weekly AI paper summaries!",,deeplearning,sf0vzp
416,"I do deep learning as a hobby and I created this deep learning model after reading the retro blog post by Jay Alammar. THe main idea is to replace bert+database in original retro paper with a probabilistic model and sample from it.
write up: _URL_
github: _URL_",,deeplearning,segjws
417,"I have a bad habit of getting stuck in the tutorial hell. I always want to master a topic before moving on to the next topic. Hence get frustrated very easily.
Now I want to get out of this hell. I want to learn only the bare minimum math, programming and anything else to get started with DL. And I will keep learning as I go. But I want to get started. 
Help me! Thanks in advance.",,deeplearning,sel9k1
418,"For a batch of size N, each image is augmented twice with _NUMBER_ different augmentations, so we get the representation , so we end up with 2N representations.Then, cosine-similarity is calculated between all pairs - \^_NUMBER_ pairs 
Then, we calculated the normalized temperature-scaled cross-entropy loss
l is defined by -log (exp) / (SUMk=_NUMBER_-2N {1func \* exp)
Then the loss is calculated over all the positive pairs, meaning the same image (example - l + l in the loss term)
I have _NUMBER_ questions:
_NUMBER_. The first question is in regard to the loss term l that is calculated with different images of the same class.In the numerator there're only positive pairs, that their representations should be attracted. Good.In the denominator there're no combination of the exact same augmentation of the same object with itself, and there're a lot of ""negative pairs"".The negative pairs could also be an augmentation of a dog with an augmentation of another dog , and it also takes into account also the _NUMBER_ augmentaions of the same images I understand that it's ok the repel the representation of a dog from another dog, but what's exactly the logic here?Maybe the higher similarity of one dog from another would make their representation be repelled less than how much a cat would be repelled from the same dog ?I need some clarification on how exactly this loss works
_NUMBER_. Why do we need both l and also l in the full Loss function?",,deeplearning,sdeuff
419,"For the last year I have dealt with classification problems in deep learning area and I have used PyTorch. Recently I got a new job and I am going to work with object detection. What framework do you use? I have seen TensorFlow a lot.
PS. Before my dive in deep learning, I have experienced with both TensorFlow-Keras and PyTorch so I am familiar with all ",,deeplearning,scjfi2
420,"I have cuda installed. I haven't installed cudnn . 
When i check gpu support in tensorflow or pytorch they show gpu is available. 
Whats point of downloading cudnn then?
Should I install cudnn ?",,deeplearning,sa7lgm
421,"Is it possible to train the model to distinguish between green and yellow vase?

Thank you",,deeplearning,sd2l62
422,"Hi, im using scheduler with warmup, also im using gradient acummuluation.For number of training steps do i have to divide len with gradient acummulation step.",,deeplearning,s9xw1r
423,"Hey everyone, here is a presentation of some new ideas in Deep Learning research from this week! I hope you find it useful and find something interesting to read -- happy to answer any questions / field any ideas!
- CM3: A Causally Masked Multimodal Model of the Internet 
- data2vec 
- LaMDA  • PromptBERT 
- UnifiedSKG 
- Collapse by Conditioning 
- and more! 
_URL_",,deeplearning,sbojyd
424,"Hey, so for a school project I need to make a car stop when it sees a traffic light and the traffic light is red. I was able to this partially using a CNN but the problem is that it can see the light from very far away and it will react to it. My question now is how can I measure the distance so that lights that are to far are ignored
I'm working in python using tensorflow if that helps",,deeplearning,scbj61
425,"I would like to share my interesting photo/video-cartoonization project. Please check out the cool demo videos :-)
_URL_ _URL_",,deeplearning,s99sk6
426,"Let's say I have:
\- An input
\- One or more NNs that process the input, not necessarily in a feedforward way .
\- An output
Now, if I asked you to come up with a model that is both descriptive and predictive of how the aforementioned mechanisms work, what would you do? The answer in my head is not clear. It goes from solutions like ""a neural network that learns to predict the output given the input"" to more ideas like ""a dictionary with features that are saying something about those mechanisms"". 
TL;DR how do you create a good descriptive and predictive model of how information is processed by a nonlinear function?",,deeplearning,sc0elb
427,"I have pretty tough task to make one class neural network classifier. The problem is that the images of different classes are looking extremely similar, and the anomaly is not visible to human eye, so I understand why my 1C-NN doesn’t work.
But I need some solution for one-class classification. Maybe somebody can help with any ideas?",,deeplearning,scmj79
428,"I am trying to implement word2vec skip-gram model. Now the data pipeline process is taking a lot time which is around _NUMBER_ minutes. If anyone can help me optimize my implementation. 
_URL_ _URL_",,deeplearning,s9dy2g
429,"I started a data labeling startup based in the US, and I am finding it hard to reach clients . so I have some questions to people in the field
_NUMBER_. what are companies looking for when hiring their annotation vendors?
_NUMBER_. What is the best way to reach out to potential clients?
_NUMBER_. What type of industries are looking for US based workforce?",,deeplearning,scy4ec
430,"**Link:** **_URL_ _URL_
Identifies research papers mentioned in websites you visit, and the extension shows you the following details about research papers,
* Two-line summary.
* Availability of source code, videos, Reddit and Hacker News discussions.
* Popularity on Twitter.
* Conferences.
Also, we released the source code of the extension,
**Github:** **_URL_ _URL_
**We love to hear your feedback and suggestions**. Thank you all, and I appreciate the support.",,deeplearning,shpm07
431,"Here is link to kaggle notebook:
_URL_ _URL_
Hi, I've been learning math, ML and DL for the last two years everyday.  I've done a few CNNs before, but mostly followed with the book. I would like to share the first CNN network made entirely by myself and I look forward to some advice for the future.",,deeplearning,sacon1
432,"Hey guys! I wrote a short article on a few selected publications on contrastive learning in self-supervised image recognition tasks. Feel free to have a look!
_URL_ _URL_",,deeplearning,sco6wj
433,"Hi!
I'm trying to build a recommendation model but my model learns very well in train but in test the results decrease .
My number of parameters were much higher than my number of data so I tried to do data augmentation but it didn't work. I tried to add L2 regularization, dropout and batchNorm but without any success.
Do you have any suggestions? What could explain this overfitting? How to counteract it?
I take all tracks to try. Thanks !
Edit:
_NUMBER_. I implement a model that already exists, so I can't reduce the number of parameters.
_NUMBER_. With data augmentation I now have more data than parameters.
_NUMBER_. In val, the model is bad from the first iterations, so I can't do early-stopping.
Edit2 : The way in which I made the data augmentation was wrong... Everything works now! Thank you everybody",,deeplearning,safm6s
434,"Hi,
I'm wondering what type of computer vision model I can use to monitor defaults on manufacturing parts. There are some specific constraints linked with the industrial context. One is that there are potentially lots of new defects that will appear in the future and that we still have to detect them.
**The real-life problem we try to solve**
We want to replace human eyes for visual inspection of parts in a factory. The goal is to detect defects and classify them according to the type.
Let's say that we manufacture a special type of gear.
Once the design and initial tests are done, we will create industrial tooling to produce it. We will test this tooling _NUMBER_ times on _NUMBER_ gears. Most of the defaults detected during this phase will be corrected but let's say that we collect the following ones:
* Burned metal spot: _NUMBER_
* Broken gear: _NUMBER_
Then we produce _NUMBER_ samples to validate the process, we get:
* Corrosion mark: _NUMBER_
* Scratches in the teeth: _NUMBER_
We produce _NUMBER_ more and we see no new defect. So we think it is corrected.
At this time we have _NUMBER_ samples and we probably want to start training a model before launching full-scale production, if possible.
We launch the first batch of _NUMBER_ parts. There we get new defects:
* Coloration marks from a bad oil quenching: _NUMBER_
* Burned metal spots : _NUMBER_
* Scratch on the teeth: _NUMBER_
On the second batch of _NUMBER_, we have again a new defect:
* Burned metal spots: _NUMBER_
* Scratch on the teeth: _NUMBER_
* Scratches on the side surface: _NUMBER_
Etc. We may on average have one new defect per production batch on the _NUMBER_ firsts then one per _NUMBER_ batches of production.
**Nice to have features**
We are also interested in to:
* Knowing the uncertainty level for each defect detection
* Knowing the exact spot of the part where the defect is seen
* Improving the model over time
**What model can we use?**
My current bet is semisupervised Generative Adversarial Networks for anomaly detection. But:
* I'm not _NUMBER_% sure it is the best approach.
* There are tons of implementations and I have no clue on which one to start with.
Do you have an opinion on this use case?
Thanks
**Edit: Some important points**
* This is not a classical classification problem. As you can see, we will have to deal with new defects on a regular basis so we need to be able to detect new defects with **zero examples**.
* There is only **_NUMBER_ part design** that is produced in batches. All parts are **quasi-identical**.
* **The most important thing is to detect anomalies**. Is there an anomaly and where? The classification is just for stats and continuous improvement. So I'm more interested in anomaly detection models such as AnoGAN and Auto-encoders. Unless you have another type of model that does both in one shot.",,deeplearning,sgi0cy
435,"I want to train a graph neutral network with multiple graphs so it learn the best characteristics from each and then generate one  best for given constraints.
I checked multiple sources on the internet and found stuff like graphtune and gcn for learning purpose and RGNN for generation but I'm still in doubt.",,deeplearning,sgsghm
436,"I'm using u-net for binary semantic segmentation in tensorflow _NUMBER_. Let's say I'm classifying bacteria cells in a petri dish. I've trained a number of models by tuning hyper-parameters, and using different training sets.
At inference, I have a model that does great identifying individual bacterium that are widely dispersed but predicts poorly where the bacteria are grouped close together.
I have another set of model weights  that are really good at the opposite - it identifies bacteria really well in groups, but misses out on the widely dispersed individuals.
As much as I try, I can't train my architecture to do well at both.
So, what do I do? The simplest, but perhaps most inefficient approach is to inference twice with the two sets of model weights and then pass the two product tensors through tf.math.maximum.
But, instead, I could build an ensemble of the two models, or use transfer learning to stack the models, and lock weights of the first model, right? 
What's the best way to approach this? If you can, please direct me to some resources that address similar problems. Thanks.",,deeplearning,shharp
437,"We're an early stage MIT based startup company called Sync Computing. We just came out of stealth and are looking for early user feedback.
_URL_ _URL_
Hopefully this tool will be useful for people. We have a case study now of how it works with Duolingo here. _URL_",Blog,dataengineering,sdaml5
438,"We have recently released a project to generate markdown documentation for data pipelines. The tool uses yaml files to load definitions, so as long as you can export your pipeline metadata and dependencies to yaml, Lineage will be able to generate a documentation site. You can then use the rendered markdown as part of a static site using MkDocs or VuePress.
Why not Apache Atlas? Apache atlas is great as an interactive tool but we often needed something to generate static documentation and lineage charts as part of a CI/CD process.
Why not DBT? DBT has a great documentation module but requires that the pipelines are modelled in DBT in order to work.
Would love any input or feedback!
_URL_ _URL_",Blog,dataengineering,s327cq
439,"A data pipeline to analyze weather and electricity prices.
_URL_ _URL_
Would you want to use a similar data pipeline?
View Poll _URL_",Blog,dataengineering,s60oad
440,"Learn how to use Snowplow Micro to implement end to end behavioral data tracking testing in your development practices. Register here _URL_ 
_NUMBER_\/_NUMBER_\/_NUMBER_, _NUMBER_:_NUMBER_ GMT _URL_
As usual, you'll have an opportunity to ask questions at the end.
Look forward to seeing you on Jan 19th, _NUMBER_:_NUMBER_ GMT.",Blog,dataengineering,s19nl7
441,"Disclaimer: I work at Monte Carlo. 
Our team put this post together to help data engineers communicate why - and how - dashboards and reports break to downstream stakeholders. 
Next time you're pinged about bad data or a wonky spreadsheet, forward this along as an FYI. :) 
_URL_ _URL_",Blog,dataengineering,s8y33y
442,"I wrote a few thoughts on why standardization in data ingestion is a thing. My experience comes from building products for data ingestion and I wanted to share the reasons it makes a lot of sense for companies to try and introduce standardization in data ingestion.
There are two different types of arguments presented, one type has to do with the benefits of building a superior product and the other one has to do with Go to Market motions.
And I forgot something important the first time I posted. The link to the post, here it is. 
_URL_ _URL_",Blog,dataengineering,shgm73
443,"Hi all,
Often see 'how do I get into the field' posts, and whilst they're no doubt useful to some, I seldom see interview advice, learnings etc. Perhaps they just don't appear in my feed, but thought it might be useful to talk about my experiences in broad terms. 
Worth mentioning that I'm a senior DE, GCP certified among a few other certs useful in this space and I'm going for other senior roles which use a broader tech stack and can help me develop. 
Learning _NUMBER_: 
no one knows what they are looking for.
Why do I say this? Well, it seems as though each company has its own definition of what a data engineer does. It could be that in some companies a DE role involves only analytical engineering, whilst in others its pipeline management only and in others its a hybrid dev ops, pipeline and analytics engineer. 
I consider myself to have most of the relevant skills in this space but the conversations I've had with hiring managers  have been so widely varied, that it's worth familiarising yourself with the concepts of dev ops/ infra management/ analytics. 
One company stated in their job spec that they were happy for someone to have an understanding of Kafka and would be trained on the job, whilst in fact wanted a streaming expert. So whilst I had already recognised this an area to develop for myself, I would say that you should be more than familiar with streaming concepts  if streaming is in the job spec. 
Learning _NUMBER_: 
Have some code ready to discuss with your interviewer. 
My recruiter got in touch with a position last week and followed it up a few times. I've had other recruiters do this amazing thing called prep and have had them run through a list of things we'd be doing in the interview. I asked this particular recruiter about this and they replied but just the day before, and told me that they wanted me to go through some code with the hiring manager. Lucky I had something I could share, but I would suggest you have a personal project ready just in case. 
Learning _NUMBER_: 
People are using AWS more than any other cloud. 
Not a problem, just an observation. 
Learning _NUMBER_: 
Some hiring managers are just there to feel good about their _NUMBER_ years experience and shit all over you. 
It's worth being ballsy with these people and start asking them technical questions in return. They may have a solid understanding of architecture but they won't know it all. Just because you don't know the answer to their question doesn't invalidate you. I had to ask a hiring manager WHY the problem they were asking was even designed that way. Of course you're there to evidence your skills but make sure you challenge, even if its just for you. 
Learning _NUMBER_: 
Most of the directors I spoke to are fucking clueless. 
Learning _NUMBER_: 
Make sure you brush up on your basics. 
I've not interviewed for a while and my mind went blank when I was asked about functional programming. It's one of those things one might read over in a document or whatever but commit that shit to memory. Other basic questions I identified as the 'basics' was 
How do you define structured / unstructured data?
What makes a database relational? 
What is OOP? 
What is setverless? 
What is distributed processing? - this came in various forms. 
What is insertfiletype? When would you use this file type? 
How do you describe denormalised data? 
These are questions that at first, I found profoundly tricky answering because I didn't have any nice quick answer for. But after a quick Google for some consolidation and revision , I was able to better summarise. I could certainly tell you about parquet, but is it useful to know that it was created by Apache? Probably not. 
Learning _NUMBER_: 
ALWAYS make sure the company you're going to will support your personal development beyond the scope of your role. 
If they aren't prepared to do this, they are too corporate and bureaucratic and often will never flex for you. As a data engineer, you're in demand and can call the shots. 
And finally, out of the _NUMBER_ I've been on I think _NUMBER_ went well. I've been invited back for _NUMBER_ second rounds already. _NUMBER_ of these interviews were today alone andi think all went really well, so we shall see. I'm not in any rush to leave the company im at, I just wanted to see if I could fly.
Edit: thanks all for the engagement. Just wanted to mention I'm in the UK. I'm sure much of my experience can be transferred to any location but just thought I'd mention as the processes might vary.",Blog,dataengineering,sexcgm
444,"_URL_
In the long-awaited, WorkflowAsCode function is finally launched in version _NUMBER_.2 as promised, bringing good news to users who need to dynamically create and update workflows in batches.
In addition, the new version also adds the WeCom alarm group chat message push, simplifies the metadata initialization process, and fixes issues that existed in the former version, such as failure of service restart after forced termination, and the failure to add a Hive data source.
# New Function
## WorkflowAsCode
First of all, in terms of new functions, version _NUMBER_.2 released PythonGatewayServer, which is a Workflow-as-code server started in the same way as apiServer and other services.
When PythonGatewayServer is enabled, all Python API requests are sent to PythonGatewayServer. Workflow-as-code lets users create workflows through the Python API, which is great news for users who need to create and update workflows dynamically and in batches. Workflows created with Workflow-as-code can be viewed in the web UI just like other workflows.
The following is a Workflow-as-code test case:
`# Define workflow properties, including name, scheduling period, start time, tenant, etc.with ProcessDefinition as pd:` 
`# Define _NUMBER_ tasks, which are all shell tasks, the required parameters of shell tasks are task name, command information, here are all the shell commands of echo` 
`task_parent = Shell` 
`task_child_one = Shell` 
`task_child_two = Shell` 
`task_union = Shell` 
 `# Define dependencies between tasks` 
`# Here, task_child_one and task_child_two are first declared as a task group through python's list` 
`task_group = ` 
`# Use the set_downstream method to declare the task group task_group as the downstream of task_parent, and declare the upstream through set_upstream` 
`task_parent.set_downstream` 
 `# Use the bit operator << to declare the task_union as the downstream of the task_group, and support declaration through the bit operator >>` 
`task_union << task_group`
When the above code runs, you can see workflow in the web UI as follows:
`--> task_child_one` 
`/ \` 
`task_parent --> --> task_union` 
`\ /` 
`--> task_child_two`
## _NUMBER_ Wecom alarm mode supports group chat message push
In the previous version, the WeChat alarm only supported the message notification; in version _NUMBER_.2, when the user uses the Wecom alarm, it supports pushing the group chat message in the app to the user.
# _NUMBER_ Optimization
## _NUMBER_ Simplified metadata initialization process
When Apache DolphinScheduler is first installed, running create-dolphinscheduler.sh requires a step by step upgrade from the oldest version to the current version. In order to initialize the metadata process more conveniently and quickly, version _NUMBER_.2 allows users to directly install the current version of the database script, which improves the installation speed.
## _NUMBER_ Remove “_NUMBER_”  in complement dates
Removed the “_NUMBER_” day in the complement date to avoid user confusion when the UI date always displays _NUMBER_ when the complement is added.
# _NUMBER_ Bug Fixes
\ fix logger memory leak in worker 
\ Compatible with historical version data source connection information 
\ Memory constraints cause errors when upgrading from _NUMBER_.5 to _NUMBER_.2 
\ Service restart fails after a forced termination 
\ Process definition version create time is wrong 
\ Failed to execute PROCEDURE node 
\ Add default configuration of quartz and zookeeper in common configuration items 
\ In the dependency node, an error is reported when there is an option that does not belong to the current project 
\ Workflow replication error 
\ Workflow is always running when worker sendResult succeeds but the master does not receive error report 
\ H2 in Standalone Server will automatically restart after a few minutes, resulting in abnormal data loss 
\ Error reported when executing MySQL table creation statement 
\ Dependent node retry delay does not work 
\ Failed to add a Hive data source
* Download: _URL_ _URL_
* Release Note: _URL_ _URL_
# _NUMBER_ Thanks
As always, we would like to thank all the contributors  who have worked to polish Apache DolphinScheduler _NUMBER_.2 as a better platform. It is your wisdom and efforts to make it more in line with the needs of users.
_URL_
# The Way to Join
There are many ways to participate and contribute to the DolphinScheduler community, including: 
Documents, translation, Q&A, tests, codes, articles, keynote speeches, etc.
We assume the first PR  to contribute to be simple and should be used to familiarize yourself with the submission process and community collaboration style.
* So the community has compiled the following list of issues suitable for novices: _URL_ _URL_
* List of non-newbie issues: _URL_ _URL_
* How to participate in the contribution: _URL_ _URL_
* Community Official Website 
_URL_ _URL_
* GitHub Code repository: _URL_ _URL_
Your Star for the project is important, don’t hesitate to lighten a Star for Apache DolphinScheduler _URL_ _EMOJI_️",Blog,dataengineering,s23dyo
445,"Secoda can now be synced with a Git repository, letting you customize how you develop and deploy Secoda and help your team adhere to application development lifecycle best practices. Sync Secoda to a Git repository, so you can manage Secoda workspace as code.
Data teams have been speaking about managing data as code and treating data as a product. Lots of conversations in _NUMBER_ circulated around bringing software development best practices into data. With this new feature, we allow data teams to manage their data catalogue the same way software engineering teams manage products. Although this is a V1 of the feature, we think it's a monumental improvement in the workflow in Secoda. In the future, we're hoping to build the ability to create full Dev/Staging/Production Secoda states to bring the best practices for version control and knowledge management to your data knowledge.
With this approach, data engineers can manage the version history of Secoda as well as manage their own data in their own instance. Teams can start monitoring the changes across Secoda and retract any changes before they impact the master branch. In the future, this will allow data teams to run tests & perform QA on the staging instance while end-users can access the application on the production instance.
Here's the full article on this feature: _URL_ _URL_",Blog,dataengineering,s2dulf
446,"I wrote a post on how we can leverage open source entity resolution with TigerGraph to solve for AML, KYC, Customer _NUMBER_ and other analytics scenarios. Hope you find it useful.
_URL_ _URL_",Blog,dataengineering,s97w0b
447,"In recent years, data modeling has seen something of a renaissance within the data landscape. Transformation  is on everyone’s lips, and powerful solutions like dbt _URL_ in combination with leading ingestion tools like Snowplow, have unlocked the power of transforming data to thousands of organizations. Data practitioners spend a lot of time thinking about how best to model their data, the best tools to use and the talent to hire to perform key transformations. 
So what’s driving this new wave of enthusiasm for data modeling, and why is it such an important part of the modern data stack _URL_",Blog,dataengineering,s9ckr3
448,"Today, Apache DolphinScheduler announced the official release of version _NUMBER_.3. In this version, DingTalk alert plugin adds signature verification and enables data sources to get links from multiple sessions. In addition, _NUMBER_.3 also optimizes cache management, complement time, data source password display in logs, etc., and fixes several key vulnerabilities.
_URL_
For more details, please refer to _URL_ _URL_",Blog,dataengineering,sdtimu
449,"_URL_
>*Since graduating from the Apache Incubator on March _NUMBER_, _NUMBER_, Apache DolphinScheduler has grown with the community for ten months. With the joint participation of the community, Apache DolphinScheduler has grown into a mature scheduling system product that has been tested in the production environment of hundreds of enterprises after several iterations.* 
*What progress has Apache DolphinScheduler made in nearly a year? Today we’re going to review the changes that have taken place in the Apache DolphinScheduler and its community with this Apache report.*
# Base Data:
**Founded:** _NUMBER_–_NUMBER_–_NUMBER_  
**Chair:** Lidong Dai 
**Reporting schedule:** January, April, July, October 
**Next report date: Wed Jan _NUMBER_ _NUMBER_** 
**Community Health Score :** **_NUMBER_ ** _URL_
# Project Composition:
* There are currently _NUMBER_ committers and _NUMBER_ PMC members in this project.
* The Committer-to-PMC ratio is roughly _NUMBER_:_NUMBER_.
# Community changes, past quarter:
* No new PMC members. Last addition was Calvin Kirs on _NUMBER_–_NUMBER_–_NUMBER_.
* ShunFeng Cai was added as committer on _NUMBER_–_NUMBER_–_NUMBER_
* Zhenxu Ke was added as committer on _NUMBER_–_NUMBER_–_NUMBER_
* Wang Xingjie was added as committer on _NUMBER_–_NUMBER_–_NUMBER_
* Yizhi Wang was added as committer on _NUMBER_–_NUMBER_–_NUMBER_
* Jiajie Zhong was added as committer on _NUMBER_–_NUMBER_–_NUMBER_
# Community Health Metrics:
* Notable mailing list trends _URL_
* Commit activity _URL_
* GitHub PR activity _URL_
* GitHub issues _URL_
* Busiest GitHub issues/PRs _URL_
# Notable mailing list trends:
dev@dolphinscheduler.apache.org mailto:dev@dolphinscheduler.apache.org had a _NUMBER_% increase in traffic in the past quarter :
_URL_
# Commit activity:
_NUMBER_ commits in the past quarter 
_NUMBER_ code contributors in the past quarter 
_URL_
# GitHub PR activity:
_NUMBER_ PRs opened on GitHub, past quarter 
_NUMBER_ PRs closed on GitHub, past quarter 
_URL_
# GitHub issues:
_NUMBER_ issues opened on GitHub, past quarter 
_NUMBER_ issues closed on GitHub, past quarter 
_URL_
# Busiest GitHub issues/PRs:
* dolphinscheduler/pull/_NUMBER_ _URL_ server integrate into worker server*
* dolphinscheduler/pull/_NUMBER_ _URL_ fix snowFlake bug*
* dolphinscheduler/pull/_NUMBER_ _URL_ Recover UT in AlertPluginManagerTest.java \*
* dolphinscheduler/issues/_NUMBER_ _URL_ \ hive sql execute failed*
* dolphinscheduler/pull/_NUMBER_ _URL_ improve install.sh if then statement*
* dolphinscheduler/issues/_NUMBER_ _URL_ \ Failed to create hive datasource using ZooKeeper way in _NUMBER_.1*
* dolphinscheduler/pull/_NUMBER_ _URL_ Auto create workflow while import sql script with specific hint*
* dolphinscheduler/pull/_NUMBER_ _URL_ upgrade the MySQL driver package for building MySQL jdbcUrl*
* dolphinscheduler/pull/_NUMBER_ _URL_ replace node-sass with dart-sass*
* dolphinscheduler/pull/_NUMBER_ _URL_ docker.scarf.sh to track docker user info*
# The Way to Join US
There are many ways to participate and contribute to the DolphinScheduler community, including: 
**Documents, translation, Q&A, tests, codes, articles, keynote speeches, etc.**
We assume the first PR  to contribute to be simple and should be used to familiarize yourself with the submission process and community collaboration style.
So the community has compiled the following list of issues suitable for novices: _URL_ _URL_
List of non-newbie issues: _URL_ _URL_
How to participate in the contribution: _URL_ _URL_
Community Official Website 
_URL_ _URL_
GitHub Code repository: _URL_ _URL_
Your Star for the project is important, don’t hesitate to lighten a Star for Apache DolphinScheduler _EMOJI_️",Blog,dataengineering,s3lm3g
450,"### GPU Multiprocessing for Parameter Optimization
>**TL;DR:** The code is here _URL_
Hi Guys, 
I published a notebook using a trick for running multiprocessing on GPU devices. 
Might be interesting for some here, so have fun:
As we know, GPU makes everything faster. Moreover, oftentimes the GPU device we use is so powerful that our code doesn't utilize the GPU code to _NUMBER_% potential. In such cases, running multiple GPU instances in parallel can come in handy and save us some extra time.
I've made a notebook that introduces a method for running parallel jobs on GPU devices. It is a bit tricky since most packages that support GPU usage aren't built from the ground up for such use cases.
**The ""trick"" is simple:** We simply do not call the GPU in the main process, the first time we call for any GPU utilization should be from the child processes. 
! _URL_
To demonstrate the power of this concept, I made a notebook that runs Parallel Hyperparameters Optimization on the GPU for LightGBM. As it is assumed, the GPU enables us to train our models faster  and by 
leveraging that in combination with the parallel execution we get to search a large space of parameter configurations for fully maximizing our model's performance .
The Optimization framework is optuna, the popular framework for optimizing the hyperparameters. Again, this is just an example of the approach. You can use the same approach for many other GPU-enabled use cases. 
Personally, This concept helped me over and over as it is easily transferable to many different GPU-based models including Neural Networks , Other GBM models , Feature Engineering , and more.",Blog,dataengineering,s6ruee
451,Preferably one that sends an email daily or weekly with snippets of info to read on? I’m more inclined to read something send to my email vs visiting something - because I’ll stop or decide I have more important things to do when I’m on the computer,Blog,dataengineering,sdcn59
452,"_URL_
Recently, TWOS officially announced the approval of _NUMBER_ full members and _NUMBER_ candidate members, Apache DolphinScheduler, a cloud-native distributed big data scheduler, was listed by TWOS.
Apache DolphinScheduler is a new-generation workflow scheduling platform that is distributed and easy to expand. It is committed to “solving the intricate dependencies among big data tasks and visualizing the entire data processing”. Its powerful visual DAG interface greatly improves the user experience and can configure workflow without complex code.
Since it was officially open-sourced in April _NUMBER_, Apache DolphinScheduler  has undergone several architectural evolutions. So far, the relevant open-source codes have accumulated _NUMBER_+ Stars, with _NUMBER_+ experienced code contributors, _NUMBER_+ non-code contributors participating in the project, which includes PMCs and Committers of other Apache top-level projects. The Apache DolphinScheduler open source community continues to grow, and the WeChat user group has reached _NUMBER_+ people, and _NUMBER_+ companies and institutions have adopted Apache DolphinScheduler in their production environment.
# TWOS
At the “_NUMBER_ OSCAR Open Source Industry Conference”, China Academy of Telecommunication Research of MIIT  officially established TWOS. TWOS is composed of open-source projects and open-source communities, which aims to guide the establishment of a healthy, credible, sustainable open source community, and build a communication platform providing a complete set of open source risk monitoring and ecological monitoring services.
To help enterprises reduce the risk of using open source software and promote the establishment of a credible open source ecosystem, CAICT has created a credible open-source standard system, which carries authoritative evaluation on enterprise open source governance capabilities, open-source project compliance, open-source community maturity, open-source tool detection capabilities, Opensource risk management capabilities of commercial products.
After being screened by TWOS evaluation criteria, Apache DolphinScheduler was approved to be a candidate member, which shows its recognition of Apache DolphinScheduler’s way of open-source operation, maturity, and contribution, and encourages the community to keep active.
On September _NUMBER_, _NUMBER_, the first batch of members joined TWOS, including _NUMBER_ full members such as openEuler, openGauss, MindSpore, openLookeng, etc., and _NUMBER_ candidate members like Apache RocketMQ, Dcloud, Fluid, FastReID, etc., with a total of _NUMBER_ members:
_URL_
Only two communities were selected for the second batch of candidate members — Apache DolphinScheduler and PolarDB, an open-source cloud-native ecological distributed database contributed by Alibaba Cloud.
The Apache DolphinScheduler community is very honored to be selected as a candidate member of TWOS, which is an affirmation and incentive for the entire industry to build the community a better place. The community will make persistent efforts and strive to become a full member as soon as possible., and provide more value for China’s open-source ecological construction together, with all the TWOS members!",Blog,dataengineering,s0g7qi
453,"We use a variety of ""orchestrators"" for our spark jobs including dagster, cron, and airflow. At this point I don't want to be tied to a particular orchestrator but I would like a central view into all jobs. Is there any product that allows us to push metadata  and then query that metadata? I don't need this registry to actually run or schedule any jobs. The Prefect cloud UI is a good example of what I'm looking for but its tied specifically to Prefect.",Help,dataengineering,sa1ur5
454,"Hello all, 
We are very close to releasing a pipelining tool to open source. Since our team is more oriented towards ml or ds, the tutorial we currently have is ml focused and considered very popular in that space .
Though there are certain data transformations that I can demo using pipeline,I was curious if data engineering community has some hallmark pipeline tutorials or examples that I can implement using our software. 
The ones I found on airflow or prefect  are a bit too simple. 
Thanks for your help. 
Cheers,",Help,dataengineering,s1yj71
455,"I'm officially a data analyst currently, but I have taken on the role and duties of a data engineer at my job. I use SQL basically every day. I use python all the time. I have work related examples of ETL pipelines I have built entirely on my own. I have taught myself to do all this without college or any mentors / training. I know I can do the work AT LEAST at an entry level capacity. However when I got on zoom for my first technical interview I blanked on everything. Forgetting basic terms and definitions. Rambled on about irrelevant junk without mentioning the important things. The white boarding section was a complete disaster because I apparently rely on the IDE to catch _NUMBER_% of my errors. I wouldn't blame them for denying me based off just that one interview. BUT I CAN DO THE WORK! IM DOING IT RIGHT NOW! I just panicked I guess.
Is there anything I can do to fix this? Should I send them samples of code I have written for work and just try to explain myself? Should I just call it first time jitters and hope it doesn't happen again? Is there anything I can do stop that from happening again? Has anyone else experienced this? Has anyone hired someone who bombs the technical interview? Any advice is welcome.",Help,dataengineering,sa9672
456,"Coming from a non CS background I'm trying myself to apply best SWE practices to data and I'm even planning on writing as a pet project a small Python-based ETL program. 
Is there any repository that can be used as a example that applies SOLID principles and uses some design pattern? I'm interested in learning how typical design patterns  are implemented. 
I'll be also grateful if someone provided me with guidelines  to continue my search. This is not a homework assignment nor I'm trying to copy anything, just to learn, but if anyone is uncomfortable with providing me real code I will anyway appreciate whatever help he/she may provide.",Help,dataengineering,sdh9n4
457,"So, I was working on a side project and needed to store nested data structure .
I came across SQL anti-patterns which goes through ways to handle this and came to conclusion that I should be using closure tables.
Are there similar books or resources that go through patterns like this ? 
Basically I am looking for book or resources that goes over complex data modelling solutions like this.",Help,dataengineering,s73s70
458,"We are ingesting a postgres table with a column formatted like this:

I’m trying to convert this into a snowflake array so we can flatten it etc. Is there a function or easy way to do this without using string functions? 
Feel like I’m missing something obvious here.",Help,dataengineering,s9eqiq
459,"Is there a place/resource for data engineering specific design patterns? 
Specific problem here: trying to build a system that can launch different versions  of a pipeline at will — mostly for testing of various commits or for releases. These different pipelines would need to spin up associated services and then spin down after completed or upon request in case of failure — curious if a design pattern for this existed already ? — leads me to the general question of resolved for DE design patterns exist?
Thanks!",Help,dataengineering,s1d1pt
460,Does anyone know what is the common practices to push changes from one environment to the other in Synapse? Specifically SQL script as it's going to get converted into json when it's in github.,Help,dataengineering,s3kerp
461,"Hi guys I would like to migrate an access file that is located in a hard drive into a web database. Ideally the web database would be simple enough to generate reports and add new records. 
Im familiar with back end programming but less familiar with the front end. What would be a starting point to initiate the migration process? What apps, tools and resources would I need to accomplish this task? 
Thank you so much for your answers!",Help,dataengineering,s4dzem
462,"I have a batch data pipeline project w/c requires the data on a near realtime  basis. 
This pipeline will move the data from one system to another using REST APIs, with minimal transformation required. 
The volume of the data I'll be processing was very trivial, at the moment we are receiving max _NUMBER_ rows per day w/c was expected to increase for about _NUMBER_ - _NUMBER_ rows per day the next coming months. 
Lastly, I will be storing my data into a backup storage  after uploading it to the target system. 
I am using Python to develop my pipelines, but this is the first time for me to utilize cloud services.
So with that in mind, there are _NUMBER_ solutions which I was thinking: 
I. Use Cloud Functions, Cloud Storage, & Cloud Scheduler stack 
_NUMBER_ GCF _NUMBER_ will request the data from system _NUMBER_ then dump it into GCS 
_NUMBER_ GCF _NUMBER_ will fetch the data from staging GCS then upload it into both system _NUMBER_ and backup GCS 
II. SAME with _NUMBER_ but I will utilize our existing Big Query for backing up the data. 
_NUMBER_ GCF _NUMBER_ will request the data from system _NUMBER_ then dump it into GCS 
_NUMBER_ GCF _NUMBER_ will fetch the data from staging GCS then upload it into both system _NUMBER_ and big query 
III. SAME as _NUMBER_ or _NUMBER_ but replacing the scheduler with an existing Staled Airflow instance. 
I'm also planning to use dbt in the future, if that would affect the solution. 
What I'm aiming with my infrastructure are low cost, and ease of maintainability & monitoring. 
Feel free to suggest another GCloud Service if that can also solve the problem. 
Any advice is very much appreciated.",Help,dataengineering,s25mrp
463,"Hi, everyone. I've been learning about companies like Confluent, Materialize, Snowflake, Fivetran, Hightouch, et al. I started by learning about data ingestion tools and have been learning more about database software itself now.
I'm non-technical so need help from time to time on topics. One thing I'm wondering about is nodes / clusters. What does this mean? Can someone describe them or link me to a ""picture"" of what a node is?
What does it mean when somebody needs a software to have multi-node availability .
This is a step more technical than I'm used to so this would be super helpful to understand. Thanks so much in advance! I'll upvote every comment as appreciation!
Edit - I should add, I have all day. I will read whatever you suggest, whether it's blog post or textbook.",Help,dataengineering,sgqvm1
464,"So, some context behind the salary increase. I've been working as a data engineer for about _NUMBER_ months now. It's been a bit of an up and down route so far, and the learning curve for the new technology stack has been steep but really fun . It's been challenging in the sense that every member of the data engineering team, bar me, has moved to other companies, with usually higher paying salaries. Then, I've had to take the little experience I have in the sector and maintain the entire stack, while interviewing and onboarding new data engineers.
Things are now looking like they are stabilising a bit with the new starters getting into the flow of things, and I've had a recent conversation with my manager where she praised me on keeping the team and work going. In addition, she mentioned that sometime next week there will be a meeting put in place to go through giving me a salary raise.
I'm currently on £_NUMBER_ per year base pay, and I've just started being paid for being on call on weekends, to the tune of £_NUMBER_ per day. One of the people who left at the beginning of this month, thankfully felt comfortable enough to talk about his salary here and what he's moving to. He was on £55k while here, but has moved to £70k for a remote job in London. In terms of experience, he had a lot more experience in Python, but almost none in data modelling or SQL.
I started applying for other jobs at the start of this month and am currently due to have some final stage interviews this Tuesday, but I have also had some rejections already. The pay for both of these ranges from £_NUMBER_-70k.
I didn't actually want to move as I enjoy the job, but I also want to be paid at a fair market salary. The reality is that, even if I was get some offers through from these interviews it wouldn't be in time for the meeting that's happening next week.
How would you go about getting the best salary? Are there other ways to use leverage within these situations? Any good resources to read through/watch before the meeting? Should I lean on that I've stuck around ? Any other UK based DEs on here think my salary expectations are realistic?",Help,dataengineering,sesp75
465,"I’m in the process of building a data processing pipeline where the arrival of files to an S3 bucket results in the publishing of a message to SQS . My message consumer runs on EC2, and is currently a dockerized Python script that reads the message and triggers a processing pipeline made up of several ETL functions. The specific ETL functions performed can vary from file to file depending on some external requirements managed by dynamoDB and are defined in the SQS message, so basically the message says perform operations _NUMBER_, _NUMBER_ and _NUMBER_ in that order. I’ve been looking for a more robust existing framework to better orchestrate my data pipeline however most seem best suited for batch processing jobs . Are there any recommendations for well supported frameworks that would suite this purpose? Switching to a batch operation isn’t ideal, but any ideas for a better architecture would be appreciated.",Help,dataengineering,sf5f94
466,"Hi, currently I am doing ETL process using synapse Analytics. We are reading delta files from data lake and doing a incremental load on SQL table using spark pool. Jdbc throughout is very low and want to avoid it any suggestions.
Using spark dataframe because transformation are very complex and everything is parameterized.",Help,dataengineering,s2sjhz
467,"If I intend to migrate a Databricks from Azure to GCP, are there migration tools native to Databricks to perform the migration? Or do the GCP offer tools in this case to perform migration? What components do I need to migrate if I run Datalake on Azure?",Help,dataengineering,s9dkiv
468,"I'm trying to create a very basic app that uses streaming data and I'd like to create a distributed data lake the cheapest way possible.
Is there any way to create a Hadoop cluster just with the laptops I have around at home  for free?
I've been searching for guides but I couldn't find a guide that explains how to do what I want to do, any source or tutorial recommendations? Any tips on how to start?",Help,dataengineering,s9q2d2
469,"I’ve recently gotten into playing around with sql after learning some backend. I’m currently loading raw transactions from a blockchain which stores the amount of two assets which will eventually be queried into an ohlc candlestick data set that is to be used on a front end. The timeframe consists of the common ones 5min, 30min, _NUMBER_ hour, and so on. Do i need to specify the chunk intervals explicitly? Looked up on the internet but still don’t understand a thing. I read that by default the chunks are set to _NUMBER_ days, does that mean any transactions from the blockchain that I loaded < _NUMBER_ days will not be shown when queried? Thanks!",Help,dataengineering,sfh5cd
470,"I'm trying to create an archiving pipeline which essentially does the following:
_NUMBER_. Call stored procedure  from a SQL Azure DB that returns a resultset
_NUMBER_. Archive the result from #_NUMBER_ onto storage account 
_NUMBER_. Extract ID column from #_NUMBER_ into an array of int
_NUMBER_. Use result from #_NUMBER_ as a parameter to call a stored procedure  in the same SQL Azure DB
So far, I've tried the Copy Data activity/tool _URL_ which satisfies steps _NUMBER_ & _NUMBER_. However, I'm not sure how to get the outputs from that step and can't find any documentation at Microsoft.
Is it not the correct usage? Do I have to manually do it instead?
Also, I'd like to do some validation in between steps .
I've managed to try the bare/general stored procedure activity but also can't find where to retrieve its output for use in the next step. I'm pretty new to Data Factory and don't really work with data engineering/piplines so please bear with me.",Help,dataengineering,sd4i63
471,Is it safe to use hdp over local machine? My pc goes like i5 7th gen 16gig ram...and everytime i login into the vm my processor creates x3 more the noise than ever. Will the use of hdp over long term destroy the soul out of my hardware?,Help,dataengineering,sferxx
472,"I'm trying to teach myself data engineering, I learn better by doing projects so I thought about doing a rather complex project that involves streaming data and it might be too ambitious for a total noob like myself.
What I'm trying to do is to create an architecture that grabs tweets, news and the prices of several cryptocurrencies and then try to give a forecast of their prices taking the sentiment of the tweets and classified texts as vectors from the news as predicting variables, this forecast gets updated with new predictions in near real time, and I thought something like this, but to be honest I'm not sure whether it makes sense or not.
The architecture I came up with _URL_
I wonder if it would be possible to do forecasting in the same process of doing the sentiment analysis and text classification vectorization with Spark, or if I'd need to store the vectors and the value of the sentiment analysis in a NoSQL database and then doing the forecasting taking the information from this database.
I'd really appreciate any insights or recommendations!",Help,dataengineering,s2kki7
473,"I have my data engineer _NUMBER_ interview coming up next week, I want to know what should I study for the same as I'm not able to find interview experience for freshers.",Help,dataengineering,s4dybk
474,"Looking for some input from the community on how you’d approach this problem. We have a Kafka topic that we’d like to start consuming. Ideally I want to just dump all data off the topic into the raw zone in my data lake partitioned by day and then run a spark job to run transformation on the previous days partition. The issue I am finding is the nature of the data that comes over the Kafka topic. Short and long of it, is the topic actually contains many different message types all with different json array structures  so it’s really like many topics worth of messages all sent over one single topic. For transformation I need to extract each message type and it’s payload and deposit it into its own table in the curated zone within my data lake. I could do the message type extraction before dumping into raw, sure but I’d rather just consume straight raw data and then deal with it. My question is how would you deal with this situation? Can I use spark to read a partition of raw data, extract the individual messages and then write them all to dedicated partitions by message type in the next zone? Few more details, I’m not using spark to read directly from Kafka, I use NiFi to ingest, daily volume is well over 100gb and number of records will be over a billion per day.",Help,dataengineering,s4kh1s
475,"I am unable find answers regarding how does one download data for dashboard building in an automated manner.
Let's say I have want to download data from Twitter everyday at midnight **without having my computer turned on all the time. How would I do it? I want to use a cloud based solution**
I would imagine a hypothetical example as:
A script that downloads Twitter from the API -> Store in S3  -> Stage them into tables -> Load them in a data warehouse -> Output as a dashboard showing some statistics
However, what I do not understand is the step before the script that downloads Twitter data and the hosting of the dashboard. What is exactly making the script download and the dashboard updating?
Sorry for this noob question as I am not a data engineer or CS major by training. I'm still learning this on my own. Look forward for some advice! I hope to build a crypto dashboard",Help,dataengineering,sfk93e
476,"So I'm a bit of a newb to ADF and just building a pipeline which takes a CSV and upserts it to a table in my database. I have the pipeline working and I've set up error reporting which is working nicely. What I'd also like to do is to capture the changes I've made in another table.
So for example if I append two rows with client refs _NUMBER_ and _NUMBER_ to my table, and change the firstname field for client ref number _NUMBER_ in my table called dbo.client then I'd like there to be another table called dbo.client\_changes which looks like this:
|client\_ref|change date|change made|
|:-|:-|:-|
|_NUMBER_|_NUMBER_/_NUMBER_/_NUMBER_|row added|
|_NUMBER_|_NUMBER_/_NUMBER_/_NUMBER_|row added|
|_NUMBER_|_NUMBER_/_NUMBER_/_NUMBER_|firstname: updated|
I imagine this is quite a common use case. Just wondering if there are any tutorials or anything that people have used successfully in the past.
Thanks.",Help,dataengineering,s7o2p4
477,"Hey guys,
I currently work as a Data Analyst with a background in Economics and find the engineering side of things very interesting and want to get into DE. I want to learn and have found good roadmaps/resources such as Seattle Data Guy and AwesomeDataEngineering but I needed some advice.
I understand DE is essentially specialized software engineering and I thought that I should start my learning through TheOdinProject . I'm currently going through the program which teaches HTML/CSS, JS and databases. My question was this: am I learning programming in a very roundabout way? in r/learnprogramming many say to start with html/css  and to learn backend after  to learn efficiently. I'm currently on this path, by learning HTML/CSS, JS, making projects and then to learn more in-depth python, data warehousing, distributed systems and so on.
Would I be better off just going straight into learning Python and data-engineering specific things? if you guys have ideas and examples please let me know!
Also, is DE a far-fetch'd career for me given that I only worked as an analyst and have a non-CS undergrad? 
Thanks for your help!",Help,dataengineering,sbbp7r
478,"here is the rough idea, we have a service that is producing data and want to use a Deep learning-based python script to train on that data and use the trained model to predict some parameters and feed them back to some other service. . We want to create a pipeline to,
_NUMBER_. Run our container-based service on the cloud
_NUMBER_. store all the data centrally
_NUMBER_. train the DL model on the cloud
I would like to know what open source tools would be best suited for such task, any help would be greatly appreciated! Thanks!",Help,dataengineering,s9kwls
479,"I am looking for training course on Palantir platform
Thanks",Help,dataengineering,s7cz86
480,"I was wondering if there is a way to feed streaming data from Spark Streaming to TensorFlow, I've read it's possible using Kafka but I don't know if what I thought here, using Kafka for the Spark output, would make much sense:
_URL_
What I want to do is forecasting the cryptocurrencies values for the next _NUMBER_ minutes taking their previous values, sentiment analysis floats  and classified text vectors  as predicting variables and use a recurrent neural network in TensorFlow for forecasting.
What I also thought is sending this data to a Cassandra database or similar but I don't know if I can pull data in streaming from the database using TensorFlow, would it be possible? I'm very new to this so I'm trying to learn as much as possible with this project of mine, thank you!",Help,dataengineering,s4rej7
481,"I get the general idea of Idempotent dags  but wanted to know if anyone had any tips on some of the extra details below? In this example we are reading data from a table in big query, calculating some additional columns and inserting into a destination table on a daily basis.
**Handling the initial run?**
At the moment, we have specific tasks to constuct the initial table if it doesn't exist. Is there a better way of doing this
**How to handle new calculated columns?**
If we add new calculated columns to our insertion SQL, what's the easiest way to backill en masses? Rewrite the table from scratch or is there a way to schedule previous runs.
**Handling inserts that may change previous data**
Say that an insert calculates values over a partition for a customer dating back, possibly, to the start of the available data. 
The nature of this means that a value calculated for a row maybe be correct at the time of insert, but may be overwritten a month later by another customer interaction. 
What would be the best way of writing an Idempotent dag that could backfill older inserts without overwritting newer ones",Help,dataengineering,savf55
482,"Hello, 
My company is replacing box with sharepoint and onedrive. Some dashboards are fed with files from Box since box connector on Tableau allows scheduled refresh thanks to Tableau server. 
OneDrive connector only works for private OneDrive, not shared ones. SharePoint connector only supports SharePoint lists, no stored files. 
 
I tried to think out of the box, so these are the solutions that came up to my mind. 
Could you please let me know if there are any limitations that I haven't foreseen? Thanks a lot. 
 
_NUMBER_)Keep the same logic of benefiting of Tableau server by hosting extracts and scheduling their refresh.
What are other common accessible location to access file from Tableau servers? GoogleDrive or Dropbox. 
=>Replace Box by GoogleDrive or Dropbox. 
Cons: none of those cloud solutions are authorized within my company.
Pros: as easy as it used to be with Box. 
_NUMBER_)Create an RPA to migrate files from shared to private OneDrive. 
=>Create PowerAutomate flow to replicate shared OneDrive/SharePoint files into his private OneDrive
use OneDrive Tableau connector to this private OneDrive. 
Cons : only the owner of the private OneDrive can take actions. 
Pros : OneDrive connector supports scheduled refresh. 
_NUMBER_)New logic: hosting extracts on an ETL solution server
=> Use Alteryx + Alteryx server. 
connect Alteryx to SharePoint using the standard input tool if configure permissions correctly on SharePoint and get the UNC path to SharePoint. 
Then in Alteryx generate a .hyper or .tde output. 
Alteryx server allows to schedule the load of data to Tableau. Upload the workflow to the Server gallery, then schedule it accordingly to requirements. 
Pros: Alteryx & Alteryx server are authorized within my company. 
Cons: requires more technical skills + license cost . 
_NUMBER_)New logic: use data virtualization thanks to Denodo -> no need of file storage. 
=> There is a Tableau connector dedicated to Denodo. 
But as with Alteryx there is a need to first register Denodo in Sharepoint. 
Pros: saves the issue of data storage. 
Cons: requires more technical skills + license cost . 
 
_NUMBER_)Other options:
=>Using Tableau Prep and create an RP. 
Same connector issues as Tableau . 
Automating this would require Tableau Conductor which is not authorized within my company.
Could automate it with an RPA , but that would still mean the data storage would be supported by a collection of .hyper or .csv files in a folder. This is not compliant with data strategy within my company. 
Many thanks in advance",Help,dataengineering,s978wv
483,I have use case for bigdata processing problem. I have an ETL that runs data deliveries. There are some tasks which generate derived tables over tables included in the delivery. Currently the script for the tasks is designed such that it requires loading all csv files in memory for processing. That often causes laptop to run out of memory. This Airflow DAG is currently hosted on local machine. Prime reason being frequent troubleshooting required during every DAG run. Would it be feasible deploying the dag on GCP cloud composer? because I need to troubleshoot it often. Make code changes to accommodate data delivery logic. How can I maximize processing and minimize time?,Help,dataengineering,shkth6
484,"I'm currently in healthcare and want to switch to DE . I have been conflicted in the best course of action because I do not know if I actually need a new _NUMBER_ year bachelor degree, masters or if people actually find profitable work with only a bootcamp under their belt. 
For transparency, I have two _NUMBER_ year diplomas in Research and Medical Laboratory Sciences so I am adept at the statistical side and fully ignorant of the computer side. I make $_NUMBER_ an hour in my current role. I am taking a quick little python course atm and absolutely loving it.",Help,dataengineering,sfbtdn
485,"In the past, I had just rolled my own, but curious what tools people use today to gather data from APIs  on an ongoing basis? I'm thinking: 
Scheduling - Prefect 
Scraping - Custom python script 
Persistence - PostgreSQL
For people that have dealt with API scraping on an ongoing basis, what are some challenges you faced & what other considerations are worth keeping in mind? 
One issue that comes to mind, sometimes with the pagination there are duplicate records. Is it worth just getting everything into some table incrementally with bulk uploads, and then having another process that would clean and deduplicate?  Or try to deduplicate in the scraping script before saving to the DB?",Help,dataengineering,s3h9yk
486,"I have a new Windows server, clean slate. Got a boot drive and a partition for installs. 
I have no working examples around me to refer to and I've never personally seen this done or worked on a team. That's the very brief context. 
___
My intention is to install python and use Git for version control. For now, I just want to write some scripts to pull data from a vendor's database and store it. Plain and simple. 
Do I need to install Git on the server along with Python and the repo, sync to a private Github and do all my work from VS Code on my local desktop? How far off am I?
I'll make this more sophisticated & work in validation / pipeline management tools as I learn, in case anyone's wondering. Trying to keep this bite size as I learn the ropes.",Help,dataengineering,s370dl
487,I've written a new project recently which focuses around these _NUMBER_ main things. I want to compare my project to that of one which utilises these tools. I'm particularly interested in the project structure so a github repo would be perfect. Does anyone have any great examples?,Help,dataengineering,s21wyj
488,"I am working in an indian IT company  and I have been assigned an Informatica Developer role. I wanted to know if there is a future for this technology, I dont see any coding here, only a bit of SQL for validating the data. 
I have seen some data engineering topics related to informatica on the internet. I want to know what other things i can do to make the most of this opportunity. 
I also love coding and pretty much like to switch jobs if this technology is not the future.
One more thing I am worried about if I want to switch to a coding job, whether they will consider my profile seeing my informatica experience since it didn't involve any coding. Idk I am just confused and worried. Any help appreciated. 
Thanks.",Help,dataengineering,s5yzn9
489,"Is it possible to store variables in a separate YML file in one of your models? For example, if I have a model calls ""test\_model\_1"", can I create a YML file called ""test\_yml\_1.yml"" with variables to be used in any sql file in that model? IF so, how do I call those variables from the sql file?
**\** I should have specified... I meant *separate from* the project.yml file. I don't want my project file being littered up with a bunch of variables that will only be utilized by one model.",Help,dataengineering,s3u58s
490,"**\*\*EDIT --> Size of data is _NUMBER_ TB\*\***
**I am writing a job In AWS Glue that should :**
* reads **_NUMBER_ TB across** **_NUMBER_ Million XML** files, each about _NUMBER_ KB. Historical data, won’t change or be added to)
* currently using Dynamic Frames, a call to resolve choice to deal with uncertain level of schema inconsistencies  among these files
* finally writes the data as parquet to an output bucket using some logical partitions based on fields in the data.
* The primary goal of this pipeline is to compact the data and provide useful partitions so that downstream research questions and ETL jobs will be significantly less costly to run.
* Further, we'd like to make as little assumptions as possible about the contents of the data at this stage and minimize data loss by, for example, imposing a particular schema on the data upon reading
* Additional context, I wrote this with PySpark, but am comfortable with Scala, so would be fine implementing in Scala if people think this is worth it.
The job runs fine on a sample of the data  to a test bucket, but the bottleneck running on the full raw data  is, unsurprisingly, in the initial steps of the job where the driver is forced to list all the files from the input source.
I’ve implemented some features that are supposed to be designed for this issue, like
* Job bookmark (_URL_ _URL_
* Bounded Execution (_URL_ _URL_
* Input File Grouping (_URL_ _URL_
* others considerations for memory management (_URL_ _URL_
But it seems there is no way to get around listing these files, and the associated pressure placed on the driver. Also complicating this is that the data has unhelpful partitioning. Basically, the data is severely over-partitioned in a pattern that follows:
* s3://bucket/**ENTITY-NUMBER**/
 * s3://bucket/ABCNEWS\*\*_NUMBER_\*\*/
 * each sub directory will contain about _NUMBER_-_NUMBER_ small XML files 
* There are likely hundreds of thousands if not one’s of millions of these subdirectories.
I’m sure others have faced this same issue, so here’s what I’ve gathered as potential solutions:
* Given a list of the distinct ENTITY values, selectively copy files from the original raw bucket into a staging bucket using a more helpful s3://bucket/ENTITY/ pattern partitioning, and then run the Glue job in batches, pointing at one ENTITY partition at a time
 * not sure if this will help as the performance deterioration on files seems to occur even at the _NUMBER_'s or _NUMBER_'s of thousands of 40KB files
 * I’ve tried using the AWS CLI sync command using AWS Cloud Shell but found that, as many have cited, there are issues here when moving tens of thousands of files
* Use AWS S3 Inventory to create a manifest of all the files, and then batch these into discrete Glue Jobs
 * Not sure how I would point Glue to a manifest of file names rather than an S3 bucket
I appreciate any feedback on these or any potential solutions here!
Thanks",Help,dataengineering,s9shwo
491,Has anybody implement data lineage project using Apache atlas ? Any link of git repo would be helpful?,Help,dataengineering,sdsccz
492,"Hi all,
I am working on Dataflow jobs developed in Apache Beam , but I think the same question applies to all MPP frameworks/engines.
From my understanding, when a step/task fails with an unhandled error, the input element is processed again and again until there is no error, which happens either when the error is temporary  or when the pipeline is updated to fix the error.
The pattern I am seeing is that when none of the two above cases happen, the input elements causing the errors are retried forever and overcharge the pipeline, causing autoscaling and a lot of wasted resources.
What is the way you handle such situations? Of course known errors are handled, but if a specific unhandled error happens, we have a problem. I also assume a big try/except is not the solution, since this would not raise temporary errors ",Help,dataengineering,s9cohw
493,"Hello,
I'm a BI analyst currently that recently accepted a DE position to work with the DS and ML people in our org, thanks to this sub and the Seattle guy on youtube. I started preparing around Spring last year. I officially start next month after my vacation.
The thing is, there's no other DE in this org. However, there are DS/ML people that did work on some DAGs but it's not their full-time job so they don't have a lot of time to debug and maintain their DAGs. We also have a full DE team on a different org but they're a little bit short on resources at the moment and too busy working on other things. They're all mostly pretty experienced and working on bigger things. Since I'm already in the same org with the DS/ML people, it was more manageable to get a position here as a DE since I've been here awhile. But I don't know if I am in over my head. I've basically been tasked with fixing two of the current DAGs with no timeline since they know I'm new and still need time to learn the current DAGs. But as a junior DE, I'm not sure what's the appropriate timeline to be able to finish it. I've also been tasked with building _NUMBER_ DAG next quarter and it seems like I have the whole quarter to do it. Does this seem reasonable?
I'm planning on spending the weekend to learn our libraries, connections and whatnot. I'm excited with the opportunity but I don't know if I'm in over my head since there is no senior DE to mentor me. The senior DS/ML people that worked on DAGs are nice enough to offer help but they are not DE. Alternatively, there are also tons of existing DAGs that the DE team has built and I'm able to take a look at, review and possibly learn but it will be all mostly on my own. Will I be ok? Are DAGs simple enough for a jr. DE to build/learn with the above resources and no senior DE?",Help,dataengineering,s47dtz
494,"So I am actually in finance but would like some insight on a report I built based on OLAP pivot. It was working great the first few runs then I started getting login credential authentication prompts when attempt to refresh the report. 
When I enter the password , it won’t take the password and eventually it will log me out of analytical services if I try to enter the credentials too many times. Initially I was thinking this was a problem of not having permission accessing the cube but when I tried to reconnect and set up the data table in a new file, I had no problem with logins.. could this due to how the permissions was set up? Only allow access to certain dimension?
And when I save the file and reopen again, it will keep asking me to put in the password again ughhh.
Any suggestions of what to do? I am trying to help the admin of the database to understand the problem but he seem to think this was a issue of entering the wrong passwordwhich is not the case.
I also share this user login with one of my other colleaguesnot sure if multiple people using the same login account will create a problem as well.
Thanks so much in advance!",Help,dataengineering,shjvxa
495,"I created a small project _URL_ on my local machine where I query an API, transform the data, and load it into different tables on an RDS Postgres database. Currently, this process uses a bash script to run activate the virtual environment and run each Python script in sequence, while writing errors to a log file. Which cloud product is most appropriate to host/schedule this process to be run daily? Ideally, I'd use AWS since I'm already hosting my database there, but happy to hear other options too. Thanks for any advice you can offer!",Help,dataengineering,se2tef
496,"We've trialled schemachange](_URL_ and [liquibase _URL_ which are change script based tools. We've ruled out a whole load of other tools that are either change script based tools or don't support Snowflake, including the following:
* Flyway
* SchemaChange
* Atlas
* Devart dbForge
* ApexSQL
* Datanamic
* DBArtisan
* DBComparer
* OpenDBDiff
* SQLDelta
* DB Deploy
Are there other state based change management tools we haven't considered for Snowflake? We started off with schemachange, but this is proving tricky to use as our customers keeps changing the priority on what we need to work on. 
By state based, I mean the tool can work out the delta of what objects are on an environment to work out what changes need to be applied.",Help,dataengineering,sfimjd
497,"Hi everyone,
I've recently been promoted to a Data Engineering position at work. That being said, my first project is helping migrate data from SAP ECC to SQL Server and solidify our data pipeline so my Analytics team can extract data in a more streamlined way for our dashboards and modeling. 
 
I don't have much guidance from technical leadership or access to technical expertise in this undertaking, and I wanted to see if there were any Sr. DE's that had common ""rookie"" mistakes they've seen in similar initiatives that I should look out for. 
Any insights are appreciated.",Help,dataengineering,schltg
498,"I'm working on a pet project and I've settled on using Cloud Composer to manage my ETL tasks. A main part of this is data scraping. I'm conflicted between two approaches:
_NUMBER_. Have the web scraping calls come from the pipelines themselves, to take advantage of the error handling and logging that comes with Cloud Composer/Apache Airflow.
_NUMBER_. Have standalone web scraping instances which listen to a message queue and scrape based on incoming messages. This way the system becomes more distributed and there's less risk for getting clogged by a surge in requests.
Have any of you set anything up like this? Any advice for which way I should go?",Help,dataengineering,s8oxj3
499,"Hi guys the data that I am working on is in a shitty state, and i have to transform it more consumable form. Is there a way to convert lat and long to city directly on BQ or I should write cron job in python to transform the data?",Help,dataengineering,s0eoiu
500,"Total noob question, hope this is an ok place for it.
Say I have multiple vendors. Some use Oracle, some use MS SQL, some use postgresql etc.
Can I stick to one flavor of SQL for my organization, or do they all need to be written depending on which DB I'm connecting to?
T-SQL for this one, PL/SQL for that one etc. Or can I write all the SQL for everything in T-SQL, for example?
The basis of my question is keeping things as simple as possible. Couldn't find my answer on Google, probably not asking it very well.",Help,dataengineering,sbyua0
501,"Hi folks. I’m very new to dbt and Fivetran. I have been doing data engineering for past _NUMBER_ years but most of my experience is in building data pipelines using legacy etl tools like Informatica and TSQL. 
Recently I completed basic tutorials on dbt. I was wondering if there are any public project that I can look into to get more hands on experience in dbt. Also, where and how do I start with Fivetran?
Appreciate if anyone can guide me in the right direction.
Happy Monday!",Help,dataengineering,sbpb8d
502,"Hi,
How do people get around the fact that fivetran cannot read data from BigQuery or other bigdata sources? Am I missing something?",Help,dataengineering,s1r3xs
503,"Hi! I am a Data Engineer & Analyst at a non profit organization. I'm pretty much the only person with an engineering/coding background and since I don't really have any guidance in my organization I was wondering what I could improve in the implementation I've built with our data warehouse and pipelines.
The setup is a BigQuery DW with some Cloud Functions running Python pipelines to feed data into it. We currently only have a few pipelines setup pulling data from a Postgresql DB, where we just pull the all the data we want from the Postgres DB table, truncate the previous version and insert all the data again
This was done because the Postgres tables had no reliable timestamps to handle only inserting deltas. Please advise if there is any other cleaner way to do this! 
We also have some more tables in there where data is inserted manually every quarter since we get a big file in a very dirty format, so I came up with a script to clean it but still manually upload it.
 We are also considering building another one to get more data from a partner's Redshift instance. 
PLEASE NOTE that no modeling was performed at all for this data warehouse, I just figured which tables and columns would be useful for our analytics and pull all these data into BigQuery to feed our Data Studio reports. 
There are also no tests being performed in the pipelines and even though I have a git repository for the pipelines, if I want to change the code for the Cloud Function I have to go in there and manually change it for each.
I am just looking for some general guidance on ways to improve this implementation since I feel a bit lost on what the best next steps would be. 
Thank you!",Help,dataengineering,sacaqc
504,"I need to extract data from Amazon Ads and DSP campaigns. The business has the reports in Amazon platform, but they would like to create more in detail reports and cross compare, so I need to take the underlying data of said reports or, at least, download and extract the data from the reports.
We are trying to load this into an Azure Data Lake, so the tech stack is Azure based .
Does anyone have any experience with this / these API?
Link to API doc below, I am reading it, but it's slow . Also, Google did not help - there are articles advertising 3rd party solutions of extracting data, so I know it is doable, at least.
APIs Documentation _URL_
le: added tech stack info",Help,dataengineering,s2tim7
505,"Hey guys,
I am trying to ingest data from eloqua uing rest api. Problem is rest api has a limit of _NUMBER_ million records per response and my pipeline is failing due to this.
Anyone has any suggestions or ideas to handle this scenario in adf?",Help,dataengineering,s8gg4f
506,We are migrating away from the old way we did data engineering to Databricks on Azure. We have various Databricks instances in our company that we need to get specific data from . What's the best way  to do this? Using ADF makes us groan because our ETL process is dynamic and we need to write code to handle these dynamic requests. Many thanks for any input provided.,Help,dataengineering,s3d7kp
507,"Background: So our data pipelines load data, update history, enrich it and then we come to transformations before actually generating the reporting table.
The data we have is revenue data coming from multiple sources mostly apis. In transformations, one transformation is to identify the type of revenue which requires manual input from another team.
So the manual process goes like this:
- takes a set of unidentified transaction rows .
- puts them in a separate table.
- this table is shared with other team who manually set the type column  of each unidentified row.
- then this table along with original table are joined together to generate reports making sure there are no duplicates and type of unidentified rows are now defined.
Ideally I know identification should be automated, which is already under development but not fully in place. So the manual process of identifying transaction cannot be replaced at the moment.
Question:
- Is it the most optimal way to do things?
- If you do something like this, do u have better suggestions?
- In business analytics, what search term should I use for this reading about this kind of business requirement.
Thanks for all the help.",Help,dataengineering,sc9iic
508,"I'm analyzing data extracted from an API in the form of JSON files in Jupyter Notebook.
What I'd like to do is dump all the JSON files the API can access into a data lake or warehouse and then write simpler SQL to access what I need when I need it.
Any recommendations on free/cheap data warehousing / data lake solutions that could do the job?
All I can think of is BigQuery, Databricks and maybe AWS?
Thnks and apologies for the newbie question.",Help,dataengineering,se180s
509,"I want to get familiar with Delta Lakes but everything I've seen on YouTube are high level overviews. There are no courses on Udemy or anything either.
I also have the book Learning Spark Lightning-Fast Data Analytics, but there isn't much on Delta Lakes in it.
I'm a person that enjoys learning with tutorials and books and I wonder if you have any recommendations or places to look for more learning resources.",Help,dataengineering,s734oe
510,"Hey I am wondering if anyone has done this before. 
At work we have to integrate data in azure blob storage with a 3rd party application that consultants are using. 
Originally I had thought of giving them specific access to the resources and operations  with a SAS . 
Since those can expire what is the best way to get them that code regularly in a secure manner? 
Or should I rethink the way to do this. 
I have thought about having them generate the SAS token on their end but you need the Account Key to do that and I know know if I want to do that. 
Any thoughts are good thanks!",Help,dataengineering,s1ib2j
511,"Hello!
I am a finance student in their last year of uni who has recently fell in love with the idea of data engineering.
In the last _NUMBER_-_NUMBER_ months I’ve started working on SQL , I’ve done some data visualizations with Tableau , I’ve done most of the easy SQL stuff on HackerRank and I just started with leetcode.
I’m at the point where I’d really like to start learning Python, but I keep feeling that my SQL skills just aren’t good enough to move on? Like I cannot justify moving forward because I feel like I haven’t gotten to a point where it’s justifiable for me to say “okay, next step”. Has anyone been in this position? 
Should I just move forward or are there any standalone SQL projects related to data engineering that when I would have completed would really help me solidify in my mind that I am ready to move forward and give me that confidence?
Any help is appreciated :)",Help,dataengineering,sf9yz2
512,"I have a few sources of patent data  that I would like to extract and process with sql and python. Is my best option to just use a notebook to do all of this? I would prefer to use GCP,  for the processing part, but I'm not really sure how to start. If anyone has any advice it would be greatly appreciated!",Help,dataengineering,s9pd25
513,"Wondering if those with more experience with Spark can help. Can pyspark parse a file that has multiple json objects each with its own schema? The JSON schemas are the same at the highest level  but then each individual object has nested arrays that differ in schema contained with in fields and tags. Is this structure something that can be parsed and flattened with pyspark? I having trouble getting anything to work due to the presence of differing schemas within a single file that is read into spark. Example structure below:
[ {
 ""fields"" : ,
 ""name"" : ""mem"",
 ""tags"" : ,
 ""timestamp"" : _NUMBER_
},
{
 ""fields"" : ,
 ""name"" : ""net"",
 ""tags"" : ,
 ""timestamp"" : _NUMBER_
}}",Help,dataengineering,s6bqyh
514,"Hello folks, looking to the seniors of this industry or anyone from a similar international background for some advice on my path.
From a SEA country, got my bachelor's in actuarial/Stats in the US, lost OPT job when COVID struck. Have since been working in my home country. I've held _NUMBER_ relevant roles: data analyst/consultant  and data scientist .
The data field as a whole is really new where I'm at, and both the positions have been at startups so I've been the only person on the team, so I've had the chance to wear the ""full stack"" data guy hat, so to speak. Engineering wise, I've helped build out some clients pipelines using GCP and services like Stitch, Fivetran. At my current role I built the full analytics pipeline  off their production NoSQL database.
Most of the work I've done is in python scripts and sql, with the orchestration being achieved via airflow/Cron jobs and the like.
I've really fallen in love with the engineering side of data over the heavy math and stats side of things, so right now I'd like to move overseas since the field isn't established here . I loved my time in the US and really felt at home there, so I would like to return there.
I know it's going to be tough, and I'm somewhat familiar with the difficulties of getting a H1B.
I guess my question is, what's the outlook in this particular field for people like me? Are companies considering international applicants? Non CS background seems to be a big hurdle in the US. What would be my best option to break into the field? Do I really need to invest in a Masters and hope to land something off of OPT?
Other international applicants/seniors familiar with overseas visa situations: how'd you guys do it? What can I do with my experience? What skills do companies look for as a bare minimum to consider foreign applicants?
TLDR: how can I break into the US/EU market as an applicant from a SEA country?",Help,dataengineering,s105u5
515,"Hey guys,
I'm a non tech guy preparing myself to switch into data engineering. I have gone through a few courses to educate myself and shortlisted one which I feel is good. I just wanted you guy's opinion who are already in the industry to advice whether the topics covered in the course are good in terms of current market demands? Would it be sufficient to grow in this field? If not any other course you would recommend ?
Would really appreciate any help here. I have provided the link below.
_URL_ _URL_",Help,dataengineering,sd3626
516,"After ""learning"" SQL and Python, what comes next in data engineering for someone who is completely new to the domain?
If say I want to build some sort of ETL pipeline to put on my portfolio, what are the tools that I would need and in what order is it best to learn these?
I keep hearing about Apache, redshift, docker, and so many more and I understand that it would be good to have all of these tools but where do I even start? Is there a certain order/workflow that these tools come into play? 
Ideally I would like to know what I would need as the fundamentals of an ETL pipeline and go from there.
Thanks so much",Help,dataengineering,sfzs2e
517,"I'm in the process of building a project to improve my DE skills, wanting to do something more complex/real-life scenario than I usually do.
Long story short, I'll be working with chess data. I'd like to ETL GM games details and build an analytics dashboard plus, if I've got time, a predictive model 
I'm fetching all data from a REST API serving JSON. Here is what I came up with. I'm working on-premises to limit costs.
_NUMBER_. Fetch some existing data from players' monthly archives. Parse them with Spark , store them into a Cassandra ""staging"" cluster
_NUMBER_. When this works, automate the retrieval of players' daily archive and add it to Cassandra's staging 
_NUMBER_. Schedule a spark job to daily treat new data and update my ""BI"" data
_NUMBER_. Build a ""BI"" dashboard with Tableau or Looker
_NUMBER_. Dockerise stuff
For step _NUMBER_, a lot of data will be fetched at the beginning . Would you store all data in an RDD  then push it to Cassandra? Or would it be more efficient to treat JSON lines one by one, parsing and storing them to DB?
I chose Cassandra because my queries would be pretty much the same every time. Hadoop feels overcomplicated here.
For steps _NUMBER_ & _NUMBER_, should I use Airflow to schedule a spark submit ? or Kafka ? or something else?
Step _NUMBER_, these _NUMBER_ tools seem to be the most used according to job offers. Good choice? Any other suggestions?
Something along these lines:
_URL_
 I'm pretty new and self-learning DE field. Is it somewhat correct? Or completely wrong? Any advice, optimization, DB archi must-do, tool recommendation, caveats ? Any constructive critiques are welcome, thanks for helping out :)",Help,dataengineering,sg5ttq
518,"I need to ingest highly sensitive data from file shares and a database using Azure Synapse or ADF and land it into Snowflake. I am not familiar with masking in Snowflake but believe that the masking/encryption needs to happen during the ingest process. That is, already masked data lands in Snowflake.
What is the best method to accomplish this? Is this the recommended method? Are there any tools/common design patterns? 
The data will need to be consumed from Snowflake by a team of Analysts/Data Scientists that have clearance to work with the data.",Help,dataengineering,sdpa0c
519,"I'm struggling a little with building a dimensional model around an accumulating snapshot fact table. I'm modeling a process for project timelines so the fact table would have start\_date, funding\_date, end\_date, etc. and the main dimension table would be project details, which would include information like title. The problem is a project only goes through this process one time so the fact and dimension table would always have a _NUMBER_:_NUMBER_ relationship which feels wrong. 
What would be best practice?",Help,dataengineering,s28kon
520,"Hi, beginner in data engineering here.
We're actually using a slow  and not very reliable  coded in Golang. Since a few weeks, we've been trying Apache Spark. It does work but this fixes only the ""transform"" part of our pipeline.
Here is an image of the total pipeline: _URL_ _URL_
As I said, this whole pipeline is in Go. We would like to use real data engineering/big data tools for this.
So, to recap, we need:
_NUMBER_. A tool to monitor multiple buckets and filter files on them 
_NUMBER_. Transform our files 
_NUMBER_. Run multiple aggregations pipeline thought MongoDB.
For example, if we decide to use only Apache Spark, then our pipeline will start from Go, then call Spark, then wait  for Spark jobs to finish, then call MongoDB pipeline from Go. This is not very practical and that does not follow any ""good practice"" in data engineering and management.
Thank you!
TL;DR: We are searching for the right tools for our pipeline , transform columns and rows, then insert all in MongoDB to run aggregations. Also, the most important, is how should we connect each part of our workflow?",Help,dataengineering,s9e3hg
521,"Hi all,
I work on a small startup that is based in Europe. Our only data infrastructure so far is our transactional database, but we already have big enough volumes of data to analyze and we need a proper data warehouse. I am more of a data analyst, but I need to set this one up myself; please consider this as a disclaimer, I'm not at all experienced in the field.
Our situation is as follows:
We have an S3 bucket where, for every transaction, a json file is stored with all the relevant information, which includes sensitive data such as the customer's name, phone number, email, nationality etc. For analytic purposes, some of the sensitive fields are not useful and can be removed, while others are useful but can be anonymized.
What is the best practice to do that?
I was thinking of an additional step in the process whereby a lambda function would be triggered everytime a json file is created in our bucket. The lambda would create a new json file with unnecessary fields removed and sensitive fields hashed, then it would store it in a new bucket. So all the processed data would live in a new location permanently while the original raw data would be erased periodically.
An alternative would be to perform the anonymization right on bigquery. But then, I would still need to remove the raw files from S3 , so I'd lose all this data.
What do you think is the best approach, and what kind of legal considerations should I have in mind?
Please help an overwhelmed non-engineer. Many thanks in advance.",Help,dataengineering,s8f21z
522,"I need to relate tables in my Postgres data warehouse for effective querying, but foreign key constraints inevitably make ETL a nightmare. What is your solution to this seemingly common problem?",Help,dataengineering,sao8io
523,"Is Airflow a good option for monitoring jobs  and tracking dependencies?
We have a bunch of different tools running ETL jobs and and no  way to track dependencies. Looking at the Airflow docs _URL_ it supports all  of our tools and it includes a SAMBA provider so I would guess you can just point to a file on a file share.",Help,dataengineering,sd1yli
524,"Hi Team,
I would like to run ML jobs in the following situation. In my home I have a powerful PC that has a GPU that satisfy my needs but sometimes I need to scale for training multiple models. I would like to setup an orchestration configuration that uses my machine but when needed ,on demand, it requested an ECS AWS machine , process my ML model and then brings the machine offline. I was hoping that something like Airflow could do it, but I can't find any resources how this would work. 
Does anyone have any tips on how to do this? Or I should use other tools? 
Thanks!",Help,dataengineering,sd61vz
525,"Hi,I am currently working in a WITCH company in DataStage Development support role. Although i am in the development team, Error fixing, incident handling is most of the Job, little to no chance of any development.
I have an offer from a mid level startup for Data Analyst role  with more than _NUMBER_% hike.
My aim is to be in a data engineer role and i am trying to learn and create projects on Spark, Kafka etc tools for building my skills.
Questions i require guidance on:
_NUMBER_. Current company has bond  and _NUMBER_ months notice period, with both of these, new company has no problems and will even look to buyout.Considering i have been rejected by multiple companies on _NUMBER_ months notice period constraint, should i switch and keep preparing my skills for internal switch to a data engineer position or to move out after a year to more relevant position.
_NUMBER_. Is my current role more relevant for a data engineer position or both the jobs will have no relevance for a data engineer position.
This is my first full time job after internship and i would not like to waste any company's resources and time.Any guidance will be highly helpful to me!!
Thanks",Help,dataengineering,s7pwum
526,"I have worked with BI in the past, but mostly in the analytics part: querying stuff from a data warehouse, generating analysis and such. So I know the very basics of databases and I know basically how PostgreSQL works.
Now I'm in a company that has zero infrastructure regarding a data warehouse. People run stuff on their own computer and store it in Excel and CSV on cloud.
I want to build a data warehouse from scratch, beginning with getting a VM to run procedures and run the database there, but **I know nothing about this part. In fact I know so little regarding the technical part here that I don't even know what to search for and can't even elaborate further on what resources I need you to point me to. Someone asked me if I wanted to run a cycle server or not and I don't even know if it's yes or no.**
**Can anyone point me to good resources to learn the technical part that goes around building a DW?**",Help,dataengineering,s85ix8
527,"I'd like to scrape a lot of data on comments from an API. The way the API works is a little funky and sometimes when paginating may encounter duplicates. What would be a good what to structure this kind of data pipeline? Each day there are anywhere from _NUMBER_-20k new comments. Here is my idea so far:
Scheduling: Prefect
Data API crawling: Custom python script to extract comments and topics from the API
Data storage: PostgeSQL
Each API call can only return a few hundred results at a time, so was planning to just do batched inserts from the python scripts directly to the PostgreSQL DB. I want to parallelize the scraping, so I guess I might just allow duplicates in an initial, raw table then worry about clean up in a secondary table. 
Questions:
* Have you done something similar? How did you approach it?
* Does going straight from API -> Postgres make sense? Should I store the raw results to S3 first? 
* Would dbt help? 
* Later on I would like to be able to do some NLP  and I think I'll need to use scikit learn / Spark. Is there another format of storage that would be better to support that? For me the NLP is an add-on that doesn't need to be ""live"". Otherwise, I am thinking I would simply schedule some kind of dump from Postgres -> S3 for Spark .
Thanks for your help!",Help,dataengineering,s3668b
528,"Hi,
I have a bunch of scripts that run benchmarks and pipe it into local text files. I would like to publish these text files with ngnix to prometheus . I installed nginx _NUMBER_ but I cant seem to be able to 
_NUMBER_. modify the port
_NUMBER_. Set the folder to the logs folder and make it scrape any logs.
The logs themselves will be changed every _NUMBER_ mins or so.
What do you think? Is there a project/guide I can use?",Help,dataengineering,s8p2o2
529,"Hello, it's my first day of internship and I was tasked to make a message bus infrastructure that can be used in connecting AWS applications. The problem is I'm new to all of this including AWS. I do not have any background at all. My managers didn't explain anything, they just told me to construct it.
I have researched the basic concepts of a message bus and what it does and read that it forms a service-oriented architecture. Could you give me some resources that can help me make the infrastructure? I checked Enterprise Integration Patterns but did not find what I was looking for.",Help,dataengineering,s65yea
530,"background: company moving from Kubeflow on kubernetes cluster on AWS to Azure and Synapse . 
We have data landing on S3/ADLS in parquet, it needs to be checked, processed, turned in useful data, and made accessible to users via python and SQL.
We work with the bronze - silver - gold principle. Raw data lands on the datalake, gets checked, put into silver, flat tables are made so they can be used for dashboarding and modeling in gold.
Data lands as parquet, silver and gold will be delta lake  to allow for schema changes over time.
Data will be landing in batch, daily, usually in the early morning. there's about _NUMBER_ distinct tables/files coming into RAW . 
These need to be checked one by one, and copied to silver  in delta lake format. The gold tables generally take multiple inputs before they can be made. Usually the order and timing of input tables is quite stable, but there have been many instances where it wasn't.
set-up:
we want to use Delta lake as storage, spark clusters as compute, synapse notebooks for code, and pyspark and sparkSQL as languages. 
We currently have much of our logic in .py files with functions, and .sql files that take arguments . 
we want to register tables in a sparkDB in synapse, but will perhaps use serverless sql pool to have data accessible via ODBC .
We'll develop code and pipelines in a LAB Synapse instance, and push to a Factory synapse instance via Azure DevOps.
the question:
What is a best practice to orchestrate all these pipelines, so they can run when their data is available? The first step should be easy, just create a file trigger to launch the pipeline  when a data file lands. 
Subsequent pipelines that take only one input can be triggered by the finishing of the previous pipeline, or we could do this as a second step in the same pipeline. I prefer to separate these pipelines to make it easier to correct things and rerun pipelines. 
This will probably mean that for _NUMBER_ tables and a load more finished tables in gold, there will be anywhere from _NUMBER_ to _NUMBER_ distinct pipelines. There will only be a few categories/kinds of pipelines , but each with distinct logic tailored to the data in it. 
Currently, we have general kubeflow pipelines that run distinct code based on arguments. Eg the compute-data pipeline is generic, and reads all new files in Bronze. it finds foo.parquet, so runs with the foo argument, reads the foo.py _URL_ logic file, and executes the foo.sql file. If it then finds the bar.parquet file, same thing but for bar. Rinse and repeat.
Is there a way to way to properly do this in synapse? 
Also, how can states be passed through pipelines? Say the table garply needs both foo and bar, but these run at different times. how does the compute-data pipeline know when both foo and bar have been updated to the latest snapshot? 
I'm currently thinking of writing logs to a DB, which some kind of meta-pipe can then read to check on states. Something like Neo4j which would also allow us to visualize the state of the DB throughout the day, and what pipelines are running and failing. 
But perhaps there's a better  way.
 
All input from people with experience on Synapse with lots of orchestration is welcome!",Help,dataengineering,s8gz41
531,"Dear all,
I would greatly appreciate any feedback on this, if you have the patience to read through the lenghtly post.
Context: I'm a DA with occasional DE duties, working for a startup. I have little DE knowledge/experience . In particular I have no experience with BQ or any other DW. Right now I want us to move away from doing BI on our transactional database  and instead copy our transactional data into BQ.
As I understand it , the most straightforward solution is to use tools like airflow or managed services like fivetran, in order to mirror  our transactional database into BQ.
But I have a dilema, as there may be an alternative solution in our case:
Our main product is, let's say, a ""reservation"". Much of our backend infrastructure is based on a queuing system . When a reservation occurs, a json file is produced in the queue that various of our microservices are using to do their job. This json document is very large and deep , includes arrays, and also contains every possible datum we would need for BI. We could say that this json file denormalizes every JOIN we would like to perform on our transactional db for that particular reservation. So, I thought that an alternative solution to mirroring our MySQL db could be to store all these json files in a datalake and batch-ingest them into BQ periodically.
I played around with ingesting a couple of thousand json files into BQ and although I find it a bit awkard as I'm not used to work with nested data, I can see that it could work. But I'm not sure how it would work at scale.
**So my main dilema is:** mirror our MySQL or batch-ingest denormalized json files? Can you detect any obvious problems with the json scenario? Do you see any benefits?
Thank you in advance.
\---
Additional information:
* On average _NUMBER_ json files produced daily. This will probably double next year.
* A %  of the json files will be updated at some point so they may need to be re-ingested in BQ. An educated guess would be at least _NUMBER_% of them. So the BQ dataset would need to be updated, possibly daily.",Help,dataengineering,sdy7g7
532,"As the title says, is there a library for Dask with similar features to Spark's GraphX for graph analytics?",Help,dataengineering,sftmkg
533,"I am trying to test the server status of my db with PyMongo and am getting an SSL error. 
I installed the root certificate to comply with MongoDB but it seems Python / OpenSSL can't access it.
Is there a turnaround to allow Python to access my installed root certificate so I can check the server status of my database?
Apologies if I've posted this question in the wrong place",Help,dataengineering,sf4fmh
534,"Hi, I've noticed that in Snowflake you can have unlimited number of Databases.
Let's say I would like to have different DB for each ""type"" of data .
Each Database would have almost identical schemas .
So, it would look like this:
PRODUCT\_DB: RAW, CLEAN, PUBLIC
CUSTOMER\_DB: RAW, CLEAN, PUBLIC
and so on... there would be a lot of Databases in the end.
What are some disadvantages of this approach? Is there something fundamentally wrong with this approach, or will it be fine even with a lot of Databases?
On the other hand, what are advantages of having RAW, CLEAN and PUBLIC DB, where we would have PRODUCT\_SCHEMA, and CUSTOMER\_SCHEMA?
Does it really matter at the end of the day? Thanks!",Help,dataengineering,s7vzl1
535,"I have worked with ADF with synapse, but my current project is ADF with snowflake.
I have done some meta data driven with synapse and adf. How can I do the same with snowflake?
Anyone tried the adf metadata driven data ingest with snowflake?",Help,dataengineering,s4x8yu
536,"Hi Gurus,
I hope you can help me. I have  is a simple conversion of a timestamp from UTC to AEST. However, when I view the timestamp in the source table , then I view the timestamp in the target table , the dates are exactly the same? Below is a screenshot of the code I've written to derive the column in Visual expression builder. I have the same issue with WhenModified.
Am I missing something?
screenshot from derived column - visual expression builder _URL_
Thank you in advance for your help, I really appreciate it.",Help,dataengineering,sfrva7
537,"I have a data pipeline where the following steps occur:
_NUMBER_. Call an index API endpoint, which provides a list of IDs in which to make other calls 
_NUMBER_. For each ID in the previous call, request another endpoint with that ID 
_NUMBER_. Extract attributes from ID call and insert them into SQL tables
When developing the pipeline, I would save the JSON responses to disk so I wouldn't eat up the API quota, but I'm wondering if it's normal to always save the raw response even in production.",Help,dataengineering,sets3p
538,"Hello, I was wondering about the best practice to notify users  about a successful/unsuccessful notebook run.
I read about job alerts, but is there a way to send notifications to slack?
Thanks in advance",Help,dataengineering,s1ce8y
539,"Hey folks!
Has anyone of you ever worked with Manta (getmanta.com]) for data lineage management? Or maybe Collibra ?
Asking for a mid-large size company that needs to get clarity on its lineage and report on it . The company is not using a unifying layer like Snowflake. Just heterogenous systems. 
Keen to hear back!
ContinentalCake",Help,dataengineering,scbp3x
540,"Hi All, Recently I have changed my company and they offered me ""senior Data Engineer"" role. I was okay since my expertise is in Data Engineer. But after joining the new organisation, they have added me to a project where the scope is for ""data analyst"". I know data Engineer should have data analytics skill to implement data engineering solutions. But, assigning data engineer to a project where the tasks are only related to data analyst??! Is this the same case in all organisation? I am confused whether to continue the job or change the organisation.Please share your thoughts.",Help,dataengineering,sc6wi0
541,"My company have a running system with RabbitMQ to assign tasks for workers. In my understanding, RabbitMQ help me schedule the order and assign which task should be run at the moment, and monitor the worker's status.
Then there is Airflow, which seems similar to what RabbitMQ does, the introduction I found says it ""schedule, monitor and assign tasks for execution"", and it also needs RabbitMQ queue as messages. What's the meaning of using Airflow if I have RabbitMQ already? 
More specific, what's the difference of function for Airflow and RabbitMQ?
Thank you",Help,dataengineering,s7ebsb
542,"All of my questions are in the title - just trying to get clarity around what tools / contexts I might hear the words ""in memory"", what it means, what alternatives are, why it is done one way vs. another, etc. . Thanks so much in advance.",Help,dataengineering,sbsb0e
543,"Hi,
I currently want to export data from two ERP systems  to Snowflake, but the number could increase to _NUMBER_.
Unfortunately the databases do not have database logs enabled and CDC/log based replication is not possible, but the tables have timestamp/rowversion columns.
I tried Airbyte and I like it because it handles schema changes well, but to do a full load of a database after Snowflake it is rather suboptimal. It is slow and if you select another table, all tables have to be synchronised again, even if nothing has changed. With many tables, this is simply unnecessary  data transfer.
Do you have a cost-effective alternative that performs well with full loads?",Help,dataengineering,scn3gu
544,"Hello all,
Hope you are well. 
Thanks to AstraZeneca, I was able to open source some of my work for the past _NUMBER_ year. 
The project is named, magnus and relates to data science/engineering pipelines.
The project is available here: _URL_
And the docs at: _URL_
Could you please share some feedback?
I am open to contributions and can use your help in making it better/useful.
We are releasing magnus-extensions, an extensions package which makes it cloud ready, soon.
Cheers,
Team magnus",Help,dataengineering,scbkbb
545,"Hi folks, I have created one ETL pipeline. I want to learn more about
ETL development. Can you suggest me any good books or some website where I can get more knowledge about ETL?
I have used airflow, gcp, python, terraform, SQL, some of AWS services in my first ETL.",Help,dataengineering,semga6
546,"I am just starting to use the delta format and I am finding it hard to find info on a topic. 
When appending to a new deltatable, does it automatically remove duplicated rows  or is this something that I will have to do in code. Here I mean rows that do not already exist in the deltatable before the append.
What is the ""expert"" way of handling this since delta tables can grow to huge tables.",Help,dataengineering,s7ltj3
547,"**I am writing a job In AWS Glue that should :**
* reads _NUMBER_ GB across _NUMBER_ Million XML files, each about _NUMBER_ KB. Historical data, won’t change or be added to)
* currently using Dynamic Frames, a call to resolve choice to deal with uncertain level of schema inconsistencies  among these files
* finally writes the data as parquet to an output bucket using some logical partitions based on fields in the data.
* The primary goal of this pipeline is to compact the data and provide useful partitions so that downstream research questions and ETL jobs will be significantly less costly to run. 
* Further, we'd like to make as little assumptions as possible about the contents of the data at this stage and minimize data loss by, for example, imposing a particular schema on the data upon reading
* Additional context, I wrote this with PySpark, but am comfortable with Scala, so would be fine implementing in Scala if people think this is worth it.
The job runs fine on a sample of the data  to a test bucket, but the bottleneck running on the full raw data  is, unsurprisingly, in the initial steps of the job where the driver is forced to list all the files from the input source. 
I’ve implemented some features that are supposed to be designed for this issue, like
* Job bookmark (_URL_ _URL_
* Bounded Execution (_URL_ _URL_
* Input File Grouping (_URL_ _URL_
* others considerations for memory management (_URL_ _URL_
But it seems there is no way to get around listing these files, and the associated pressure placed on the driver. Also complicating this is that the data has unhelpful partitioning. Basically, the data is severely over-partitioned in a pattern that follows:
* s3://bucket/**ENTITY-NUMBER**/
 * s3://bucket/ABCNEWS\*\*_NUMBER_\*\*/
* There are likely hundreds of thousands if not one’s of millions of these subdirectories.
I’m sure others have faced this same issue, so here’s what I’ve gathered as potential solutions:
* Given a list of the distinct ENTITY values, selectively copy files from the original raw bucket into a staging bucket using a more helpful s3://bucket/ENTITY/ pattern partitioning, and then run the Glue job in batches, pointing at one ENTITY partition at a time
 * not sure if this will help as the performance deterioration on files seems to occur even at the _NUMBER_'s or _NUMBER_'s of thousands of 40KB files
 * I’ve tried using the AWS CLI sync command using AWS Cloud Shell but found that, as many have cited, there are issues here when moving tens of thousands of files
* Use AWS S3 Inventory to create a manifest of all the files, and then batch these into discrete Glue Jobs
 * Not sure how I would point Glue to a manifest of file names rather than an S3 bucket
I appreciate any feedback on these or any potential solutions here!
Thanks",Help,dataengineering,s9shwo
548,"My team has a pretty large pipeline setup. Running the entire pipeline with production data  takes like _NUMBER_ hours. We have some lower-tier environments with smaller datasets where things take like _NUMBER_ minutes, but at the end of the day, the issue with these datasets is that they might be ok for building the happy path but bugs are not always apparent until we hip preprod and run an _NUMBER_ hour execution.
Having _NUMBER_ hours of downtime minimum per user story is forcing my team to take on larger end-to-end stories. The reason for this is that breaking a story into _NUMBER_ or _NUMBER_ pieces would mean running the whole pipeline a lot more. On the other hand, user stories feel too large at the moment so I am thinking of ways of mitigating the issue.
Right now one option I see is to reengineer the pipeline so that it runs in stages and each stage has its storage. so if we run for _NUMBER_ hours and then find a bug in stage _NUMBER_ of _NUMBER_ we would only have to fix the bug and rerun the last _NUMBER_ stages.
Another option could be making better datasets for the lower tier environments but I am not sure how to do that.
The goal is for the team to be able to decide on how to tackle user stories optimally, Ideally tackling issues in multiple, smaller, safer steps. Not having our pipeline forcing us to clump everything together.
Edit:
More details. 
We have unit tests for everything, not as many as we would want but we are working on it.
Regarding CI/CD: 
Right now these big user stories require us to use feature branches that have lifetimes of _NUMBER_ day to _NUMBER_ weeks. This brings me to another question. In software engineering, I see that the tendency is to go for Trunk based development + feature toggles. I could envision that working really well with something like an API, but I struggle to imagine that working out well in a large big data pipeline.
How are other data engineers with large pipelines doing CI/CD where everything lives in one branch?
Any Ideas? If anyone can point me to any books or talks that might point me in the right direction that would be great.",Help,dataengineering,se0to8
549,"Tools like DBT have made it easy to version/source control data in databases, but my team is struggling to do the same with managing security  Has anyone else run into a similar issue? What solutions did you find?
EDIT: TIL that terraform is open source and not an AWS-specirfic tool. looking into it now.",Help,dataengineering,s2aisk
550,"Hello, 
I am new here and I am in the process of setting up my LinkedIn profile. I need your help in finding an appropriate banner that communicates my specialty as a Data Engineer.",Help,dataengineering,s5znhw
551,"I'll start by stating this is really an Analytics Engineering question, but hope someone here can help me out a little or at least point me in the right direction.
The EL part is done. Unfortunately, it's not currently done incrementally but is instead executing a raw ingestion of all data from the source system. Regardless, I have access to all my source tables. Next comes the T, which right now is a series of sequential SQL files that execute in a specified order. Not using dbt or Airflow, this is actually external client data flowing into our SaaS product, so execution of the files are controlled via the backend. 
We are currently truncating or outright dropping and recreating the data for all time even though only a small fraction of source data changes each time we run the EL process. Right now, we allow the transformation process to write data again for all time. Highly inefficient process, but it got the job done fast and cheap. The primary key where all of those files write to is a UUID. 
Hopefully that paints the picture well. The issue is that we're finally at the point that truncating the data in the final output table no longer works for us from a scalability perspective . I know in Postgres INSERT...ON CONFLICT UPDATE is how to do an UPSERT, but that's not really my issue. My problem is that the PK of this table is the UUID, which each time we execute the process, it creates new UUIDs. 
So if I don't truncate the output table at the start of the process and simply change my INSERT INTO to INSERT...ON CONFLICT UPDATE, how do I define the conflict when the output of the transformation process will have a distinct UUID from the last time the process ran.
Am I overthinking this and the reality is I need to segment my process so that it only executes for the new subset of data ?
I'm new to the role and am trying to understand how to approach the problem correctly. All ideas welcome. Thanks",Help,dataengineering,secen2
552,"I know Spark is supposed to be fault tolerant etc. etc. 
My question is -- for those of you who run mission-critical data pipelines with Spark, do you guys actually rely on this fault tolerance? i.e. take out spot instances and pray that everything goes fine if one of them gets pre-empted? Or do you guys still use on-demand instances and raise errors when an instance goes down?",Help,dataengineering,s7dull
553,"Folks, I have attached a reference architecture diagram here that depicts a typical old-school architecture using a NoSql database. Can someone help me understand the arrow pointing from the *SQL database* to the *API/Web Tier* titled ""analytical queries""?
What sort of analytical queries are we talking about here?
Is it referring to a Reverse ETL Process?
_URL_",Help,dataengineering,s6cce5
554,"Good afternoon, I have a problem regarding accessing data from within a pipeline.
I need to access some data from within a pipeline, but I DO NOT want to pass that data as a variable to my PTransforms. . I also don’t want to hard code this data into the script that will be ran in the pipeline, because that’s sensitive information. I have tried two things that didn’t work:
- I have tried getting this data from the OS environment and dynamically changing the variables that belong to another python script before the code goes into the pipeline itself. The plan was to have my other script which is the one that runs in the pipeline to import that first script and use its variables. But when I tried running it, all the variables were still None.
- I have also tried creating an object before going into the pipeline, with the credentials, pickling it and saving it to a temporary file. Then, in my script in the pipeline, I would open that file, and get the credentials. However, when I tried doing that, I got an error log on GCP saying that the file didn’t exist, even though it did exist on my machine. 
Can anyone give me any other suggestion? Thank you.",Help,dataengineering,s75tbw
555,"I'm planning to play with aws a little meaning there's alot of uncertainties and a lot of unexpected charges that might occur, as far as my research goes aws budget could only alert you when the threshold for a specific service has been breached but it's not really going to do much more. 
I'm planning to use a debit card with a very low amount of balance so when things go wrong they couldn't charge me anymore as it doesn't contain enough money, i've tried searching but i haven't seen anyone who's done the same - now i'm getting paranoid whether what's wrong with my idea, can anyone point it out?",Help,dataengineering,s19rv3
556,"Hi there,
I'm facing this problem at my org. I'm working as a data eng in a team of a dozen analysts.
Analysts were tasked with creating tables in BQ to support their dashboard . They would also schedule their queries in Airflow. The reason they were doing this is because of scarcity of data engineering 
We ended up with many datasets and tables parts of different data pipelines. A lot of the SQL queries in these pipelines were building the same transformations and sometimes same aggregations .
In some cases it led to having different ways in SQL to calculate the same metric. This is obviously a problem because an end user consuming the same metric from different data products could spot the difference and be confused
So my question is:
How do you go about building a process which makes it easy for analysts to find what has been built already and that they can reuse without spending too much time searching/browsing through lots of sql files in gitlab for example?
Thanks!",Help,dataengineering,s8ypeu
557,"I have a question -
In current project ,we want a way for users to upload files.
Once files are uploaded to S3 folder,lambda takes care of rest processing.
In which way users can quickly upload files to S3 without having any access to aws. Users are not at all tech savvy.
Creating a flask website is long term solution ,but I just want quick and easy way as interim solution.",Help,dataengineering,s35m72
558,"Hi all,
apologies for what may seem an uninformed question but I've been trying to think of an architecture to build off of an API. We use a SaaS service to collect machinery time series data . The data is fed into the proprietary SaaS through a ""historian"". The SaaS then offers a public facing API.
However when we tried to fetch a large amount of data the API just can't handle it . I would like to build an ETL and/or pipeline to get all this data on our own cloud  and be able to run machine learning tests etc. on the data. 
Am I thinking in the right direction? Do you have any technical stack to recommend?
Thanks.",Help,dataengineering,shd811
559,"We are standing up a Snowflake DWH newly. Now the question is should I opt to go with snowflake datalake for file storage or Azure datalake storage. Considering the fact all my current files are already stored in Azure datalake.
_NUMBER_)Need help on understanding pros and cons of using snowflake datalake vs Azure datalake.
_NUMBER_)Does snowflake datalake has GUI kind of interface to archive the processed files for future usage, regulatory usages?",Help,dataengineering,sdajbp
560,"Title says it all ..... Searched the internet alot, couldn't find anything other than few half baked stuffs.... Any leads would be extremely helpful!",Help,dataengineering,sdyzwb
561,"Has anyone here appeared for the technical round at 5x for the data engineer position? They've told me on call that it would be a _NUMBER_ hr technical round, which would involve taking a test simulating day to day work of a 5x DE.
Has anyone appeared for it, or has any tips for it ?
Thanks!",Help,dataengineering,s1evh0
562,Took a new job and I've been asked to choose a new machine. I was disappointed to see that Apple doesn't sell Intel chips at all anymore because I would've clone my _NUMBER_ MBP. I know when M1 came out there were a number of compatibility issues with data science related packages. Have those been resolved? Do they not really impact your day to day?,Help,dataengineering,s077i4
563,"I'm  a DS where I work, wearing different hats as necessary, but I'm having to become less ignorant about DE because we don't have one and it's clear my company is not handling data well.
Right now I'm working with longitudinal medical assessments of our users: questionnaires assessing their health before and after using our product. Some of the assessments are in an internal MongoDB database , others are in an external forms website . The ones in our internal database have a user ID, but there are several versions of our questionnaires on the forms website without user ID, so I have to match names by Levenshtein algorithm to an enormous list of users. I have Python scripts collecting questionnaires from all these sources , matching them to users, interpreting what they are and ordering them by datetime, deciding if the timeline is acceptable for analysis, etc. so the data can be assembled and explored for a causal effect. Each time I want to tell my boss who needs an assessment , I have to run a script that collects data from all of these resources, matches names again, interprets the metadata, etc. in a very inefficient way.
Clearly, it'd be better if all of these assessments were put into a central repository I could query from for faster insights and even for dashboards monitoring what % of each user base has assessments completed, who is due for them, etc. It seems to me a data lake would be most efficient, because there are different variations of forms with slightly differing questions, with pipelines scripts running on a schedule that extract from each resource and match names when needed . Am I correct in thinking a data lake, hosted somewhere like AWS, and a few pipeline scripts, could solve this problem? Thanks for your help.",Help,dataengineering,savhoq
564,"Hello, you mystical beasts.
I am currently employed and hold a masters degree in an unrelated field. I want to earn a bachelors and/or masters in the field of data engineering. My hope is to find scholarships and grants that can help make my dream a reality. 
Sorting through the internet for scholarships has been a daunting task. I want to find real opportunities to apply for grants or scholarships specific to the field for women.
Please help with any information you may have. If you were awarded any scholarships, I'd love to know which! Also, any websites or collective of where to best find and apply would be greatly appreciated.
Thank you!",Help,dataengineering,shgvr7
565,"The main problem is, I have some table's from Postgres and We build a batch pipeline to load these data into BigQuery, there are some columns from these tables that are updated each hour, day, or week.
I'll give an example, customer status, this status could change from ""active"" to ""inactive"", ""blocked"", ""pending"" somethings like that. 
 On Postgres, it works fine because it handles well with transactional data operations, but BigQuery doesn't handle well with that transactional data operation, So our solution was to create a view with all deduplicated data for each table.
The thing is, it won't scale, because now we have large tables and our BigQuery's bill is too expensive.
There is some move this data from Postgres to BigQuery without creating duplicated rows when some data is updated?",Help,dataengineering,seybpa
566,"Recently I have completed _NUMBER_ courses on Udemy. 
i.e. Azure Databricks & Spark Core For Data Engineers]",Help,dataengineering,sdyec5
567,"Hey all, 
We’ve recently had a databricks demo session and now are scoping out the cost of using it in our organisation. 
We are currently building a data warehouse and etl pipelines in Azure, however we are interested in using databricks for our advanced analytics and machine learning in the future. 
Cost is a significant factor, and we want to compare the cost of databricks to the cost of other azure tools in this space. 
Can any of you give me some direction on how to begin getting $ figures for thIs comparison? All help is appreciated as I am just a junior who is trying to show some initiative. 
Thanks!",Help,dataengineering,s2v7qi
568,"Hello everyone,
I've written a script in Scala that reads data from MongoDB collections and store their data as parquet files on HDFS using Spark.
The problem is, in one of these collections, when I read their data, there is a column that sometimes its data type is NullType and sometimes is DoubleType!
I used to convert all NullTypes to StringType, but in this specific situation I can not do such a thing.
I really stuck on this problem and I don't know how to handle it. And there are some nested columns that makes the situation even more complex.
I really appreciate your help on this if could give me a hint how to solve it.
Thank you in advance.",Help,dataengineering,s2zn63
569,"Hi everyone, first of all thanks everyone for building this great community! You have helped me tons so far.
I am a jr data architect and trying to get up to speed as quickly as possible with data management in general. After reading DAMADBOK I want to get some more experience with data warehousing and hands-on modeling. Following your advice I have started The Data Warehouse Toolkit by Kimball. 
Do you have any tips on a real-life test project I could work on while reading the book? I have access to an AWS Sandbox account, so something on there  would be great.",Help,dataengineering,sftpkt
570,"Hi All,
I am new to Dagster and wanted to see if there are any online resources/videos/courses available for training?
I couldn’t find much online. Any help would be greatly appreciated.
Thanks!",Help,dataengineering,scg8bn
571,"I'm starting to get into ""big data"" and the lakehouse architecture . 
Do you know of any resources that document/ explain the different patterns how data can be aggregated into the gold layer tables ? 
Example, assume I have an immutable transaction log, are there patterns that I can adopt to summarize or aggregate these transaction log entries into a single entry ? OR any patterns that describe how such a table can be joined with other tables to create a de-normalized view?",Help,dataengineering,saop3p
572,"I'm a newbie to Redshift, have worked with Oracle, MySQL before.
There you can do a desc.
What is an equivalent command in redshift?
Also the way I access table is like abc.xyz .
I did some googling and found pg_table_def, svv_columns but they don't seem to work in my case.
I believe I'm missing an escape character or something like that.",Help,dataengineering,sblxng
573,"Hi all,
I work on a startup as their only data analyst and, by necessity, I deal with data engineering tasks on occasion, at least until we have the luxury of hiring a real data engineer.
Our main data are our transactions  and google analytics. We also have data from various services we use , that stakeholders analyze via the tools offered from these services, such as online dashboards. I want to improve the situation by setting up a basic data infrastructure with bigquery as our DW where, as a first step, I will be storing and analyzing our transactional data.
Now, while I'm in the process of setting up our DW, as a company we have almost decided buying the services of a team of BI consultants that have built their own BI platform. Their backend is based on bigquery and the frontend on power BI. They will be ingesting our data  into their bigquery, then they will be merging them with various domain data they generate from independent sources, and then they will be making customized dashboards for some of our teams and provide insights.
My main concern is that they insisted that they will not be using our own bigquery for their platform and thus we will start building separate data pipelines for the same data.. Both google analytics and our transactions will be ingested in both DWs. I am afraid this will introduce double the complexity, maintaining two pipelines instead of one.
Am I right to be concerned about that? Is it common practice that external data consultants would maintain their own independent data infrastructure? I have little knowledge when it comes to Data Engineering nor I have been exposed to this kind of situation before, so any advice would be very appreciated.",Help,dataengineering,sbk5o6
574,"I'm somewhat new in the DE world , and I really want to increase our certainty that our ETLs are doing what they're supposed to do.
We have both our lake and warehouse in different Redshift clusters  and we have some very standard spark jobs for those ETLs. What I want to have is some ""validation"" step that the result is what we expect:
- Number of rows in source and destination is the same 
- Test columns against source 
- Foreign keys point to the right entities 
- Some more, maybe 
We started looking at Great Expectations and invested some time on it but it doesn't seem to support checks upstream vs downstream, at least not easily or that I can see.
How are people doing this?",Help,dataengineering,saqti9
575,"Hello guys,
I'm building a database on Bigquery for a project written in Python using the client libraries which is basically a recommendation system but rather than for ads, it's for better-tailored search results. 
Should I make it so every new user automatically creates a new dataset? or just a new table for that specific user .
I also believe I'll need a table of users and their information. Any recommendations? 
Thanks in advance!",Help,dataengineering,s5nuxw
576,"I frequently use ast_node_interactivity _URL_ setting in Jupyter to get multiple outputs from a single cell. It does not seem to work in Databricks and I cannot find any mention of it online. 
Any way to make it work in Databricks?",Help,dataengineering,s8hv69
577,"Hi folks, 
I am a master's student looking for an MLOps internship. I would like you to give me your valuable feedback/suggestions so that I can improve my resume. Thanks!
_URL_",Help,dataengineering,sclz30
578,"Hi! I am trying to build a project for my uni's FYP and decided that I will go all-in so that it can double up as my personal portfolio as well. My idea is to build a modern batch end-to-end data pipeline with containers orchestrated with Airflow, from ingestion to visualization.
Originally I was hoping to collaborate with a company for the data source , but as time goes on I began to feel that it is incredibly difficult for companies to share their data to a student. Therefore, I began to look into the possibility of using open source data or Live API.
The problem is, I am facing difficulties of choosing the most appropriate data sources. I hoped my project could accomplish and demonstrate these characteristics below:
\- Integration from multiple diverse data sources
\- Batch import every X time 
\- Data sources rich enough for a few visualizations 
Can anybody advise me what data source that I can potentially use to accomplish this? Thankss!",Help,dataengineering,s2sdrb
579,"TL:DR: On read, a column in spark is an Intiger, during convertion to Pandas its a Long, write fails because in the silver table its a Double  - Have any of you encountered this before?
I have a process where I use spark to read in data from a delta lake using a sql query. I am finding the closes Lat Long pairs between a every day updated file with a static file. I am doing this Raw-to-bronze, bronze-to-silver, silver-to-gold. In the bronze-to-silver I read in the data using spark, I convert that to a pandas dataframe so that I can finish adding the columns with the closest lat lon pairs and convert back to spark to write to the delta lake. A few columns, that during read are Int are converted to Long in pandas, then I create a spark to write to delta lake but then I cant because in the silver table that column is Double. I have tried many times and overwritten the schema, but always the same error comes up. Has anyone ever fixed something like this before?",Help,dataengineering,sgv5fo
580,"Hey everyone,
Trying to get a grasp over our event traits being sent to Segment and soon RudderStack. I've been auditing our event data and it seems like our tracking plan basically mashes as much data into traits that don't really have anything to do with the event. Here's an example:
 track;
Our product person claims they need the data but we already have video author information in MixPanel. It seems to me like MixPanel should be able to derive the videoAuthorUserType no? Is the above bad practice? Any tips on best practices for shaping event data?
It seems to be some of this data is being flattened together to compensate for a lack of a feature on certain destinations but that should probably be fixed with a transformation to the destination no?
Appreciate any advice or tips.",Help,dataengineering,s2ocuj
581,"My thinking is that: you could get ksqlDB or Materialize or another streaming analytics offering to perform analytics in  real-time after joining multiple tables of streaming data from both  your OLTP database and  streaming event data from IoT sensors that are written to Kafka topics.
Since this is the case, why would you ever use something like HVR for CDC to have  real time into your OLAP CDWH ? 
* Is there any need to actually have that information move into your CDWH real-time? For me, I see two use cases of that data once it's in Snowflake
 * _NUMBER_) Analytics: when you are doing analytics on data in Snowflake, are you able to have those updated in real-time if you are querying Snowflake?
 * _NUMBER_) Reverse ETL / Operational Analytics: when you are syncing modeled data from Snowflake into your SaaS apps like Intercom or Salesforce...
 * Do the underlying data models in Snowflake get updated as soon as new data comes in ?
 * Regardless of the answer to the above, is data synced to your SaaS apps  updated *in real time / as soon as the modeled data changes*, or is it updated in batch?
So, in a nutshell, I'm wondering why there is even a use case for a company like HVR when you could just analyze that streaming data with a different streaming data analytics tool and no need to put it in your CDWH. 
Thanks in advance!",Help,dataengineering,sbuge9
582,"I find it a PIA to use the wiki, to put it frankly. For example, if you go to the FAQs and take a look at the ressources you have to juggle _NUMBER_ scroll bars at once to search a page that is only viewed on not even half of the screen. The interactive graph on the right should not be permanently visible but only optional at max. Furthermore, I find the whole concept with showing multiple pages over each other rather disturbing and quite the opposite of a clean and clear design of markdown/HTML files.",Help,dataengineering,s4lljg
583,"Does anyone have a good ""how to get started-"" guide on building infrastructure for a DWH in the cloud for a private account? 
I'm interviewing for jobs and have heard from colleagues that some companies have task' included in the hiring process containing the above request. So was wondering if anyone have a guide on how to get started, similar to all the available guides on how to get started with respective programming languages.",Help,dataengineering,s785uk
584,"Mods - I'm sorry. I know this is a stupid post. I recently posted a thread about where Kafka fits into the modern data stack and someone offered to speak with me. I accidentally ignored the chat and I am upset because I was excited and this is a rare opportunity for me.
I will take down this garbage post as soon as they see this and message me. I know this is silly but please let me keep this up. Thanks.",Help,dataengineering,rzuz4i
585,"I feel like when someone is explaining their data stack, it is a word salad of brand names. I have a hard time lining up a particular technology  with its function within the enterprise.
As a bonus, I would love some primer that explains what all the functional areas are and how they work together. Not sure that exists though!
Much appreciated.",Help,dataengineering,s2cyhg
586,"In order to work with Spark, I would like to learn Scala. Until now I only have Python knowledge. I have browsed through Udemy and Coursera but there doesn't seem to be plethora of Courses like for Python. Can anyone recommend good learning sources like courses or books based on their experience? Thanks a lot!",Help,dataengineering,s7nexo
587,"I noticed that Data Scientists and Analysts at my organization prefer to use Jupyter Notebooks to create data processing code. 
It is often the case that they create complex pipelines consisted of multiple interconnected Jupyter Notebooks where one Jupyter Notebook is consuming the output of another Jupyter Notebook. 
At the moment they don't have a way to schedule this ""notebook pipeline"" for regular execution. 
I wonder if there a tool that they could use to define a dependency graph between notebooks and schedule that graph for daily execution. 
Personally I use Apache Airflow for workflow orchestration, but Airflow is too complex for Data Scientists to use. 
Is there a tool that people without deep SWE knowledge could use to achieve a similar result?",Help,dataengineering,s3qb5g
588,"Hey guys!
I'm currently in the process of studying for DP-_NUMBER_, the Azure Data Engineer Certification. However, the MS resources  are too detailed and overwhelming since I have no prior technical experience at all.. 
Can anyone recommend sources to study from? Any courses out there for beginners that provide a simple explanation to things? 
Any resources would be much appreciated _EMOJI_️",Help,dataengineering,s1i5no
589,"Setting up config and infrastructure is a nightmare in Windows. Even with terraform and docker and what not.
Windows WSL is a bandaid to most stuff. I find myself constantly jumping between powershell and WSL even though I have just only started learning DE. There is a unique problem in every step of the way of when setting up infrastructure in a Windows machine and eventually when you do find yourself ultimately landing in the WSL github issues page you know there is no fix for that particular issue.
Now I thought about using linux exclusively but I really need to use Excel from time to time. 
So, I wonder who even uses Windows as their main machine? Why and how?",Help,dataengineering,sco2zg
590,"I have a table in snowflake which contains around 15M rows. Now for few columns I need to convert it's type from varchar to numeric and datetime. Currently snowflake doesn't support this. So, I have created a new table and moving data from old table to the new one using python.
However, after _NUMBER_ hours of running the code stopped due to session error and only half of the data loaded in new table.
Now it's hard to see which rows are missing because there is no any primary key or unique column.
So, Is there any other efficient way I can use to accomplish the task?
#snowflake #dataloading",Help,dataengineering,s4cuwk
591,"The thing is, I am not a good writer and I do not know how to sell myself, so I really would appreciate some help :) 
WORK EXPERIENCE 
xxxxxx , Remote 
Data Engineer, _NUMBER_/_NUMBER_ – Present 
- Implement a data lake and data warehouse, using Airflow and AWS cloud, allowing non-tech 
members of the company to have access to business data in a security and localized place. 
- Built a data framework that improved overall performance for building a single data pipeline in 
_NUMBER_%. 
- Integrated data from multiple third party APIs. 
xxxx , Remote 
Data Engineer, _NUMBER_/_NUMBER_ – _NUMBER_/_NUMBER_ 
Implemented real-time data ingestion pipeline for non-sql data with Apache Nifi, AWS 
SNS and SQS and Kafka to store data in S3. 
Design and maintained a data ETL pipeline, in AWS, improving performance and 
reducing operation costs. 
Maintained StepFunctions and Airflow pipelines, adding new features and improving 
overall performance. 
xxxx , Sao Paulo, SP 
Machine Learning Engineer, _NUMBER_/_NUMBER_ – _NUMBER_/_NUMBER_ 
Worked with a multidisciplinary team to develop an artificial intelligence computer vision 
solution in Python . 
Design and implement an AI Pipeline in Google Cloud Platform  environment for faster 
model training and deployment, reducing the overall time of _NUMBER_ weeks to _NUMBER_ days. 
Create data ingestion pipelines in GCP BigQuery improving the data science team productivity. 
Implemented Data Version Control  framework into company data culture. 
EDUCATION 
Universidade Federal xxxx, Brazil, _NUMBER_ - _NUMBER_ 
Computer Engineer 
SKILLS 
AWS  
Spark, Kafka, Airflow 
Python, Pyspark and SQL",Help,dataengineering,s8n92n
592,"I need to trigger my ADF pipeline when three files arrives in paths : container/folder1/file1.parquet container/folder2/file2.parquet container/folder3/file3.parquet
Only when these _NUMBER_ subfolders gets new files should the ADF pipeline trigger.
How can we achieve this?",Help,dataengineering,s24qyo
593,Can anyone help me with some good interview questions on azure data engineering tech stack? Any design related questions are also welcome..,Help,dataengineering,s700yf
594,"**So a little context about me:** 
Undergrad: Double Major in CS and DS 
Masters: Business Analytics 
Experience:
* Two Internships as a Data Engineer .
 * I learned how to build database schemas from an ETL process to production.
 * Became familiar with Azure Synapse and Data bricks.
**Context about my situation:** 
I was recently hired as the first ""Data Scientist"" at a quickly growing startup . My first task is to move their on-premises database to a cloud-based data architecture. Our department is low on funding and I'm currently the only person working on this project. . I want to build the best architecture that I can given my experience .
After building this data architecture I'm hoping to move into more analytical role where I would like to leverage data to make business decisions.
**The Current Data Model:** As of right now, the database is built in PowerBI using many DAX and PowerQuery statements. . Most of the data is populated out of salesforce.
**Objective and Advice:** Although I have experience in Azure Synapse and Databricks, I am open to learning any data architecture platform. I hear many good things about snowflake and I'm wondering if the company should pursue this tool. Before I start anything, I am focused on learning how the current database was built and I think I want to reverse engineer the process and build it from the ground up.
I have a lot of questions:
* What should I do first?
* Data Lake vs Data Warehouse?
* Which platform should I be using? 
* For those experienced in the field, what is one thing you wish you had known sooner?
 * What platform is the best in your opinion?
**TLDR:** I am a fresh grad tasked to build cloud data architecture, looking for any and all advice. 
Excited to hear your input :)",Help,dataengineering,s3bb7b
595,"if any one have experience on ADF and Airflow what is it look like on working , which is better?",Help,dataengineering,s76yg7
596,"Hi all! I am studying a MS in Big Data and this year I have to do my final project and I would like to know the opinion of the community. My main objective is to use this project to help me to get a junior job as a Data Engineer . After some research, I came to the conclusion that I mainly need a project to show my skills in Python, SQL and some Big Data technologies, and preferably using real data instead of a static dataset.
Considering this, I have decided to use the **Twitter API** to read tweets with the #nowplaying hashtag and get song information from **Spotify API**. The technologies that I plan to use are **Airflow**, **Spark**, **Cassandra** and **Metabase** or, if I have enough time, build some frontend with **Flask and Bootstrap**. Also, I would like to use **Docker** to run the project in a container and make easier to reproduce it. Additionally, my tutor is a researcher in the Data Science field and we will probably add some machine learning when I talk to with him about my choice, so this may vary.
Any thoughts or opinions? Would you change anything in this project considering my objective? I am new to technologies like Docker, Flask and Bootstrap, so that is why this part is more like a ""possible next step"" than an actual phase. I also have a question related to Docker: if I develop my project and then I decide to give a try to Docker, can I just migrate my full project to Docker, creating a container with all the ETL flow and the visualization part? Would it be difficult?
Thank you in advance! _EMOJI_
_URL_",Help,dataengineering,sgptiz
597,"My company is sponsoring the courses on Coursera for the next _NUMBER_ year. is there any course on data engineering/data engineering tools on there worth taking? The IBM professional data engineer course seems comprehensive but have read lot of negative reviews regarding it here. Any suggestions?
Edit: my company currently makes use of Airflow, AWS S3, splunk, Qtest, Redshift and Informatica Data catalog for data lineage and governance.
There is news floating around about migrating from redshift to snowflake.",Help,dataengineering,s9ba7p
598,"I understand that bad things can happen when SQL queries are executed based off user input but I'm having trouble figuring out what situations are acceptable and if they're not, how I can fix them.
I'm using dbt and I have an automated ETL that runs my dbt models on a daily schedule. I've recently added a variable/parameter to one of my models. My pipeline runs in Github workflows and just calls dbt run through the command line with the --vars option. I guess if someone was able to hack into my GH, they could change what's passed with --vars and add something sketchy to the SQL? Idk... is this something to worry about? On one hand it seems like this is exactly what people talk about with SQL injection but on the other hand, I'm confused why they make variables an option if it wasn't safe!",Help,dataengineering,sasfso
599,"Hi,
I am trying to work on a solution that ingests medical records. Due to compliance, it is mandatory to encrypt the data in transit and at rest.
I am planning to read the medical records using AWS textract OCR and update them in DB. The idea is to connect this to AWS Quicksight and show the visualizations.
My questions
_NUMBER_. What are the better alternatives for OCR software to read and ingest medical records? I am using AWS textract and Comprehend Medical. I have tried Python tesseract lib but AWS textract is neat w.r.t identifying forms/tables/blocks in the documents better. 
_NUMBER_. What DB performs better with encryption. Any leads? I am not good at identifying whether to use SQL OR NoSQL DB here, mostly I tend to use what I know, which might not be the best choice. Right now I am planning to use MySQL but I am not sure if it's the best choice.
Thanks, appreciate your time and help.",Help,dataengineering,s3qppg
600,"`pipebase` _URL_ is a low code data integration framework.
In general, the framework allow developer customize data pipeline through `manifest`](_URL_ definition and wire a variety of system through [`pipeware` _URL_ plugins to sync/transform data.
Here is a `Tutorial` _URL_ as quick start, build your first hello world app  with CLI
And here is a list of `Example` _URL_ demonstrate how to compose manifest  and wire external system ex: `kafka`, `rabbitmq`, `mysql`, `cassandra`, `rocksdb`, `aws-s3`, `mqtt` etc.
Have fun !",Meme,dataengineering,s0ooxy
601,"Virtual community peer-to-peer DataOps sessions: **_URL_ _URL_
* Building multi-cloud platforms with tight regulations
* Strategic data team composition with talent shortages and constantly increasing priorities/objectives
* Open source cloud native data lakehouses
* Building vs. buying considerations
* Operational analytics loops on the modern data stack
* Streamlining notifications/comms/jobs with Slack integrations
* Delayed releases
* Data warehouse performance tuning
* Orchestration in modern data platforms
* Modernizing workloads with EMR
* Moving from batch to near-real-time analytics and visibility
* End-to-end orchestration of ML models 
* Building resilient systems
Starts at _NUMBER_ AM PST on Wednesday - appreciate the support as we all learn and grow together!",Career,dataengineering,shezw2
602,"The peer-to-peer DataOps community agenda is live for the free, virtual DataOps/pipeline sessions at _URL_ Feb _NUMBER_ _EMOJI_️ Both live and on-demand. 
Each session will be practitioner-to-practitioner focused so we can learn from each other and includes perspective on:
* Building a multi-cloud data platform in a tightly regulated industry at Babylon Health
* How the team at Slack streamlined their DataOps stack
* End-to-end orchestration of ML models at Volta
* Multi-cloud architecture at Arcus payments
* Open tech cloud native data lakehouses at IBM
* Modern data platform orchestration at Dutchie
* Moving from batch to near real-time analytics at Akamai",Career,dataengineering,sahaj5
603,"I just got an offer for a DE position at Meta and was interested in knowing if others could shed some light on the role to help in my decision making process and check a few assumptions I've made. 
Briefly, what I have managed to understand is:
_NUMBER_) Meta DE work is, mostly, closer to the role of an analytics engineer involving a lot less development and more SQL/dashboard building 
_NUMBER_) Work life balance can be hard to achieve 
For the first point I'm less worried. I do enjoy that kindof work and am comfortable in that kind of role. My main concern here is work life balance. Reviews on Blind and Glassdoor are kind of all over the place with regards to this but many mention long hours and high pressure. I get that this can vary a lot by team, but I'd appreciate any insights any of you might have.
Is work like balance as hard to achieve as others claim? What's the culture around overtime/after hours work?
Edit: clarifying questions",Career,dataengineering,s0xr0d
604,"Currently I’m working as a consultant, with the latest cloud and data tech . 
I enjoy the stack but I came to dread consulting and am looking for a way to transition to a more pre-sales oriented role which I cannot do at my current company and have not much experience in. 
I have the following offers on the table:
A) switch to other consulting company as a principal consultant. Tech portfolio is wider, not focused on “being cutting edge”. Pre-sales is part of the role, as is delivery, project org etc. Probably least jump in TC. 
B) become a trainer for cutting edge big data tech, e. e. spark, Kafka, Hadoop, at renowned boutique consulting firm “experts among the experts” stuff. Keeps the knife sharp, no delivery anymore, customer facing, opening doors but not pre-sales per se. Acceptable jump in TC. 
C) _NUMBER_% pre sales architect/consulting role at a  vendor of a solution for data estate building automation, enabling smaller companies to join the data game etc. Tech wise this feels like a step back, the role however is closest to what I’ve been looking for. Biggest jump in TC.
now my question: if the longer term goal is to become a pre sales guy or even transition completely over to sales in the big data tech I use today - is it wiser to stay closer technology wise even if it’s still more on the delivery side? Is it a good move to take a detour to training to get more exposure towards customers? Or is it ultimately the pre-sales game that I should start leveling up and transition to the same role, different tech later?
I know there’s not a straight forward answer, would just appreciate to get other opinions on it, as I have been discussing with myself a lot already. 
Any insight from people who have done a similar transition already is especially appreciated :-)",Career,dataengineering,sfghem
605,"I finally got a really cool Junior level job in London, and want to thank the community for keeping me sane in the process. I applied to over _NUMBER_ jobs and the whole process took _NUMBER_ months. 
They said I'll be primarily using AWS Glue for the first couple of months, which I have no experience with. If any of you know any tutorials I'd love to take a look.
Thanks!",Career,dataengineering,s5cldd
606,Hi all! Hope my post is relevant to the group. I am thinking of applying for a position ay Spotify and would like to ask if anyone has any experience of what working there is like. Keep in mind that I am referring for the emea region and I am really familiar with the Dutch working condions. Every input will be highly appreciated! And a happy & healthy new year!,Career,dataengineering,s2defm
607,"Hi all,
I am interested in contributing to building technologies that facilitate data engineering. Example technolgies like spark, kafka, Airflow etc. Basically I'm interested in building data engineering tools more than using them. 
Should I then target DE roles or SDE roles?
Please let me know if you'd like to me to add more context to the question.",Career,dataengineering,s59snh
608,"Taking in count that usually Data Architect are very senior Data Engineers, also Enterprise Data Architects and Data Architects being almost the same job. 
Are the salaries of Data Engineer Staff/Lead, Data Architect and Enterprise Data Architect close?
Thank you",Career,dataengineering,sdtvny
609,"_URL_
We at DataTalks.Club _URL_ are running a free data engineering course.
We'll cover:
* GCP, Terraform, Docker, SQL
* Data pipelines orchestration 
* Data warehousing 
* Analytics engineering 
* Batch processing 
* Streaming 
More details here: _URL_ _URL_
See you tomorrow!",Career,dataengineering,s59bpv
610,"Hi,
I've just started a new job as a junior DE and I will work with Ab initio, among others. I can choose how much of my time will be spent with Ab initio and I'm wondering whether it's a helpful technology for my future career or is it becoming obsolete.
Thanks",Career,dataengineering,s745m0
611,"TLDR;
No Bachelor's degree, and not much time, money or motivation left to get one. Former musician employed in BI through right time, right place and hard work. Should I look away from the data space for an opportunity to build things with code? 
I started my journey into tech/code ~_NUMBER_ during what would be a _NUMBER_ year battle with an adverse reaction from a common antibiotic. Prior to this, I had successfully made a career for myself as a professional musician and director at an entertainment park just before landing the role of my dreams as a guitarist in the United States Air Force band. Once I realized my health wouldn't allow me to get through the compulsory basic training, I decided to make a pivot into something I could do if I had to keep walking to a minimum. I went through Al Sweigart's entire Automate The Boring Stuff With Python which somehow led to a now seemingly misguided entry into Lambda School's  Data Science program.
In retrospect, I think I may have been better off in the web development track, but for someone with only an associate's degree in music and zero quantitative skills, I did surprisingly well in most areas and finished the program. After over _NUMBER_ applications and exactly zero interviews, I started to feel a bit hopeless until an old pianist friend saw one of my projects on social media and decided to send a small machine learning POC my way. This led to a contract-to-hire role at his company, a small BI consultancy almost entirely fueled by Power BI.
I love the people I work with and intend to stay for at least another year . Still, I certainly felt a bit off track once I understood it would be a rare occasion when I got to code anything. I've learned a lot of valuable business, time/project management and customer service lessons at my job. I'm even being given the opportunity to help build data engineering into the business which has really encouraged me to stay, but will likely still be mostly tied up learning how to manage expectations with Power BI deliveries for the foreseeable future.
The thing I'm most discouraged about: it seems that everywhere I look in the data space, my lack of a bachelor's degree seems to be the biggest barrier to a future in this field. I can and will learn more data structures and algorithms, new programming languages, constructs, frameworks, design concepts and tools, but at this point in my life, I don't feel that it's practical to spend my limited time and money in pursuit of a bachelor's degree to fill what feels like an imaginary gap. I feel a welling frustration just typing this out _EMOJI_. 
What advice do you all have? Should I start thinking about pivoting to a space with a lower barrier to entry? Should I sink the time and money into that bachelor's? Staying motivated without knowing that hard work will lead to bigger and better things is turning out to be pretty difficult. Thanks for anything this community's got!",Career,dataengineering,s2zue2
612,"I'm trying to decide if the pivot from AppDev to Data Engineering is in my future and would like to get a sense of what it involves in the real world. I'm hoping for a broad-brush breakdown of responsibilities and what languages/technologies/systems are the most important to get your job done.
Bonus points if you're willing to give a rough-swag of your experience and compensation.
Thanks people, I appreciate you taking the time to share your expertise.",Career,dataengineering,s1nb6u
613,"Hi guys,
I'm interested in data engineering but I don't want to end up as simply another SQL monkey. Are there any jobs out there that have you working with container orchestration, IaaC, Cloud, big data tech like spark/databricks and deploying machine learning models all together or would it be better to pick just one of DevOps/Data/ML engineering and just stick with that?",Career,dataengineering,scdesz
614,"so I’ve been a web dev for about year and might be moving into this cloud data engineer position that pays a lot more than I’m making. The talk with the hiring manager seemed nice and she liked I have a cert in aws and was glad I was asking questions and wanted me to schedule a meeting with the tec lead. This was awesome and I was really excited until I realized I legit have zero experience in anything data. I’m only _NUMBER_ and didn’t really do much sql and data wasn’t offered in college so I have pretty much zero knowledge of snowflake, databricks, lake houses, data warehouses etc. 
The manager did tell me the first year for the team was rough since everyone is brand new to all the new technology which made me feel better but I’m nervous I’m going to be hired with high expectations with data and python knowledge when my only professional experience is web dev technologies and languages. 
I love the idea of the huge pay raise but don’t want to screw over the team with the lack of knowledge and catch up time I’ll probably need. Am I overthinking this and just move forward or should I wait to get more experience",Career,dataengineering,sf2zkc
615,"I have been teaching myself to become a data engineer for a few months now, but I wanted to know if I am wasting my time. Should I go for something like data analysis instead, or could I actually land a job without a degree in CS if I can prove I know what I'm doing? Any input is appreciated.",Career,dataengineering,sei1dj
616,"I see a lot of post on how realistic it is to become a data engineer from X field and I think to many of them are worried about what they're doing and not how they're doing it. I went to school for criminal justice and was a phone rep before picking up reporting that consisted of copy and paste cognos reports. Unintentionally I've developed a SQL skill set and picked up some python to move data for reporting efforts. 
If you're using the tools and can speak to it clearly you have a good chance of breaking in. More effort should be placed on this and how you share those achievements on your resume you are ahead of the game.
I'm sure people will disagree, I just want people to not stress out about not being to break in because of lack of opportunity. Make your own opportunity who cares if you over engineer a solution.",Career,dataengineering,s5ndm2
617,"Are there people here who do freelancing as data engineers around here? How is the freelancing spectrum for data engineers, how is the pay, how do you find clients, what pros and cons do you see to it?",Career,dataengineering,sf4g0y
618,"Just saw it on their website as I was looking for the job postings:
_URL_ _URL_",Career,dataengineering,sdxt1n
619,"Any fellow women in Data Engineering?
How are you enjoying the free women's toilets at tech conferences?
How are you doing in your career progression?
I often feel like I have to work 3x as hard as a male Data Engineers to even get noticed in the right ways.
I recently found out I'm being underpaid by quite a bit compared to my colleagues  who add a similar amount of value to the company. I try not compare myself to others too much, but knowing I'm on roughly the right track is useful.
Also I have to deal with a lot of assumptions from my colleagues that I know less than they do. Each time I join a new company, my colleagues are ""surprised"" at my abilities, and it takes them time to realise I'm not a complete idiot and I actually know what I'm doing.
I've been discriminated against job applications. I once applied for a job to a company only to be ghosted. A few months later, my ex colleague got the very job I applied for. We graduated at the same time, same degree and grade but my university is more prestigious. I also graduated with a masters, he graduated with a bachelors. We worked at the same two companies, quite literally joining the same day at both companies. Same job titles and similar experience in both companies. Yet, I didn't even make the interview stage.",Career,dataengineering,s6ac6p
620,"Hi All! 
I have a question about what should i do with my career, but firstly, let me give you some background. 
I'm a data engineer in a small/medium consulting company. We are building modern data warehousing solutions . Right now I'm stuck in a project where we are using Azure Data Factory for ETL, Azure SQL and Azure Data Lake. In this project we are focusing exclusively on no-code, low-code solutions. Unfortunately there is no way for me to change the project. 
Here is the question.
I started having doubts if staying in no-code / low-code stack is a good idea. Whenever i look at the job postings there is always Python / Scala / Java required . Because of that, I started to think that if anything bad happens it would be hard for me to find another job without proper experience in code-based solutions.
Should I look for another job where I would get more code experience, what would you guys do?",Career,dataengineering,saoqks
621,"I've been working in the data space for _NUMBER_ years. Predominantly designing, building and supporting data warehouses. I transitioned to python _NUMBER_ years ago and currently using python and neo4j in the clinical data space. 
When looking at job specs, my most obvious gaps are cloud and ML.
I'm curious about, cloud, machine learning, blockchain and kubernetes, but I'm well aware that if I don't use what I learn, I'll forget it. 
The other aspect I need to consider is the accreditation. It's possible to learn all the skills using free educational material online, but that doesn't come accredited. 
Courses can be remote or based in the UK. 
Is there a highly regarded course provider in the UK that holds training bootcamps or similar? 
Any help appreciated.",Career,dataengineering,s8g2am
622,"Following a CS conversion masters, I was hired as a technical account manager a few months ago at a startup. I started automating some processes for them, which my boss took an interest in, and after a few conversations I'm now in charge of building ETL pipelines with Python for internal data  into SQL databases, analysing the data, and developing PowerBI dashboards. It's early days but, if I make some good progress with it over the next couple of months, I'll likely switch to being an actual full time data engineer at the company and not a TAM with some engineering responsibilities.
This is obviously a great learning opportunity, but as the company has no formal data engineers working there and the dev team are all product-focused I have nobody directly above me for guidance on this specific project. While it's exciting to be managing this project end to end at such an early point in my career, but I want to make sure I'm setting myself up for success later down the line.
So I figured I'd ask here:
* What are some technical skills/technologies that I should aim to build up while working on this project?  
* What are some organisational skills that would be worth practising? 
* If you work with data engineers and analysts, what do you wish they knew/did differently? 
Thank you!",Career,dataengineering,s1q190
623,"Hey Everyone 
I am learning Data Engineering, have couple of roadmaps that I am following, question that I have is that I wanted to be a bit ahead of the curve and update my resume and LinkedIn profile, is it ok if I add python and sql etc into my profile early on? I am only few weeks in. I am coming from Transportation and Hospitality industries, so besides what I am learning not much relevant experience 
Just like everyone else new to this, I want to get my foot in a door as soon as possible, haha
Thanks in advance Everyone",Career,dataengineering,seqfas
624,"So I'm currently working as a Data Engineer on GCP/Bigquery, but I also have courses on Databricks and Snowflake that I could study but not sure if it's really worth it? Is it better to stick to one stack and really build experience and expertise on it, or is it better to learn multiple stacks even if you aren't going to work on any of the other ones for the foreseeable future? Is there even a benefit to doing so? No clue what the right way to progress in my career is and hoping for some insights on this from the folks here",Career,dataengineering,s9vz5f
625,"_URL_
""Hi guys!
I have recently got a job as a reporting specialist at a Law Firm. My responsibilities however are different from what had been promised at interview and I actually cannot develop my SQL skill so I'm currently looking for a job as a Junior Data Engineer.
Could you please have a look at my CV and give me some feedback on what can be improved?
I would be very grateful for your help.""",Career,dataengineering,sbojex
626,"Planning a training for entry level data engineering positions for _NUMBER_-_NUMBER_ months covering the below topics. 
**Part _NUMBER_** 
_NUMBER_.Core Concepts
_NUMBER_.Hadoop
_NUMBER_. Hive
_NUMBER_. Sqoop
**Part _NUMBER_**
_NUMBER_. Spark Data Processing 
_NUMBER_. NoSQL Databases
_NUMBER_. Cassandra 
_NUMBER_. Hbase
_NUMBER_. Airflow/Nifi
**Part _NUMBER_. Streaming**
_NUMBER_. Spark Streaming 
_NUMBER_. Kafka Streaming 
**Part _NUMBER_. Cloud**
_NUMBER_. AWS
_NUMBER_. S3, Athena, Glue
_NUMBER_. EMR
_NUMBER_. Step execution
**Part _NUMBER_ Projects**
_NUMBER_. Project _NUMBER_ : Sqoop to hive import
_NUMBER_. Project _NUMBER_ : Spark Batch Job data load to Data Warehouse via orcheration tool
_NUMBER_. Project _NUMBER_ : Apache Kafka realtime sata load via spark to Cassandra
Anything more I should add to make it more appealing ??",Career,dataengineering,sewhgf
627,"Hello,
I was first BI hire in company . 
Because of type of the business, usage and ""agile"" thinking I chose the database of choice PostgreSQL, on-premise. Database is scaling amazingly well , there are literally _NUMBER_ problems, small to none maintenance needed after properly setting the config. 
Now, because of this boom in hiring, I decided to try job market and I horrifying discovered that _NUMBER_% job positions are requiring some cloud experience. 
What are my options for keeping up to date with cloud, given that I am keeping same position? 
I still fail to see how for exactly our type of business would db and ETL process on cloud be advantage (very predictable and stable usage and processes, hosted on computer with specs far higher than needed, far less unplanned shortages as AWS's us-east1  
Only thing I can think of is to create some bullshit ML side-project, where we need cloud for burst computing power. 
Any suggestions? I have theoretically full power as I am in charge of everything BI related.
Tl;dr: I love my on-premise Postgres, job market wants me to switch to cloud, how to get best of both? 
Also, is it hard to lie about your cloud experience? I mean everybody is saying that its maintenance free and things just work, so in theory it should be super easy to learn",Career,dataengineering,s1spyc
628,"Sorry if this may not be the right sub Reddit for this question but I recently received an offer for a data engineer position at 75K, is that reasonable? I have _NUMBER_ yoe working as a DE, where I’ve been getting paid severely below market value already  so this is a huge jump for me, but just want to make sure I’m not selling myself short again. I currently reside in Texas",Career,dataengineering,sgapx2
629,"Hey guys, I joined as a data engineer intern about a week ago. This is my first job straight out of college, and I have no prior experience in data engineering whatsoever. 
I was advised to learn Python and PostgreSQL by my leader, and for now, I'll just be watching others work. Everything is new and overwhelming, and it's hard to understand the architecture. I asked all of my questions, but I still don't seem to understand it, and most of the senior leaders are always busy due to remote work.
Some of the technologies I'll be working on are S3, Lambda Functions, Elastic Search, Kibana, Kineis, and Glue. I have a conceptual idea of what they do, but overall, it's still blurry. 
If anybody could point me out to the right resources or path, it would be of great help to me.",Career,dataengineering,sbvbau
630,"Hi everyone! I've been offered 100k for a DE position in the LA area. I will be moving there and will get a 4k relocation bonus. Wanted to know if this will be a good salary to have a normal/bit above-average life. Currently live in Phoenix, AZ on a 60k salary. As far as I've researched I will be able to maintain my way of life and maybe even increase it a little bit, but after checking LA rents I'm not _NUMBER_% sure, pleaset let me know your thoughts!
Edit: Wanted to add that I have 2YOE total with only _NUMBER_ months in Data Engineering.",Career,dataengineering,s36eqp
631,"Hey folks. I’ve been working as a data scientist for the past _NUMBER_ years in the Canadian fintech industry, on top of my regular, or traditional if you may, data science/analysis or machine learning projects, I got more and more involved with building automated ETL pipelines and cloud infrastructure and ML API development. I started a few weeks ago to apply for some data/cloud engineer roles kinda just wanted to see what’s out there.... Now surprisingly and with a bit luck I guess, I received an offer for data engineer with a consulting firm... I want to pick your brains to see do people usually move from data science to data engineering or the other way around?
A bit background about me: I only have a BS degree in engineering with a minor in statistics  not trying to show off or anything and prob not important but the reason I am bringing this up is the majority of my data scientist colleagues have a master or Phd in stats/engineering/physics, I used to feel that I was just lucky to be working among them. Not saying data engineers are anything less, but I expect data engineers are more engineers and not necessary need a Phd for that matter which is why I never did a master or even interested in going back for one... honestly I was once been told if something is not working blame data engineer not the data scientist.... Lol imaging it prob would be a lot more stressful in that role...
So just want to know has anyone done this switch or the other way around and do you think you’ve made the correct move? What would you consider before making any decisions?
Thanks in advance :)",Career,dataengineering,sdoguq
632,"Interviewing at a company  that has a young and small DS team  looking for a DE. The team is comprised of _NUMBER_ DS, _NUMBER_ Developer, and _NUMBER_ Data Analyst. This role is geared towards serving DS exclusively and bringing their work to scale. A DE team also exists, but they serve the company as a whole. In terms of maturity and tech stack, they are in the process of migrating data from on-prem to the cloud . 
For myself, I've been an analyst within the scope of BI at my current organization and I've touched everything from data modeling, developing ETL/ELT processes, stakeholder management, and building dashboards. I'm looking to jump into an engineering role since that's what I enjoy the most.
Has anyone been in a position of building new infrastructure from the ground up? Do you think it was more helpful in your learning and career progression, as opposed to a company that already had a mature and up-to-date data stack? For example, a number of other companies I'm interviewing with are already fully on the cloud and are using tools like dbt. The benefits I personally see are being a ""pioneer"", setting up/introducing the new infrastructure, and understanding the data lineage better.",Career,dataengineering,scx9kd
633,"About me: 
- _NUMBER_ years in academic-style roles in economics. Experience using R and Python to build datasets for econometrics. 
- Enrolled in Masters of Data Science degree part-time. Most of the classes I’ve taken are DE courses .
- Looking to move into more more DE-esque roles but worried that I don’t have the correct “formal” training. 
About the Role
- Mid-size firm specializing in financial analysis for small financial firms.role is on a small team of _NUMBER_-_NUMBER_ data analysts.
- Job title is “data analyst” but tasks mainly involve moving data submitted by clients into their on-prem data warehouse. “Side-projects” to automate this system and eventually move everything to cloud in a year or two. 
- job would be to bring “technical skills” to the team and help them make this transition.
My thoughts/ concerns
- I don’t have any “formal” DE experience, what I do have is mostly self-taught. How should I think about continuing this self-education in a role like this? 
- Is it more important to get “hands-on” experience, even though it’s outside of a modern DE solution? Or, should I focus more on trying to get into a role with more established De practices?
- Pay seems mediocre for this type of task .",Career,dataengineering,sc5f7d
634,"This will be long so I'll TL;DR at the bottom. 
I am trying to get some outside perspective on my career from the sub. I have been feeling the pressure of my position and it has been triggering a flight or fight response from me. Half of me says to dig in and learn, and another part of me says to get out asap and find a new path. 
For starters I work for the fed. I began my career in mid _NUMBER_, shortly after graduating college. I was hired into a rotational management program that lasted _NUMBER_ months. After completing the program I took my first official position within marketing but in a reporting role. This was my first exposure to SQL. The position had great potential for me to learn how to manipulate data for reporting purposes under a great boss, but that never happened. A reorg later and I was basically a report jockey. I would run some basic stuff and generate some pre made reports and then send out emails. It was extremely easy, but I was bored and wanted to get into a job with real skills... 
That led to me getting a job for the data warehousing org. I was not qualified by any means, and they knew that, but they were willing to bring me on. Initially they started me in one of the data marts. They told me they wanted me to work on a few things but that the primary goal was to get trained and learn development. Now there was essentially no onboarding, no training, no real guidance at all. I sat at my desk for weeks, months with nothing. I was trying to get a grasp but the world of development has so many facets and with the layer of government red tape over every process I was at a loss. After a few months and another reorg later I land on a different team within the same org. Now I am handling audits and agile PM work, no development. 
After the better part of a year of doing that I end up in a temporary assignment as a developer on a different team. Now this is related to the first position I had but very different in terms of what tools they used and the processes they used to promote the code changes. However I have a manager who knows my situation and was great with working with me and giving me projects I could do early on. I wasn't treated as a full on dev since the position was temporary but I was promoting code and learning. 
Well that lasted a handful of months and fell off to not having much work. Well what do you know, another reorg, a big one. Our whole organization was wiped and rebranded, a full on top down reorg. I was not selected early on to lateral into the equivalent position in the new org. 
From March _NUMBER_ - Sept _NUMBER_ I was unassigned. I still had to sign on, join some meetings, but I had no work, and nothing to do. The manager of my team was also in the same position so he was checked out. Everything was leading to me being laid off in October. At this point I haven't really had much contact with my previous team in a while. I wasn't technically on their team, and they had new managers, I was on an island trying to find a job. 
In the last round of interviews before being laid off, I got several interviews. Before I could even attend them all, I get a call from the new head of the new version of our organization. No interview, just says you got the job if you want it, but I need to know within a few hours. So right then I have to decide to stay in the role, but take the new position which ended up being a promotion for me, or risk it on the interview I had just done and the _NUMBER_ more that week. I ended up accepting the position because I found it interesting and didn't want to change AGAIN. 
I have now been on this team since Oct _NUMBER_. I have roughly _NUMBER_ months of actual dev work and I am in a full on data engineer role. I have inherited _NUMBER_ applications that originated in the early 2000s and have also been through reorgs and conversions and I am on a _NUMBER_/_NUMBER_ on call rotation every month or so. There have been several major changes through out the reorg process to how code is promoted and there is very poor/ little onboarding or training material. The manager I was working with previously did not get rehired into the same role, a new manager with no experience in what we do is now the manager. 
This is so long but typing this has been cathartic. 
TL;DR - I am not qualified for the role I am in. I have essentially no support from team members or my manager any more and I don't have much means of training at work. I feel like I am just waiting for something to fail or a project to come to me and blow up in my face. I find this field interesting, but I just don't know if I can get there. Part of me says to dig in and try but another part of me says I'm too far behind and I can't ever be great at this job. I don't even know how to begin to be good at the job. 
For those that are interested in what tools I am using... 
Ab Initio, UNIX  , SQL  some other things too but that is the primary ETL stuff.",Career,dataengineering,s1he6t
635,"I'm a data engineer with _NUMBER_ years of experience in public sector primarily working with Microsoft stack . Been looking for jobs and got these two offers
_NUMBER_) Senior Data Engineer
 In a greenfield startup, that has been hiring a lot and had a number of acquisitions over the past few years. Job sounds very technical and in a small team which i feel would be a great challenge. 
_NUMBER_) Enterprise Data Architect 
In a large established organization, building data model through low code from CRM solutions but involves a lot of comms with loads of stakeholders, and setting up the general data strategy for organization. I felt it would be a great opportunity to learn the other side of things sort of related to data engineering. 
Main question I guess how to evaluate which offer to pick considering in future Id want to have a bit more of leadership role but still be involved with creating technical solutions :/",Career,dataengineering,se0mk7
636,"Hi everyone,
I've been at my current company  for around _NUMBER_ months and it's pretty fun - although it's more analytical engineering than core data engineering I'm still learning a lot and gaining domain knowledge. 
However, because I worked at an early adopter of the MS Azure stack I get daily job offers in the region of _NUMBER_-90k - this is substantially higher than what I'm on, for reference _NUMBER_ years ago I was working as an ops analyst earning £37k.
I wanted to ask has anyone jumped shipped due to the crazy market demand and taken on the salary increase.",Career,dataengineering,s24yoe
637,"I have started my career as a Data Engineer and currently working on Informatica Powercenter ETL tool and a lot of SQL.
I got aware that this ETL tool has no future and now i want to shift to other roles in the DATA domain.
By surfing the net ,i found out that Data Engineers mostly work on Bigdata stuff like Spark, Hadoop, Kafka etc. or they work on cloud stuff like Azure , AWS etc.
Experts in this community please help me to chose between Bigdata or cloud based on its future, number of opportunities etc. and please lay out a roadmap to follow from scratch.",Career,dataengineering,s5kc29
638,"Hi,
I have around _NUMBER_ yrs of experience into software developer + data engineering. I am pretty interested in blockchain and would like to transition to that domain. How easy or difficult it is to transition at this time? Are there any work involved of using blockchains in big data?",Career,dataengineering,s1yy9j
639,"Hi, I m an experienced BI Data Analyst with around _NUMBER_ years of experience in Reporting and writing SQL queries. I have recently completed my masters in CS in Europe with majors in  as well. But Lately, I m interested in Cloud DevOps Engineering and even cleared AWS SA certification. Is it a good move to shift to DevOps/cloud ? or should I concentrate on Data Engineering? my only concern for Data Engineering even while applying for jobs is half of DE jobs are based on tools like ADF, Informatica,SSIS and remaining have good job openings like SQL,Spark, Python, Airflow, etc. Despite all that DEVOPS ENGINEER were paid more than Bigdata Engineers and has a high volume of openings too, To add on that DevOps engineers have the same tech stack everywhere unlike Data Engineers.
Is this a good choice moving from BI data analyst to Devops cloud Engineer ? Does hiring manager will even consider some one like me for Devops positions ?",Career,dataengineering,sbuzdq
640,"My current job is offering me a boot camp to either learn ab initio or azure  in for the next months. Do you have any opinion in what is the best option in terms of carreer development? 
PD: ATM I don't have any prior experience in either tool and I can only select one option",Career,dataengineering,s3k2np
641,"Hi, I'm a Sr. Data Engineer with ~_NUMBER_ years of experience looking to transition into the realm of Data Architecture. I've found passion in building scalable systems and cloud architectural components that support data engineers, data scientists and MLEs perform their duties better. 
How do I make an active transition into Architecture driven roles? Data architect roles are sparse, with not many options for less than _NUMBER_ YOE. What kind of jobs do I look for and how do I let recruiters reaching out to me for DE roles that I'm more interested in architecture? 
Would love to hear the community's advice!",Career,dataengineering,s0x928
642,"company I work for does all of their work manually. for instance if a price of one item changes, then every item that is related to this one item also needs to change. it is basically a large network of products and prices and orders change constantly. they have a person dedicated to typing in and updating new prices into spreadsheets manually and calculating the new price.
What I was thinking of doing was creating a simple Microsoft Access database that will have connect all of these different price listings in such a way that once the price of one item changes, the prices for all other items change accordingly and automatically.
Since this isn't part of my work responsibility I would have to do this additionally once Im done with my work and Im wondering about how to charge for this.",Career,dataengineering,s5hzs2
643,"Hey Everyone 
What is the best Job board for data engineering?",Career,dataengineering,sa9m13
644,"Personally, I am finding myself less interested in the business and more interested in the technology and data. Is that a similar motivation or perspective for those who switched out of DA/BI?",Career,dataengineering,s04c4i
645,"I am joining a company as an “associate data engineer”. Would it be wrong for me to put “data engineer” for the job title on LinkedIn? Which title would you put and why?
View Poll _URL_",Career,dataengineering,s56arg
646,"I'm a DE/DA with <1YoE. Although I'm on a team, all our roles are very independent and responsibilities change week-to-week . My work involves automating current processes, making reports/dashboards, implementing/improving ETL. Most of it is ad-hoc requests or fixing bugs.
Manager has been satisfied, and only suggestions for growth have been quite generic: ""take ownership of more projects"", ""think about where you can help"". I like not being micromanaged, but sometimes think I could use a bit more direction.
_NUMBER_. Any tips for growing at a small company? Is it possible?
_NUMBER_. What skills should a DE aim to develop, or what projects should one have experience with, within their first year?",Career,dataengineering,s6iyeh
647,"I recently got an interview with one of the Big _NUMBER_  for a data engineer job. Just curious if anyone worked for them before?
What was the work like and how was your interview? 
Thanks in advance.",Career,dataengineering,s51d6t
648,"After being confused for multiple years, I've finally decided that a career in data science & engineering is right for me. I just wanted to know which tech companies have good data engineering teams so I can align my job search accordingly.",Career,dataengineering,s7n0bw
649,"Reaching out to my lovely community in Ontario and Canada to understand how much comp adjustment should one expect and in particular how much raise should one expect for _NUMBER_-_NUMBER_?
As an architect in the data engineering, what should be the expected salary range for a mid senior level position.
Any information any existing resources would be helpful.",Career,dataengineering,sa6ucd
650,"Hi all,
 
I hope this is appropriate to ask. I'm transitioning over from a wholly different field and have been approached by recruiters to interview for META , but have so far been hesitant, as I have heard once you work for them it can be hard to find work again, and from what I know of FBs practices, any gig with them would be basically a quick cash grab for a year or so. 
 
Is it the case that working for META can ruin the odds of future employment in DE?",Career,dataengineering,sakz79
651,"Hey guys, I joined as a data engineer intern about a week ago. This is my first job straight out of college, and I have no prior experience in data engineering whatsoever. 
I was advised to learn Python and Postgres by my leader, and for now, I'll just be watching others work. Everything is new and overwhelming, and it's hard to understand the architecture. I asked all of my questions, but I still don't seem to understand it, and most of the senior leaders are always busy due to remote work.
Some of the technologies I'll be working on are S3, Lambda Functions, Elastic Search, Kibana, Kineis, and Glue. I have a conceptual idea of what they do, but overall, it's still blurry. 
If anybody could point me out to the right resources or path, it would be of great help to me.",Career,dataengineering,sbvbau
652,"After being confused for multiple years, I've finally decided that a career in data science & engineering is right for me. I just wanted to know which tech companies have good data engineering teams so I can align my job search accordingly. 
Edit: I'm not worried about the experience and skillset part, I've got that covered. I'm interested in knowing which companies are considered as good places to work by the data engineer community",Career,dataengineering,s7n0bw
653,"Can you guys drop some names for me to research? If you have opinions or experience to share, even better.
Edit: to clarify, I don't necessarily mean that they're actually ""good"" at DE , but that they specialize in it/are active in that space. Trying to learn what the DE/BI consultant landscape looks like.",Career,dataengineering,s4rtcq
654,What are all the primary/fundamental skills needed for a Cloud Data Engineer?,Career,dataengineering,s7tmqx
655,_URL_ _URL_,Career,dataengineering,safl90
656,"Hello Team, recently I was rejected at a retail company mentioning that I did not write optimal code that they expect from senior software engineer using OOP's concepts. Since then I have been learning OOP's and now I am not sure how do i proceed from here.
_NUMBER_. Where do i get use cases to write OOP's code.
_NUMBER_. How would i know that the code I have written is optimal
_NUMBER_. Are there any sample codes or books that gives me an example on how do i start after finishing OOPs concepts?
Also, can someone point me to some code base or book that shows how Data pipelines are build using spark? I am interested to learn how raw data gets transformed before it is put into Curated layer of Data Lake",Career,dataengineering,sbfh57
657,"I am currently working as a junior machine learning engineer. But the uncertainty in outcome and the requirements of learning a vast variety of skills and the long hours of time consuming training makes me want to transition into a data engineer.
I have always had some attraction to cloud  and bigdata. I just want to know if data engineer skills are concrete and is it good for me to transition in terms of career growth and salary. Please provide me some insights on this.",Career,dataengineering,sbdycg
658,Was hoping for this to be my first big boy job in data engineering but alas :/. Any advice?,Career,dataengineering,schsvv
659,"I'm on the job hunt and would love to be working with sports data. I had a look on LinkedIn and some of the top sports teams in the UK and Europe  don't have any data engineeri employees.
Do these companies go direct to the likes of AWS and Microsoft and work with their best engineers to build their solutions, or do they do work with specific consultancies?
Any tips for trying to get into the sports industry would be greatly appreciated.",Career,dataengineering,s27npk
660,"Hey everyone! I started my data engineering adventure for about two years now, when I got the offer to join this new department into the same company I was working in a position of a database administrator. During all this time I researched about what data engineer role is assuming, but I feel that I cannot find by my own a good learning path to become better in this field. The company is moving slow in this direction, and the department is not involved into many projects that can help me develop my skills.
Is anyone of you interested to share with me his/her experience as a data engineer, develop new projects to learn and get new skills together?",Career,dataengineering,s6yhkn
661,"Hey there,
I work at a smallish company in the ecom business. I use azure synapse and power bi for all of my reporting needs.
Originally, I had used dataflows in power bi to report on data loaded from a traditional sql database, but as our datasets have grown we've quickly outscaled that solution as any report over 1gb failed to refresh in Microsoft's BI service appropriately. Scaling was a nightmare and I quickly asked sales to cool it on this feature. 
Today, I load data from various sources, then generate KPIs and dimensions in synapse to load into power bi. This makes the refresh times much quicker as I am refreshing data already prepared and aggregated. This also means I can scale my solution out to multiple clients with relative ease.
One thing to note is i do most of my work through the synapse UI. I create all of our dataflows and pipelines through the synapse UI. It is very 'low code' and I'm not sure if this is optimal or even how to make it a better solution. 
When I look at posts on here, everyone is honing their craft with Python notebooks or using some programming solution to get it done. 
My questions to my fellow redditors:
Is it even possible to get a data engineering job with some SQL, BI and azure synapse knowledge? I've built our data warehouse from scratch, but I worry because I'm learning to float and I'm being asked to swim. I don't really have a mentor and I'm kinda making it up as I go.
What are some good resources to help up my game and give me the best opportunity possible to build cool things in this space? As I said, I am building everything from scratch myself. I am being asked to lead this front and I have no clue what I'm doing. I'm desperate for resources to learn from.
Thanks in advance",Career,dataengineering,s2oev1
662,"Hey Everyone! I’m interested in pursuing a career in data engineering and am currently in a dual degree master's program for healthcare and information science. I’m starting to take some courses in the Info science degree and was wondering where my focus should be. 
I am currently enrolled in a database systems course and will likely be taking a sequence of _NUMBER_ classes in databases systems that will cover design, implementation, and SQL. 
I will also be taking ~_NUMBER_ courses in python programming throughout the program . 
Other than those two conceptual areas, what other things should I focus on to maximize my educational opportunities and foundation to shift my focus into data engineering?",Career,dataengineering,s30v79
663,"I have just been asked by the lead of the DE team I just joined to start learning Bamboo. I love learning new tech, but would like to inquire with the sub if this skill will be useful in future DE job hunts.",Career,dataengineering,sbn6n4
664,"I moved from a product analyst role to a data engineering role. At first I had tons of work and was so busy. Now that most of the pipelines have been built I feel like there isn’t much for me to do. I am a bit worried that I am no longer providing value to the company, but because I have less than one year in my current role I don’t think I can change jobs easily, nor do I want to. I feel like I have to figure out what to build next, but the business doesn’t seem to have the demand for it. Has anyone else experienced this?",Career,dataengineering,sdhqna
665,"Hi, 
I'm looking to move to AU and I was wondering how is the data engineering landscape in Australia? In term of opportunities and salary. 
I am a medior DE with _NUMBER_ year of experience, have been working mainly with Spark, Airflow, DBT, Containers on GCP and Azure . 
How hard is to find a company to sponsor your visa? 
Thanks",Career,dataengineering,sdh63a
666,"I was wondering if some can explain the difference between the roles to give me a better idea about both fields. Also, which one would be better career wise. Thank you!",Career,dataengineering,sdh98x
667,"Edit: Amsterdam 
Hi all, have an interview coming up and was wondering what sort of range I should mention if they ask for salary. I don’t particularly care because I just want a foot in the door, but it’s probably not wise to say ‘I don’t care what you pay me I just want to work in DE’. So what range is normal for fresh graduate junior DE positions in western/Central Europe in a big city. I don’t expect much but it’s probably good to go in with an idea.",Career,dataengineering,sar8nq
668,"context: although as a data analyst I have requirements but often it’s show us insights etc which is creative... For lack of a better word, really 
Do you find your de job interesting? I’m really debating btw de or ds, unsure. I enjoy coding . I don’t love reading research papers. Any and all comments appreciated.",Career,dataengineering,schh5e
669,"Hi everyone. Just looking for some advice about the current situation I’m in. 
I’m _NUMBER_ and graduated uni with a BS in CS this past May. I’ve been working in IT straight out of college and been wanting to get a job in data, so I applied for a junior ETL developer at a startup. I got a call from the CEO two days later telling me he wants to hire me on the spot without any formal interviews. I thought this was great since I have no experience in ETL development or anything in this field at all, and thought this would be a great start. 
The current job I’m at now pays me well. At $_NUMBER_/hr, it’s pretty good for a consultant . I do get the usual health/dental/vision/401k benefits but I don’t get holidays or PTO. If I don’t work, I just don’t get money. Also, I’m on-site _NUMBER_ days a week with no remote option. The big downside is that I do not see myself growing in this field at all. I don’t do anything data related where I can perform SQL queries or code some Python or R. 
With this ETL position, it’s gonna drop me down to $_NUMBER_/hr but the CEO was adamant that I will be learning good real world tools, software, and programming if I want to get into data engineering later . It’s remote but the only benefits it looks like I’m getting is PTO. 
This is where I’m having this weird feeling in my gut. I asked him that the job board listed $_NUMBER_-$_NUMBER_/hr and I would like to meet in the middle - so around the same pay I’m getting at $_NUMBER_-ish/hr. He started growing frustrated  when I asked for that pay. He said he’ll have to see how my performance is and he’ll bump my pay up, and proceeds to say his company is the only company that will give me this kind of experience and further my skills for any future careers I might get in data engineering
He also proceeds to say “people negotiating their pay with me turns me off when I have people in India, or people without experience at all taking that offer in a heartbeat.” Afterwards, he says “this job is the real sh*t and the sh*t we do will give you the experience you need. As long as you prove to me that you can put in the _NUMBER_+ hours work, you’ll get the pay bump”. 
Are all these red flags, or am I just being too greedy with the pay?
Thank you in advance! I’d love to hear your thoughts on this.
EDIT: I also want to note that I am currently going through a Data Engineering bootcamp and am willing to put in time and learn in this field. I just don't know if leaving my job for this will be worth it, and I'm worried I'll be taken advantage of. I do really want experience but don't know if I should start with a company and CEO like this.",Career,dataengineering,s9q178
670,"Hey, I'm a junior data engineer. 4ish months in the job  and I've been asked if I want to learn Ab Initio as one of the tech leads handles it almost single handedly. I initially said yeah, sure. Never heard of it and thought it'd be a good personal development opportunity to add a string to my bow alongside using scala and spark.
However, after doing a bit of research online I'm not so sure anymore. Is it a dead tool everyone is trying to get off of? The company I worked for have renewed it as they aren't ready to get off it after trying but the financial cost of it is something they would rather do without.
I know I'm starting to rant a bit here but should I try and back out of this as it isn't of much use to my career or is it worth learning alongside other stuff for a broader perspective?",Career,dataengineering,sfttyh
671,"Hey everyone, need your help on evaluating my resume.
I recently switched to a job which is more of ops than data engineering. But I want go back to being a DE. Have experience in designing and developing batch and streaming apps 
But since I made a few short switches recently, I guess my resume looks bad  and my applications are getting rejected even through referrals.
Though I am skeptical on this because I have seen many people moving to faang with more frequent switches. 
So I believe there is something wrong in my resume, please take a look and share your suggestions on what needs to be added/removed to make it better for job searches. Would be very helpful and I'd greatly appreciate it.
_NUMBER_. I had intentionally removed most of my previous orgs' experience because it would make my resume longer .
_NUMBER_. And I removed my education details and redacted the recent org I work with.
Thanks everyone.
resume link _URL_
P.S. I have a bachelor’s degree in computer science. Seems I have removed that information instead of redacting it.",Career,dataengineering,s1gezp
672,"DataOps community virtual talks _URL_ _URL_
Practitioners from Zillow, Google, Slack, Babylon, Squarespace, Volta, Unravel, Akamai, EasyPost, Baker Hughes, IBM, Cisco, Wheels Up, DBS, Capital One, Wistia, AWS, J&J all doing sessions. 
Would appreciate the support of this peer-driven community.",Career,dataengineering,sfoqkg
673,"Hi, I'm 32M and currently live in the UK  and it's about _NUMBER_ months I'm looking for job, I'm new immigrant here but have no visa problem to _NUMBER_, I was a .Net back-end freelancer developer for about _NUMBER_-_NUMBER_ years design systems and databases, built web APIs, Windows applications, and I'm familiar with python, wide ranges of databases such as SqlServer, MySql, PostgreSql, SQLite and also noSqls, recently  I've started to learn about Data Engineering and started to learn about ETL and cloud base databases. 
so here is the problem, I started to apply for paid internships or junior roles on LinkedIn, Reed, glassdoor almost everywhere, but the companies reject my application sometimes in under _NUMBER_ minutes. 
could this community give me some advice about what should I do?",Career,dataengineering,s0n04a
674,"I'm starting my first DE job next month, just wanted to know what your day to day was like? How do you get work, what do you use to get your job done and any advice you might have, thanks!",Career,dataengineering,s1vuip
675,"Hi guys,
In around a week or two I'm gonna have a technical interview for Junior DWH Engineer. I'd like to ask you which topics I should have covered by then from **The Data Warehouse Toolkit** by Kimball?
* the job is for a junior 
* I have a job of a Data Analyst right now
Thanks for your help.",Interview,dataengineering,sdu1bx
676,"I've been asked this question in almost every interview i've attended. I'm a Software Engineer looking to transition to a DE. I understand the concepts of Spark and Hadoop and have done several projects using them, but lack practical exposure on how to actually deploy spark jobs/applications in production. I can only explain how I've got it running on my system using containers, but they expected me to explain how I'd do it on a cluster.
Can anyone share some insight on how you deploy to production or are there any resources that can explain how i can implement or learn this.",Interview,dataengineering,s9cjww
677,"Soon I have an interview at one of the Faang companies, I have been preparing at leetcode and feel pretty confident however since I have a friend with whom I have worked previously and knows me , was thinking if I should let my recruiter know, will this help??",Interview,dataengineering,s4dbuz
678,Why can't analytics be done on top of backend database?,Interview,dataengineering,s6wazj
679,Failed coding portion at a faang interview. Although I knew how to code the problem given I froze during the interview.. Are all interviews like this?,Interview,dataengineering,s91atx
680,"I had an amazing conversation with Vinoth about data lake and lakehouse technologies. He gave one of the clearest explanations of what a lakehouse is. Vinoth is an amazing guy and has crazy experience in large scale data systems and he was kind enough to share part of this experience with me.
You can listen to the conversation here: Data warehouses, data lakes, lakehouses and large scale data systems with Vinoth from Apache Hudi. _URL_",Interview,dataengineering,s2e9ct
681,"I have a SQL, Python, PySpark test on Devskiller next week. Has anyone have had experience giving test on Devskiller?
I'm nervous.",Interview,dataengineering,s9fhiw
682,"So I was given a take home assignment for an interview for a new job that is to create a Python library that basically takes a pretty simple csv, runs some basic tests on it, transforms it a little including separating it into different tables, then loads it into a sql db. Included should be tests, explanations on how to run, and pseudo deployment code. It’s taking me a little longer than I expected for a take home assignment and part of that is just distraction etc but I guess I’m just curious as to how long it would take others to do something similar.",Interview,dataengineering,sgny62
683,"Sorry if this has been asked before multiple times but I wanted to include the job specifications in my question. I'm applying for a DE internship with a sports team and I'm preparing for the second interview that will include live coding.
The main project during the length of the internship involves taking JSON files, doing transformations to the file, and sending it to the database, and it would be done in with a Python data pipeline project.
I reached out to a member of the team and in his words 
> I plan on it being one more traditional “blackboard” question and then one higher level pseudocode type question with database concepts that I’m just more curious on your thought process on. While I don’t want to give away too many specifics, the overarching goal of that part of the interview is just to make sure you have experience to do the technical part of the job.
Skills required from the job listing _URL_
Any help would be greatly appreciated!",Interview,dataengineering,sgmrrg
684,"Interesting episode I thought I’d share that discusses big data, and why so many business are hoarding their information. 
_URL_
Description copy and pasted below:
Big data is a big deal! Today, I was glad to welcome Viktor Mayer-Schonbergeroday on the show to discuss how impactful data information and security is, along with how our mental frames change the world.
Bio: Viktor Mayer-Schonberger is the Professor of Internet Governance and Regulation at Oxford. His research focuses on the role of information in a networked economy. Earlier he spent ten years on the faculty of Harvard's Kennedy School of Government.
He has published eleven books, including the international bestseller ""Big Data"", ""Learning with Big Data"", and the awards-winning ""Delete: The Virtue of Forgetting in the Digital Age"" with Princeton University Press. He is the author of over a hundred articles and book chapters on the economics and governance of information.
In _NUMBER_ he founded Ikarus Software, a company focusing on data security and developed the Virus Utilities, which became the best-selling Austrian software product. He was voted Top-_NUMBER_ Software Entrepreneur in Austria in _NUMBER_ and Person of the Year for the State of Salzburg in _NUMBER_. He has chaired the Rueschlikon Conference on Information Policy in the New Economy and in _NUMBER_ he received a World Technology Award in the law category for his work.",Interview,dataengineering,sd6g5k
685,Has anyone interviewed for RedHat Data Engineering Intern role? Could you share your experience and how to prepare? Thank you!,Interview,dataengineering,s20gtf
686,"so..we use git @ my current place of employment and i am starting to look for a new job. 
how should i go about listing it on my resume? 
does ""utilized git for ci/cd"" sound too generic? i wasnt the one who implemented it, i just use it so i cant say that it was me who decided to put this in place.",Interview,dataengineering,s3j1kv
687,"I had a question from a data hiring manager today who asked “ What is single source of truth ? How do you maintain it”?
I am curious to how you would approach the question. I know they use dbt. Does that change anything ? I had difficulty answering because I work for a company and we are about _NUMBER_ people so it’s easier to maintain. The tech company I was interviewing for was size of _NUMBER_-_NUMBER_.",Interview,dataengineering,s6ool2
688,"I'm in the midwest, and not moving. >_NUMBER_ years of experience, and currently work remote for an F50 company. Recently, I've been able to get multiple offers remote from big and small companies, but they're not the famous FAANGs that have onsites and tough leetcode-style technicals. 
I've been upfront with all my recruiters and HMs about my need to be remote. I bring it up in my first conversation. No need to waste anyone's time. Is this the right approach for FAANG? or should I go through the whole process, pass all the cycles, then make that request? I ask because I understand these companies often interview people before they're even considered for a specific role/team.
I also know that Facebook is the most remote friendly. Amazon has people working here, but I assume that's because of their infrastructure. Ohio is one of the AWS Regions",Interview,dataengineering,sadjin
689,"Hi Data Engineering Community,
I am writing this to seek guidance on how I should go about preparing from leetcode for DS/Algo rounds for DE Interviews. I am currently working as a data engineer but planning to switch in next _NUMBER_-_NUMBER_ months . In my day to day job I write a lot of sql and spark pipelines in pyspark , but DS/Algo is something that I am not it touch with. 
l want to understand 
What topics in DS/Algo should I practice from leetcode and how many questions should be good enough? 
There are just so many questions on leetcode , I want to prepare a collection of problems for practice.Can some one who has prepared for DE interviews from leetcode , please guide me",Interview,dataengineering,s7lffn
690,From my research it seems like DE interviews are some of the hardest out there because you are just required to know everything-- your standard leetcode style questions plus SQL questions plus system design questions. Is this correct? Can you give me some examples of interviews you've had  and what questions they asked for a DE role?,Interview,dataengineering,sdg1rr
691,"What should I expect? I was told it is going to be a SQL, some python scripting and Datawarehousing logic questions. .
How many questions will I get and how many should I get right? Thanks",Interview,dataengineering,s15n8b
692,"Currently interviewing for an AE role and made the final round.
Prior to this stage, I completed a take-home challenge that was basically two components:
* Design a data warehouse  that answers business requirements. I also had to make a write-up that justified my design choices and detailed the relationships. There was no code involved in this step. This took up the most time of the challenge for me.
* Based on the same dataset, create an ETL that fulfills another business requirement. Then, query from it to calculate some KPIs and answer some business questions.
What can I expect the final round to be like? I will be presenting my take-home challenge to a couple of engineers, which will be followed by another meeting with a PM and hiring manager . 
Any tips, or things that I should watch out/prep for? I would appreciate anyone who can offer some insights and help!",Interview,dataengineering,s9iqw5
693,"I am looking for a Microsoft solution to the following requirements
* Orchestrate R scripts that perform ETL processes
* Fully Cloud based
* Small scale, low budget
* Needs to manage different R versions and package versions
Our data pipeline using R scripts currently extracts data from our Azure SQL database, and other cloud based applications using API's. Transforms the data then places it back in our SQL databases.
I have had a look at databricks, however i don't think this is appropriate for us, as we'd need to rewrite our scripts using rSpark. I would like to maintain our scripts, and just upload and version manage in the cloud.
Is Data Factory an appropriate solution? Can you manage R versions using Jupiter notebooks? Can you execute these within Data Factory? This may not make sense - i am still trying to understand the environments.",Discussion,dataengineering,sgolt0
694,I installed the gratis Wolfram Engine to play with because I watch the Wolfram channel a lot. I don't remember it getting listed in most of the usage statistics. Is it a really niche thing?,Discussion,dataengineering,sexv8n
695,"Hoping to learn how to make Airflow ""self-service"" for Data Scientists or ML Engineers.
Does anyone have examples they can share on using a framework for the DS/ML teams to create/submit models for production?
Also, would be interested to hear if people are using Airflow and something else like MLflow, Kubeflow, etc to fit DS/ML Ops.",Discussion,dataengineering,s3vzkg
696,"I created a docker image to pick some data from an API and then store it in our data lake, but as an intermediate step, it stores and processes the data ""locally"" before uploading. In this particular case the data is not really sensitive, but it still made me think - **is the data stored in a docker container secure?** is it immediately destroyed after the container is done? can it be accessed by the service I'm running it on, Azure Container Registry in this case? Or persist after the container is done?
In case of processing PII, are there any considerations when processing and storing the data even temporarily in a container?",Discussion,dataengineering,s2dw46
697,"I'm studying for Data Engineering role, and trying to setup pipelines in my own job for experience.
One of my jobs is to take **time writing data**  and **invoice data**  and use them to generate a new report.
Here's the basic steps:
_NUMBER_. Finance team send me these reports each month, or every couple of months. 
_NUMBER_. I manually restructure them as they aren't always in a consistent format. This doesn't take long, as there's only _NUMBER_ or _NUMBER_ columns in each report that are relevant to me. But it's still manual work.
_NUMBER_. I run a python pandas script to join the reports, run some fancy calculations, and output a new report. This new report lets finance know how much money to charge each project in the business
_NUMBER_. Send the reports back to finance.
_NUMBER_. I do this for each month.
I guess for the time writing data, I could find out where these are coming from, and setup something that would extract the reports each month to a repository of my own.
For the invoice data, I'm not so sure. As far as I know, Global IT upload these to a private Sharepoint, and then the Finance team lift the relevant information, before sending to me.
I'm just wondering how a real data engineer would actually approach this, and whether it's worth me trying to fully automate it, or just stick to the way I'm doing it.",Discussion,dataengineering,scaifd
698,"Let me preface this by first making clear I have no connection whatsoever to Dagster or Elementl or anyone involved in the project. In fact, I hadn't even heard of it until very recently. But this shit is unreal, so far it seems like one of the most well thought out tools I've ever had the pleasure of working with. It seems like every feature I think ""that would be pretty cool to have"", they've done it, and beautifully. It makes Airflow feel like this clunky monster that you have to work around, instead of working with. And I think Airflow is pretty cool. I just feel so less limited by Dagster's model. Not to mention their huge focus on testability of pipelines is a complete shift from frameworks where testing is often a  afterthought. I get that adoption is and probably will be slow with Airflow as competition , but Dagster honestly feels like something that if adopted could fundamentally change how people think about building data pipelines. If Dagster was selling stock , I'd be buying.
All this high praise should come with the acknowledgement that the project is young, the community is small, and documentation, while good on the surface, gets pretty scant when you start customizing a deployment. These things come with time. And I'm hopeful their eventual managed service will solve some of the infrastructure complexities. 
This brings me to a question - do others have any experience running Dagster in production? Any issues at scale? Are there any specific shortcomings or issues you've had that would be good to be aware of before diving in?",Discussion,dataengineering,s71ao7
699,"I created a data streaming pipeline using AWS Kinesis. I write data to a Kinesis data stream which triggers a Lambda every _NUMBER_ minute or after _NUMBER_ records - whichever comes first. The Lambda processes the semi-structured data and inserts it into a MySQL RDS database. I also have a Kinesis Firehose that inserts the raw  data into an S3 bucket. I also created a real-time data analytics application to analyze the streaming data 
Link to Medium article  _URL_
Link to GitHub repo with the code is included in the article.
I am however curious whether that approach is the best way to go. So, would like to discuss
_NUMBER_. The best tools available for real-time streaming of data from IoT devices 
_NUMBER_. Which tech stack are you guys using for data streaming
_NUMBER_. Review and suggest improvements
Thanks",Discussion,dataengineering,s5gh8n
700,"I am working on building a ETL Data-pipeline and wanted to ask what is the best practice when it comes to extracting and loading data. 
Scenario - both sql servers are on premise practically next to each other. 
Is it better to 
Read from sql—> Write to csv —> Load to sql 
Or 
Read from sql using pandas —> directly load to sql. No csv generated. 
Both work for me. Wanted to see what is the best practice ?",Discussion,dataengineering,shiifd
701,"Has anyone ever used Azure Databricks  for their ETL? What was your experience in terms of performance?
Context:
We use ADF to bring data into Delta lake, then run numerous transformations using spark SQL, scala, and Python UDF's. 
Goal is NRT data  for our clients.
Fastest we've managed to do this so far is _NUMBER_ mins.",Discussion,dataengineering,s69nfz
702,"Hi all,
I'm a pretty seasoned data engineer  but I have a new use case. I am working at a company thats product will be exporting large swaths of data to clients externally. Traditionally I've built DWs that plug into BI tools or can serve simple exports, but I'm wondering about scaling of exports. If I need to serve 100GB a day to external clients, what is the ultimate tool to do that with?
Env:
We are using AWS, our raw and stg layers are in parquet files, and our DW will be in some sort of DB 
Do I use spark to build exports via parquet files? Do I put in DW and query and send out from there? What are thoughts?
I know using something like Aurora or Snowflake to export will be very expensive as it will be cloud egress but idk how that holds up to cost of just running exports off data lake.
Thoughts?",Discussion,dataengineering,s0omwg
703,"Folks, if you are using databricks, can you please describe:
_NUMBER_. Stack - what are you using for orchestration, managing dependencies, storage, transformations, business intelligence, data science. 
_NUMBER_. What are the major use cases you are using this stack for?
_NUMBER_. Are you using anything for managing the metadata?
_NUMBER_. Do you have a data warehouse as well in addition to the databricks stack?",Discussion,dataengineering,sdpz1y
704,"So a lot of Kimball's principles like strict normalisation and star schemas/snowflake are being replaced by wide tables, which take advantage of cheap storage/columnar querying and are faster to query compared to multiple joins. However, over time wouldn't wide tables be a disaster if certain dimensions  get crammed into the the wide tables only for them to later have to change? How do you guys decide between the efficiency and speed of wide tables vs the strict data validation and robustness of a star schema setup? Or are there cases where you build ur base data architecture as a star schema setup and then create views/materialized views which are in the form of a wide table at the presentation layer?",Discussion,dataengineering,s1668j
705,"How do you handle data quality issues in data engineering pipelines.
For example in our use case we run spark scheduled jobs every _NUMBER_ minutes from S3 to Hudi  tables and jobs that load data from Hudi tables to Redshift fact tables . we do basic data checks before landing to fact tables. 
But our plan is to implement a frame work that can check data quality and take action based on pre configured rules. 
_NUMBER_) what are general design pattern for data quality in such cases 
_NUMBER_) how does it impact on the data latencies . How to minimise. We can also check and correct data after the data is loaded as a secondary batch process.
_NUMBER_) are there opensource data quality tools/ framework that can work in these scenarios ?",Discussion,dataengineering,s594ms
706,"Suppose we have a very big table that is being pushed to S3 . It is greater than _NUMBER_ TB, now changes to the table are being captured using CDC. 
Now we want to read the actual state of data using the read query, which will require merging the big table with the CDC. 
We can't have a job every _NUMBER_ mins that will be merging the whole data. This would be very expensive. 
Also, we can't merge the data for every read query.
What should be the solution in this case?",Discussion,dataengineering,s7nc34
707,"Background: So our data pipelines load data, update history, enrich it and then we come to transformations before actually generating the reporting table.
The data we have is revenue data coming from multiple sources mostly apis. In transformations, one transformation is to identify the type of revenue which requires manual input from another team.
So the manual process goes like this:
- takes a set of unidentified transaction rows .
- puts them in a separate table.
- this table is shared with other team who manually set the type column  of each unidentified row.
- then this table along with original table are joined together to generate reports making sure there are no duplicates and type of unidentified rows are now defined.
Ideally I know identification should be automated, which is already under development but not fully in place. So the manual process of identifying transaction cannot be replaced at the moment.
Question:
- Is it the most optimal way to do things?
- If you do something like this, do u have better suggestions?
- In business analytics, what search term should I use for this reading about this kind of business requirement.
Thanks for all the help.",Discussion,dataengineering,sbr4in
708,"At Airbyte, we just raised a $150M series-B. We also openly shared our investor deck and pitch.
_URL_
In our deck, we share how being open-source and community-powered is key for our growth. We want to solve the long tail of integrations problem with our participative model, remain a non-opinionated ELT tool to address everyone's needs, and provide a fair compute-based pricing in Airbyte Cloud.
We are curious to know what the data engineering community thinks about our strategy moving forward? What are you most excited about? What are we missing?",Discussion,dataengineering,s2bgsp
709,"I believe that companies are waking up to the fact that the time of Data Engineers is better spent on creating assets and building pipelines, not maintaining a dimensional model or optimizing a SQL query. There are many cloud products  that have optimization algorithms that can outperform any human. I think that dimensional models like star schemas, are going the way of the cube. Soon to be relegated to outdated technology. Cloud compute has gotten so cheap, and cloud storage has gotten so cheap, that the cost/benefit analysis is now incredibly obvious: spend more on good engineers, spend more on cloud compute, and get more value per dollar.
I explain more about how Google is implementing this strategy effectively if you want to read more, but the bulk of my thoughts are above \^ Learn from Google's Data Engineers: Don't Optimize Your SQL _URL_
What are your thoughts? I know this is kind of controversial, because so many people are proud of their learnings in optimizing queries.",Discussion,dataengineering,se0mz6
710,"Hi All,
What can be good ways to earn some decent money with data engineering skills as part of second source of Income.",Discussion,dataengineering,s55ilu
711,"We're trying to pick the right tools to build a near real time analytics data pipeline. We have about 60TB of data stored with around _NUMBER_ million new records a day. We're on DB2 and the queries take hours to run . We have a private cloud with open source options available if we want to customize. We're also looking into Azure and Snowflake as alternative in the public cloud. Curious what you think a good stack of tools looks like to deliver this kind of volume in near real time. We think delivery will be either through Tableau, Power BI, and/or APIs.",Discussion,dataengineering,s6xm81
712,"Hi experts,
Assuming I have a bunch of sensors sending out JSON data to Kafka, and on the other side there is a Vertica database.
Now that I'd like to make sure data coming in are of good format, and those don't fit get sent to a data lake dump for further investigation. Further assuming that I know the right schema beforehand.
I think maybe I can setup a schema registry, put up a validation layer between Kafka and Vertica, and route the rejected ones to a separate data lake, say, just a S3 bucket, and convert the good ones to avro format and load into database.
Now the thing is how would you propose to implement it? Data coming in pretty fast , and owner needs real-time streaming for monitoring purpose. I think I can pick say _NUMBER_ of them and write some tests, but I'm not sure which tool I should use for the validation. 
For example, I can definitely choose Apache Spark, read from S3 and validate one by one, and do the conversion to avro, and send to another Kafka server, which stream them to Vertica. So the pipeline looks like this: S3 -> First Kafka -> Spark apps -> _NUMBER_) bad ones to another S3, and _NUMBER_) convert good ones to avro -> Second Kafka -> Vertica.
What's your experience about this kind of application? Thanks in advance.",Discussion,dataengineering,s6dtvj
713,Datacamp seems to be having a neat promo now but I was hoping to get insights from anyone who has taken the course before and thinks it’s worth the time and/or money or not. There seems to be limited data engineering resources  and I can’t seem to find one that covers the most ground and puts me into consideration with interviewers. Thank you in advance!,Discussion,dataengineering,s8qkd3
714,"We use Stitch at my work to get data from various SaaS tools into our data warehouse, and airflow for cases where a stitch integration doesn’t exist or we want to do something more custom. 
The airflow DAGs I write follow a general pattern of 
 _NUMBER_. Create some db tables
 _NUMBER_. Make some API requests and store them in s3
_NUMBER_. Parse the requests and upsert the data into the tables in step _NUMBER_.
I just read the readme of the Singer getting started repo _URL_ and am excited to write my first tap! I’m thinking instead of writing a new Airflow DAG whenever I want to pipe API data into our data warehouse I could write a singer tap and use Stitch instead. Is that a stupid idea?
For anybody who has gone down this path, would you recommend it? Any advice?",Discussion,dataengineering,sgd6cu
715,"Hi Everyone, 
There is lots of discussion around internal data movement, but I'm curious what everyone is doing when it comes to external data exchange? Does anyone else have automated processes that connect partners? Either through APIs or batch files? 
I'm interested in building something in this space, and wondering what everyone does today, and what sucks about it.
Thanks!",Discussion,dataengineering,s3ukpe
716,"I started to wonder if it's okay to have different data models in a single database . 
For example, a database has tables. Some are built as a star schema dimensional model, whereas the rest are created as a 3NF model. 
I understand that this may not be a good practice in a business context. But I wonder if this is feasible and may be helpful in some cases.
Thank you!",Discussion,dataengineering,s4bdit
717,"Snowflake is promoting Streams and Tasks a lot but DBT already provides Incremental Model and Scheduling capability .
I am curious if anyone using DBT + Snowflake stack found use case for Streams and Tasks.",Discussion,dataengineering,s0jyj1
718,"We have a client SDK that sends logs to Elasticsearch.
Sometimes, the SDK might send a batch of millions of logs to Elastic.
I want to enrich those logs.
I want to do this after inserting and asynchronously to not block the insert.
As a solution, I thought about sending the logs also to a messaging broker.
I want other microservice to pull from the broker, enrich the logs and update Elastic.
I want the pull to be of a batch of thousands of logs to perform the enrichment faster.
I thought about using Postgres as a broker because we already use PG, it's performant, and the learning curve is zero.
We use the same PG to store all of our data.
I thought about implementing a table named \`logs\` that the SDK will insert into, and the microservice will pull a batch from it every X minutes.
I am concerned about the performance deterioration that'll happen to PG as a whole because of the large volume of reads and inserts that'll be in the \`logs\` table.
_NUMBER_. **What do you think about the idea of using Postgres, and my concern?**
_NUMBER_. **Which alternative will suit my use case better: Kafka, Redis streams, RabbitMQ, or another one?**",Discussion,dataengineering,s1hsmy
719,"Hi everyone, 
wanted to get your opinion on the subject on how to best manage ETLs at my current job. 
Basically we have not so bad architecture of dozens of micro services, all deployed to AWS, infrastructure managed by terraform. 
But when it comes to the ETLs I think it is very cumbersome what we basically do:
_NUMBER_. all our micro services have GraphQL API exposed internally
_NUMBER_. in order to run an ETL we fetch data from different sources 
_NUMBER_. than we enrich data with lots of different data fetched from sometimes dozens of our internal APIs, the practice here is that usually we use DataFrames for that 
Most of the time this is basically fetching and joining data - which could be easily done in any DWH or even database  but we have everything spread between dozens of postgres databases. Most of the jobs are batch ETLs. Than with that data we do something . 
So now there are two things: 
_NUMBER_. The only advantage of having this architecture  is that the schema of the API and logic behind is decoupled. 
_NUMBER_. On the other hand if we would bring the data into single DWH or something extracting data in the ETL would take few lines of SQL . 
Let me know what do you think is the way to go here.",Discussion,dataengineering,s1ies3
720,"Is it typical to use a DW to reduce complexity of integrations across what are essentially source applications? I.e. use cases where applications need data to function, not just a reporting use case?
E.g. Oracle and ServiceNow both consume/produce different elements of employees, departments, budget accounts, building locations etc. Currently we have a spiderweb of ad hoc integrations between many systems. The pro-DW group of folks would like to simplify this by having applications put/pull from the DW instead of many connections between source systems directly.
Some folks don't seem convinced this is a good idea and are saying it's adding complexity not removing it since we're adding more systems to the mix. But if you draw out the proposed end state of a DW based integration architecture, it would seem to be a lot simpler and easier to govern than our current approach of a spiderweb of many many vertices and edges. Am I on the wrong track?
What do you all do at your organizations? Just tons of JDBC/REST calls, or put stuff in a central model and have everyone try and use that?
Thanks!",Discussion,dataengineering,secb4r
721,"How do you manage your data schemas? I'm curious to find out if anyone stores their data schemas in a central version controlled repository as opposed to something like the confluent schema registry. I like the idea of a central 'data dictionary' and not having another critical service to manage and monitor and it feels like it would make it easier to maintain standards across an organisation. 
But, I have a worry that having a single dependency across our organisation, which in turn may be used for code-gen across many projects, could be problematic. The main issue I see is losing release independence; as soon as a schema change is committed it needs to be released quickly otherwise you block the whole organisation. But, with backwards compatibility rules in places and a fast release pipeline it seems workable.
Does anyone have any experience of something similar or have simply ruled it out as a crazy idea?",Discussion,dataengineering,s3trjg
722,"I recently joined a team that's using Scality for storage. What's the difference between Scality and AWS, and what's the advantage of using one over the other?",Discussion,dataengineering,s2ddl0
723,"My customers are online retailers and the business model is a subscription service where my customers pay a monthly fee for me to maintain the data analytics .
I currently see it working in AWS, where I hook up to their ERP system and bring it into my own database  on a schedule. From there I can automate analytics PDFs in Lambda and connect to Power BI to maintain their dashboards.
Is handling this many different customers  viable or is this completely impractical?",Discussion,dataengineering,s0gdc7
724,"For people out there running Spark on _NUMBER_ c5.2xlarge instances on EMR or Databricks -- why not just use _NUMBER_ c5.8xlarge instance? Everything will just be so much faster. Is there some benefit of using many small instances?
Edit: I see a lot of comments about finer-grain scaling benefits. This is very true. Are there any other benefits?
Wow a lot of comments! I want to give a tldr for people who don't have time to scroll through them all. Seems to me that for data science workloads where you care about getting results quickly and no fault-tolerance, autoscaling etc., go for one big instance! And for data engineering workloads where autoscaling and fault tolerance required, go for many smaller instances. 
I also made a point that on AWS  bandwidth scales with cores -- actually I did an experiment and that is not the case, at least when downloading from s3. So there is a point to be made that more small instances == higher S3 bandwidth.",Discussion,dataengineering,sdqfib
725,"Hello, we are researching options to introduce data lineage into systems at my company . I see many projects and I don't really know in which terms should we compare them. We have our own custom scheduler and we are mostly running Spark  and Flink  workloads, reading from Kafka, sinking to Cassandra and S3. I was looking into DataHub, Amundsen, Marquez and Apache Atlas. From the high level perspective, all these solutions seem pretty similar to each other - of course integration options vary a bit. How have you made the decision to go for one over the other?",Discussion,dataengineering,s5ibx3
726,"I was looking for some declarative ELT tool for creating my analytics solutions, and DBT was the closest I've found. I liked its concept, but I came across quite a few limitations when I wanted to use it. I couldn't specify and create basic things like data types, indexes, primary/foreign keys, etc. In the end, I decided to implement my own - more straightforward and more flexible. I've published the result - dbd on GitHub _URL_ Perhaps, you can find it helpful. Your feedback is greatly appreciated!",Discussion,dataengineering,rzvqj5
727,"Hey everyone,
This can be an odd question. I am in my late 20s. I have _NUMBER_-_NUMBER_ years of Big _NUMBER_ and banking experience and another _NUMBER_ years of audit background in maritime / logistics sector. Besides, I have Economics degree.
In my latests job, I had to work with OLAP cubes to create insights for upper management. This led me to dig a bit deeper to learn SQL. For last _NUMBER_ months, I've pretty much automated my boring tasks on SSRS and I told my manager about it to get database access to create and automate more tasks. Since then I am working with BI Developers and Analysts to create dashboards from these reports but tech stack they use is pretty much outdated.
I realized I enjoyed technical part of the jobs much more than 'finance' part of it. Since I have no background on CS. I've decided to start CS50. It was tough but I learned so much from it. I've stumbled upon Seattle Data Guy on Youtube. Almost watched his all videos he posted in last year.
My question is: is it being too unrealistic to be a Data Engineer? I think switching to Data Analyst is much more doable but that is not what I want to do.
PS: English is not my first language, please ignore any sort of mistakes.",Discussion,dataengineering,s5aoyz
728,"I could not get any response to this _URL_ question. I did more research and work and realized I could phrase the problem better and may be can get better response.
I want to implement SCD
A chunk of rows in data mart table can have SCD in two ways:
- api data is updated.
- manual changes that are conveyed via google sheets by another team.
Is there a way in DBT, to cater for two sources for snapshot.
Thanks.",Discussion,dataengineering,sd3gco
729,"Currently, there are new Reverse ETL products. However, I wanted to understand why there are new Reverse ETL products and why not the cloud ETL/ELT players launched Reverse ETL as a feature only.",Discussion,dataengineering,sgw37e
730,"Dynamic programming, Tree, Recursion etc ....How common are these concepts employed at real time data eng projects? Should I dedicate lots of time learning these concepts or move to core Data/ETL/Data operations related concepts.",Discussion,dataengineering,s9xkw5
731,"Given a greenfield project with a customer with no data warehouse, and no reporting across its separate on-prem systems 
How would you build a solution that enables analytics, and enables the use of data from the various systems in some dashboards, and apps?
The customer has a small IT department, with only one data analyst . There’s no devs, and little room for a big project. I just started reading up on the topic of data engineering and data plattforms and currently this is what I’ve come up with. 
### Part _NUMBER_: Put everything in blobstore
_NUMBER_. Extract data from the systems using Matillion.  Matillion seems like a simple tool to use for non-devs.
_NUMBER_. Place all data in blob store, with each system in a separate folder.
_NUMBER_. Put the relationaldata as csv files in the blobstore
_NUMBER_. Put the timeseries data also in the blobstore 
### Part _NUMBER_: Transform it to a common model
_NUMBER_. Make a common language and define one big model that spans the entire company
_NUMBER_. Use snowflake to transform and build that model using a virtual warehouse 
_NUMBER_. Use snowflake to further transform into a star model
### Part _NUMBER_: Use the data
_NUMBER_. Connect Power BI directly to snowflake start model
_NUMBER_. Build Apis that connect directly to snowflake, and builds apps on top of that 
I would be really glad if you could let me know if I’m on the right track, or if I’m doing something really stupid, and maybe guide me into better solutions.
Thanks",Discussion,dataengineering,sby36j
732,"Hi everyone, 
Currently looking into what the best data quality tool would be . We use Airflow for ETL , and are on Google Cloud for context. 
We are looking at:
_NUMBER_. Monte Carlo _URL_ and it seems good in that you don't have a lot of set up and it will be a complete package. However, since it is closed source it would be an expensive option.
_NUMBER_. Great Expectations _URL_ We like the open source nature of it but have a hard time getting it to work with the rest of our set up. Also, it seems we need to implement it per project/ team where Monte Carlo would be a solution for the whole company.
_NUMBER_. Soda _URL_ This one is the newest and we like the SQL structure of it which goes well with our infrastructure. 
However, really curious if anyone here has experience with one of these tools and what the experiences are! Also, if there are recommended tools that we are missing this would also be very helpful.",Discussion,dataengineering,s7n0qc
733,"Hi everyone,
Just wanted to know if anyone had experience building an events pipeline in GCP.
Mainly, our clients will make calls to our APIs and we will publish the resulting events to PubSub and then Bigquery.
Any tips or pitfalls ? Is Dataflow the right tool for moving the raw events from PubSub to BQ ? 
Appreciate any tips of if someone has experience in this area.
Thank you !",Discussion,dataengineering,sezbcp
734,"Hello folks,
We need some customized metric tables and views built out for a new business team that's growing fast. The data sources will be a combination of external SaaS/ DBs and some internal data, and it's what you'd expect with a ELT/ ETL data stack with transformations done in a DW.
A question that's come up is, could we have these pipelines built and managed for us? If you're familiar with the Managed Service Provider  model, we're looking for a similar solution, where we can define the requirements  and have the 3rd party provider take care of the data engineering aspects for us.
Our CEO does not want to bring in _NUMBER_-time contractors who set things up and leave, and we don't want to hire someone who would be dedicated to this, so this model seems to be the best one for us.
Thoughts? Have you seen such engagement models at your company? Is it even common in the data engineering space?
Also, been searching for a few weeks, so would really appreciate any recommended vendors.
Thanks!",Discussion,dataengineering,s31sjh
735,"Recently, I've been more involved with data architecture at my job . I am mostly designing things based on my experience but I haven't really been following any people or blogs about data architecture yet. I wonder if you could suggest some valuable resources to me.",Discussion,dataengineering,s0pu76
736,"e.g. Top Medium authors, blogs, newsletters, etc.",Discussion,dataengineering,s3x6ab
737,"I currently govern an environment that houses data in a few different tools: MySQL, Redshift, and S3. I'm trying to generate a data dictionary that will make navigating and documenting these systems easier. This tool would ideally allow me to define objects and columns, and link objects where appropriate. 
What tools are folks using to house data dictionaries? My fallback is creating Confluence pages with database diagrams, but would ideally like something more robust _EMOJI_",Discussion,dataengineering,sd8kuf
738,Any ideas on how to approach while designing data systems? Are there any good resources which talk about data pipeline or framework designs?,Discussion,dataengineering,sbzk4x
739,"I'm working on a monolithic PySpark project and have now an opportunity to refactor and redesign things a bit so was wondering what's your approach for code organization and composition, do you introduce any additional abstractions for your data transformers?
It seems like this topic in the context of pure data-oriented projects is quite poor... There is a ton of books and talks around software design but it often seems not applicable when building regular data transformers. 
I started to dig into some potential functional programming approaches and I came across **polylith** but also no success with data context examples there.
Do you prefer an OOP/class-based approach or FP?",Discussion,dataengineering,s2i4ho
740,"Modern Analytics platform covers both Business Intelligence  and Predictive Analytics . The Data Architect of yesteryears are not knowledgeable on AI/ML stuff, whereas Data Engineering being a new and more of a hands-on role, it is capable of serving both of these deliverables.
If we agree that the head of an Analytics platform should be from Data Engineering background, than what should be the title of that role: Senior or Lead or Principal Data Engineer?",Discussion,dataengineering,s9t66m
741,"My company is in the process of moving their Consumer Data Platform from Microsoft Azure to Snowflake. The CDP when I was hired a little bit less than a year ago was basically utilized for one off excel pulls with very limited comprehension of the data for the business. I was hired on as an analyst to do what analyst do - run analysis on the data, showcase use cases for the data and help the business utilize the data. Much of my job is pulling together ad hoc insights on our customers and making it digestible for our business partners. The tool of choice in the company is Spotfire which I don't care much for considering the tool is slow and as an analyst with several years experience in developing Power BI dashboards from the ground up - I have to basically ask IT to make any minor change and then wait for them to publish and then if it's not correct I have to come back and repeat this cycle. I am the only analyst in the company who was able to get their hands on Power BI license and this took a lot of political leverage from my manager and leadership. When I first joined the company I had a lofty task of rewriting majority of the queries that were initially shared with me because of very slow execute time of these outputs  and because the data wasn't in a format that was necessarily beneficial to my analysis. 
With the Power BI Pro license and the fact that the CDP as it stands right now all being MSFT products - the usability from an analyst perspective is a dream, I'm able to set up direct connection from my PBI reports to the tables of interest in the format I need it in . 
I have a pretty good relationship with one of the lead data engineers and he has told me that I'll be able to get the data in the same format in Snowflake as my hard coded SSMS tables but with a view in snowflake. I am incredibly weary to go back to using views as I'm afraid I'm going to hit walls with load execute times. I feel very left in the dark with what the hell this data engineering team is doing in snowflake and how it'll compare to the azure platform as there are different data connection pipelines being laid down. 
To make things even more complicated - I have a background in data engineering from a previous role where I was writing SSIS ETL scripts as well as doing pretty intensive DQ checks. Right now I'm sitting under the marketing org and I worry that this IT team looks at me like an incompetent dumbass . From a data engineering perspective how can I best communicate my concerns with this team and ask for more insight into their workstreams without looking like a jackass? This team is incredibly understaffed and dealing with a ton of tech debt - so I'm trying my best to be empathetic and not a bother but I'm worried the work their doing is going to cause me and my team a huge headache. 
Side note - this role has been vacant for over _NUMBER_ years prior to me joining. Prior to myself joining the role was occupied by a data scientist who ended up leaving the company because he was finding himself doing more data quality work than actual modeling. My manager is NOT technical and has no technical background (apart from some Tableau report building some years back and some data modeling but with very small amounts of data -  ). 
I have a lot of hope and a vision for what this CDP can mean to the company so I want to make sure I'm airing my concerns/desires in a way that doesn't come off as a wagging my finger.",Discussion,dataengineering,se19h9
742,"“Data lineage is like a family tree but for data”
_URL_
Data lineage is a technology that retraces the relationships between data assets. In the data world, you start by collecting raw data from various sources and refine this data by applying various transformations.
Building a table for a particular use case, or a “child table” requires using other data tables called “parent tables”. The data lineage helps you retrace which parent tables have been used to build child tables. It basically helps you rebuild the family tree of your data.
In reality this is what data lineage looks like 
_URL_
New interfaces for data lineage tend to be simpler and address one use case per visualization in order to make it easier to understand for business experts. ‍Here’s an example of data lineage in Castor _URL_.
_URL_
Data lineage is a technology, not a product. It is one of the underlying technologies behind a lot of data products .
The reason data lineage is so popular is that there are a lot of new use-cases, both for business, engineering, leadership, and legal department.
 
The most common use cases for data lineage are the following: 
\- Data Troubleshooting 
\- Impact analysis 
\- Discovery and Trust 
\- Definition Propagation‍ 
\- Data Privacy Regulation  
\- Data assets clean up or technology migration
More on each use-case here _URL_",Discussion,dataengineering,s34bvk
743,"So I'm studying for the GCP PDE exam and don't have a huge amount of hands on DE experience. I finished the _NUMBER_ part Coursera course that's mostly aimed at preparing a DE for working within the GCP ecosystem. It's been helpful but I've been going through question banks/practice tests and realized I'm missing a fair bit of the hands on knowledge about migrating from legacy setups and how to properly troubleshoot things when standing up a new pipeline/tool.
I know a lot of this comes down to hours spent on the job but do any of you have recommendations on resources that were helpful for things like going from Hadoop to Spark workflows, updating legacy SQL code, when to push or pull new data or use a publish/subscribe model etc. Basically looking for resources on data architecture and common troubleshooting.
I'm currently a few chapters into Designing Data Intensive Applications and it's been great at helping me build a mental model of how some of these tools work but so far it seems a bit general. Also this sub has been great too for exposing me to new topics and y'all are great!
**TLDR:** Looking for resources on data architecture, troubleshooting and hands on stuff. GCP specific would be nice but AWS or open source works too. Halp plz!",Discussion,dataengineering,s7zsi0
744,"this seems a topic wich gets discussed the least yet it is a big issue in our company right now. We already have an warehouse solution SAP BW on HANA and have a BI tool Tableau. We are now discussing how to create info cubes.
Now the IT wants precise requirements but we know SAP mostly from the UI and know little or can only guess the underlying data structure. We want to cluster and specify our requirements to be able to discuss this topic with the IT.
Now to my question. Are there any methods to collect/cluster requirements for data solutions? How do you deal with requirements? Do you use any tools? Is there any literature to this topic?
I’ve been going through some books and most just mention that requirements have to be collected and business context understood but don’t mention any way how to tackle this task.",Discussion,dataengineering,shc7jn
745,"Hey, I'm getting into the data/AI space from a history predominantly in Linux and writing application software. I'm really looking to create a new project, and I want that to be in the data/AI space and runnable in kubernetes. I've been looking through the Linux Foundation data/AI landscape and I'm not sure where there is a gap in tooling. 
My instinct is something around data ingest, even moreso around consuming social media data or something. However I don't have a network of contacts yet to know if that's useful, or not. I wouldn't mind of the tool was in data storage, or processing, or filtering, I just want it to have a need and a community. 
Not looking to make any money - I'm a FOSS advocate and enjoy writing Golang on Kubernetes. Purely looking for a project to build an interesting tool in my spare time.",Discussion,dataengineering,sde1mm
746,"I’m in a team where our pipelines are reading data from Athena. Potentially lots of records. Are there any concerns of performance of reading from Athena vs RDBMS? I’ve never personally seen a use case of Athena for reading from it. Only used as a way to query the data and run some analytics against it. 
I’ve seen some limitations on Athena as well which can pose as a issue. Is there anything else?",Discussion,dataengineering,s79d7f
747,"Hello Peeps,
I am trained as a data engineer in my company. I have been an offered a role of the snowflake developer for a project should I take a decision to work in that or wait for another one?
**Trained On:**
Azure, ETL basics, Scala, Python, Data bricks, Spark, SQL, Linux and currently training on AWS.
FYI: Its not based on cloud.
Thanks for answering_EMOJI_",Discussion,dataengineering,s0p65j
748,"What are some competencies you would expect of a mid-level Data Engineer, and what are some good stretch goals?",Discussion,dataengineering,sbwsyf
749,"Hi all
I'm curious as to how other companies collaborate and maintain that collaboration organically. I know there are Community of Practices and I have heard of guilds, but I've mostly experienced these from a high level / business value perspective, rather than less informal, more engaged experience that is driven from the bottom-up.
Just curious as to whether there are ways of bridging barriers across business areas that I haven't seen or thought of!
I did find Bloomberg's article on their internal guilds very interesting: _URL_ _URL_",Discussion,dataengineering,s6tmax
750,"Recently started working in a computer vision/robotics company as ML ops. We're building some in house tracking tools -- dvc has become a bottleneck and wandb is unreasonably expensive. 
Part of the plan is to have a django API servicing dataset retrieval/creation, model-reuse, pipeline introspection, etc. This was actually going well, until I started drafting up the.... models.
My Django models now reference my research teams models, which in turn depend on various other data models , and its starting to drive me nuts.
Someone on my team casually suggested namespacing ML stuff as ""AIModel"" or ""MLMode"" but I still hate how this looks:
 class AIModel:
 model_name = CharField
...
Anyone have any tips on naming stuff in this day and age?",Discussion,dataengineering,s0wpv9
751,"I spend a lot of my time reading log files, and I’m realizing that there is probably a lot of room for creativity. It’s the primary UI for my python jobs. Does anyone have a favorite resource for how to pick what formats and information should be included in your logging statements?
One of my favorite patterns was to always end a job with either of the _NUMBER_ lines below:
$timestamp|END:SUCCESS
or
$timestamp|END:FAILED",Discussion,dataengineering,s9ky4n
752,"Any chance we could use a different format for the salary discussion threads? Maybe like a Google Form with results presented in Google Sheets so that we can filter on location?
I'm based in the UK and find I'm scrolling quite a bit before finding posts from fellow UK engineers.",Discussion,dataengineering,s80dd7
753,"Modeling question from a n00b:
My org has customers and often the business questions are around already-aggregated facts about each customer. For example: ""How many customers have _NUMBER_-_NUMBER_ transactions, _NUMBER_-_NUMBER_ transactions, etc"". I think to get these answers I create factless fact tables, but for the purpose of data exploration  I'd like to roll up a count of those transactions by customer and store it as an attribute of the customer. 
I'm sure there is a different way to think about this so any direction is appreciated!",Discussion,dataengineering,se3e16
754,"I have a couple of databrick clusters which I want to assign only read access to specific users, so they can just get read the event and driver logs. 
When I look through the permissions, the lowest permission is ""can attach to"" which still allows the user to create notebooks. 
Is there a solution to give user just read access?",Discussion,dataengineering,s7uheh
755,"We have built 180Protocol, an open-source toolkit for data sharing. It targets enterprise use cases and improves the value and mobility of sensitive business data.
Our alpha release is live on GitHub _URL_ Developers can quickly build distributed applications that allow data providers and consumers to securely aggregate and exchange confidential data. Developers can easily utilize confidential computing  to compute data aggregations from providers. Input/Output data structures can also be easily configured. When sharing data, providers get rewarded fairly for their contributions and consumers get unique data outputs.
Read more on our Wiki _URL_",Discussion,dataengineering,s9d1q2
756,I am a ds and I see the differences mentioned everywhere when it comes to data management. I am having trouble understanding why this is the case if anyone care sharing some insights. Thank you!,Discussion,dataengineering,sfme7l
757,"Curious which you would choose and why.
View Poll _URL_",Discussion,dataengineering,s9ghhk
758,"I'm curious how various companies measure ROI for their data platforms. You could attribute indirect value from each new use case enabled through the data products, but there's additional costs outside of DL/DW that would need to be accounted for . Do you assign an arbitrary % to each use case enabled or ignore the additional costs from those downstream teams that use the data? Costs are easier to track but I'm struggling with how to measure the business value across the current user base.",Discussion,dataengineering,s704lo
759,Trying go from Postgres to snowflake environment. Not a historical load. It will be incremental daily. I wilL NOT have access to download raw Postgres files. I will only have access to move the postgres data using the tables there. Unsure size of data as of now. Best approach for this?,Discussion,dataengineering,s7riow
760,"As a data engineer, I'm pretty happy with my salary and technology used in my current position. I probably make slightly less than software engineers at my level in my company. I enjoy using python and SQL, and also being familiar with a wide range of AWS services. I'm pretty interested in learning devops skills too, and I feel like data engineering has been great teaching me this.
I noticed that most senior engineering management/CTO's/tech entrepreneurs have software engineering backgrounds, as well as maybe an MBA. Is this a result of data engineering being such a new field, where very senior data engineers have yet to progress to those types of roles?
How do you imagine data engineering career progression to be like?",Discussion,dataengineering,sa5ugk
761,"So I had a interview recently with Afterpay which I totally bombed. I was given a transaction table of a buy now pay later company . The loan process can go like : approved, authorized, charged, refund, etc :
Example of schema :
merchant , 
Customer , 
amount, 
loan_status, 
date. 
 
 
 
I am curious to how you would design such a table for olap systems ? Some questions that need to be answered :
How many customers rejected , approved ? 
How quickly a loan is paid? 
How many checkouts were initiated with Afterpay? 
Monthly cash flow of given loans? 
I am a total noob so I went with : just join all the tables . Wondering if there was better response ? How would you do data modeling ? The company uses ELT with cloud warehouse.",Discussion,dataengineering,sgup4d
762,"Hello everyone, as a Data Engineering consultant I don't always have the luxury that a Data Engineering environment is setup already, so I have to do that first. I find it sometimes quite a process to make a choice where and how to deploy each tool in the best way. 
Some cloud providers are making this choice a bit easier by providing those tools out of the box, but this is paired with somewhat lower customization and higher usage costs. So it is still a tradeoff. 
A trend that I am seeing is that a lot of tools can nowadays be deployed on Kubernetes, which is a container orchestrator and supports automatically scaling resources up and down based on application load. I see the benefit of using it for data engineering applications, since a lot of my data pipelines run for a relatively short time , and thus it is not required to keep a machine running the rest of the day.
That's why I started digging deeper into deployment of tools such as Airflow and Spark on Kubernetes. Learning everything about Kubernetes can be a bit daunting, especially if you're not used to working in operations it the past. So I was glad that there are ways to make the deployment to Kubernetes easier with the use of 'Helm', which helps define, install, and upgrade Kubernetes applications. 
There were already a number of Helm Charts available to deploy Airflow on Kubernetes, such as the Bitnami one](_URL_ and a (_URL_ These Helm Charts are definitions of Kubernetes resources which can be used to easily install applications on the Kubernetes cluster. Recently Apache Airflow released [an own 'official' Helm Chart _URL_ as well, with lots of functionality that can be enabled with minimal configuration. With this introduction it feels like deploying the tool on Kubernetes is a great option that has a great future ahead.
What are your thoughts? 
If you worked with Airflow on Kubernetes already, what is your experience with it? 
Would you prefer it over other ways of deployment?
 
PS. I also definitely see the potential of hosting Spark on Kubernetes for many of the same reasons. So I thought it would be interesting to share this upcoming virtual meetup with you . Two engineers from one of the largest retail companies in the Netherlands, HEMA, will explain how they setup Airflow and Spark on Kubernetes. They tried tools like DBT and Lambda functions for data processing, but decided to make the move to an easily scalable, low-cost PaaS EKS environment. Might be nice to have discussion with them about it in the virtual meeting room. 
February _NUMBER_ _NUMBER_:_NUMBER_ CET / _NUMBER_:00am EST  
_URL_ _URL_",Discussion,dataengineering,se3yul
763,"So I've noticed that AutoML is reducing the need for Data Scientists that build models. Shoving data into models can be done by anywho who knows SQL  and AutoML is MASSIVELY out-performing teams of data scientists. 
Do you think data science is overhyped right now? Do you think there's a need to build a train models when AutoML can do it better and cheaper?
See: _URL_ _URL_",Discussion,dataengineering,s7aj4r
764,"Hello :) 
I would be interested to learn more about the history of distributed computing/data engineering. 
I've seen that reading/watching about how people did things before helps me better understand why we do things today in a certain manner today. 
So, any good blogs/videos/books about the history of data engineering/distributed computing/databases/etc.? 
Thanks in advance!",Discussion,dataengineering,s8sxg9
765,"Hi, what is best way to store and manage data to identify outliers such as security breaches, network outages or machine failures in real time. In financial services, companies can respond to potential fraudulent sign-in attempts or credit card transactions by joining a real-time activity stream with historic account usage data in real-time.",Discussion,dataengineering,sd54mi
766,"Hi! How do you organize access to multiple csv, xml, xlsx files via SQL? Is there an integration solution that provides an automated scan of files on shared endpoints such as files on Google Doc or Dropbox, scan them and add them into a relational database? E.g. postgres. So I could work with the data later on via a single access point and the data remains in sync all the time?",Discussion,dataengineering,sc7zfo
767,"It seems like to do so, you would need a really expensive development/test environment. Maybe it is just extra hard for us because we have protected health data, and you can't just replicate that in non prod systems?
I just want to know does anyone do this? If so I have some other questions. Thanks.",Discussion,dataengineering,scjfnb
768,Hello there. Any news you have heard if Azure will add Airflow as paas or managed service? Or opinions why they won’t...,Discussion,dataengineering,sbrc7a
769,I'm curious what tools are used to create canned reports  that can be run in batch mode  on a set schedule . Maybe cognos? Any others?,Discussion,dataengineering,s7925h
770,"This is a DBT question, but possibly has broader application. I have one main DBT tag that refreshes _NUMBER_ or so tables every 24h. Those tables are used downstream for a lot of use cases. Now I have a request to refresh _NUMBER_ of those tables every 1h for a new application. I think I have the following options to support this and I'm trying to understand which one makes the most sense:
_NUMBER_. Overlap both schedules: This means I'd be refreshing every 24h for all _NUMBER_ tables, but also every 1h for _NUMBER_ of those tables. That way I don't have to replicate data, but some tables will be ahead of others and have fresher data. I'm not sure how that will affect the analytics downstream.
_NUMBER_. Replicate the tables and create a new tag: This would create two separate flows, one that refreshes _NUMBER_ tables every 24h and another that refreshes _NUMBER_ tables every 1h. This is cleaner from a scheduling standpoint, but it leads to replication. I'm not concerned about costs but more with the codebase complexity. And the more schedules I need to support the more table copies I'll have to create.
Is that something others have faced before? Please, help me think through what option makes the most sense.
Thanks.",Discussion,dataengineering,s9bfnu
771,Curious!,Discussion,dataengineering,shmkaq
772,"I'm a BI developer that turned Data Engineer. I'm now in a Senior Data Engineer role and I know a big blocker in my career is that I do not know Python. Six months ago, I left an organization after being there for five years where we didn't use Python at all in the shop. It was a large retail company and it was all corporate IT .
My new company is a startup that is moving out of the startup phase, they're in super growth mode, and I'm helping build their data ecosystem from the ground up. I can see that we can possibly benefit from Python in our stack and also I think for going forward in my career, it's a critical skill to have in my toolkit.
What I'm wondering is if you all took any DE-focused Python classes or had any recommendations outside of just the normal suggestions for learning Python that are over on /r/Python. For me, I learn best when there's a practical application to the course.",Discussion,dataengineering,sh4u69
773,When someone asks you what you do for work what is your response? I feel like most people don’t know what a data engineer is.,Discussion,dataengineering,s85nc8
774,"Hello everyone!
As you know, to achive exactly once processing in Apache Kafka we have to create two topics: one to store producer's system information like change logs/offsets and another one for data. In order to write a piece of data you have to open transaction and make two writes
 kafka_transaction
When a producer restarts it must re-read system topic to recover state to continue data write. 
This way leads to x2 writes, x2 topics and tons of complexity during producer start-up. If you have distributed systems and _NUMBER_+ producers and _NUMBER_+ topics, the Apache Kafka usage turns into pain.
Is there any messaging system that natively supports exactly once sematics like part of API ? The solution with two tropics looks unnatural and ugly. 
For example, system log/information can be placed into write api:
 write
aslo during producer's initialization the log item can be returned in producer's API
 log = init_producer
There is no two topics, no need to re-read system log. Simple and easy",Discussion,dataengineering,sgf04p
775,"Will Snowflake Scripting reduce the use of writing javascript store procedures?
_URL_ _URL_",Discussion,dataengineering,s93fki
776,"Hi guys, 
Looking for your advice / opinion: 
With so many ML/DS YouTube channel , what thing you feel is missing ? or What kind of YouTube channel in this niche you will like to see . 
Thanks for your answers .",Discussion,dataengineering,s32tqp
777,"I’m exploring data solutions for my workplace. I wonder what people here use.
Currently I’m using
SQL Server for database
ADF for data pipelines and orchestration
Metabase for data visualisation and reporting
These are put in place before I joined. I’m not too happy with ADF. I think it’s inflexible and solutions we have in place are mostly quick hacks. We’re also lacking a proper solution for data lineage, data testing,...",Discussion,dataengineering,s4cdew
778,"I've used Informatica, Control-M and Wherescape for different requirements. What is your preferred or frequently used data orchestration tool and why?
I prefer Wherescape as it has various pros and some cons which I have learned to adpapt. Best pros is metadata versioning, automated code generation and deployment agility. Also, seen some new promising tools such as dbt, Astera DW Builder and others.",Discussion,dataengineering,sdnhws
779,"I specialize in data migrations using SQL Server to Salesforce and customers are more frequently needing data warehousing strategies then integrating it within Salesforce afterward. 
I have no experience with cloud computing and how it works, but I’d love to learn about it. 
Where is the best place to get started in learning how I can use AWS as it relates to data warehousing and the like?
Sorry if this seems like too loaded of a question, but I truly don’t know where to start.",Discussion,dataengineering,sajl94
780,"Hi All,
Let's say I am having a data of _NUMBER_ TB residing in AWS S3. I have do a simple transformation and aggregation. I am very new to EMR/ Spark. I am having difficulty choosing EMR cluster size and Spark Configuration. 
I know it is very vague and depends upon use case. But atleast to start off, what would be the best cluster size and spark config so as to not incur more cost or get OOM issues.
I am trying this config. Please let me know if this is correct
_NUMBER_ master node instance - m5.2x large _NUMBER_ VCPU and _NUMBER_ GB and _NUMBER_ GB EBS 
_NUMBER_ core nodes instance - m5.2x large _NUMBER_ VCPU and _NUMBER_ GB RAM _NUMBER_ GB Storage
""spark.dynamicAllocation.enabled"":""false"",
""spark.driver.memory"":""10g"", ""spark.executor.memory"":""20g"", ""spark.executor.cores"":""_NUMBER_"", ""spark.driver.cores"":""_NUMBER_"", ""spark.executor.instances"":""_NUMBER_"", ""spark.yarn.executor.memoryOverhead"":""4g"", ""spark.default.parallelism"":""_NUMBER_""",Discussion,dataengineering,sb1vqc
781,"On episode _NUMBER_ of the Data Engineering Podcast](_URL_ they talk about [Anomalo _URL_ which looks a very interesting commercial tool to test data using statistical anomaly detection - they have a short and sweet video demo on the landing page.
Think getting an alert when in the new batch of data loaded in the warehouse there is an abnormal amount of Null values.
Has anybody here used this? What about comparable solutions? Anything open source?",Discussion,dataengineering,s74re4
782,"I recently created an automated ETL pipeline on Amazon Web Services  with SNS email notifications using S3, Glue crawler and ETL job, Lambda and EventBridge. Here is a link to the Medium article
 link _URL_
When a file is inserted into the S3 bucket, it triggers a Lambda functions that starts the Glue crawler. When a crawler is done, it creates an event in the EventsBridge that triggers another Lambda that starts the Glue ETL job. When the ETL job is done, it creates another event in the events bridge that triggers another Lambda that activates the SNS to send out an email notification
I have a concern
_NUMBER_. Is there a way to send the bucket name and folder details to the crawler using Lambda so that it does not go through the entire bucket but instead crawls that specific folder only
Thanks",Discussion,dataengineering,s3j3h4
783,Hello! I am in a part time CS program and I am really interested in data stack and upstream data management. But after five courses into my master of CS program I still find much of data engineering concept and task incomprehensible. Is there a particular class like database or Hadoop that are most useful for my purpose?,Discussion,dataengineering,sfmamr
784,"Edit: I'm sober now and thought about deleting this but the comments have been interesting so I'll keep it live for a while.
Databricks poses as the ""open source"" solution to the big data platform challenges of modern data-driven businesses. It certainly has contributed A LOT to OSS. MLFlow, DeltaLake, Spark, Koalas.
But the sham of the supposed 'open' lakehouse architecture is quite subtle. Does it operate on an open source format? Yes...kind of... the format is parquet, but the benefits of the 'lakehouse' really come with the delta log. Now is Delta open source? Yes....kind of.... It certainly is open source, but is most useful, and only reasonably used in production when coupled with Spark. ""But I can read parquet with dozens of clients that aren't spark"" I hear you say. Yep, you can. But reading the parquet files from delta lake won't do you any good beyond what you could have done just with vanilla object storage. The entire lakehouse promise is void in that scenario.
The above highlights some of the inconveniences, but isn't really an indictment of the lakehouse. Where the company's messaging/branding and capabilities really fail is they've tried to put on the bold face of being the ""Data + AI"" company. A company that no longer wants to be just the ""Spark people"".
So how does the lakehouse paradigm support AI in an open and extensible way? Well... it doesn't. Say you have a delta table in your s3 bucket that was written there by a databricks cluster as part of an ETL process. Maybe its done some overwrites and has run multiple times. Say you're looking over a state of the art research paper that mentions they trained their network on two A100 Nvidia GPUs. You go and spin up a EC2 instance with that same backing. You fire up your vscode.
Okay now you need the data....wait but how do you get it?...Well its in delta lake...open source format...hmm but you need a delta compatible reader... Wait but you can just read the parquet? But then why did you bother with a lakehouse then? Okay well...what if you just used a local pyspark session to get the data? Great! But you'll need spark, and hadoop, and any additional jars to be able to connect with the storage. Ugh. Okay, well you schlepp your way through all of that. Now finally, time to get the data. What...java heap out of memory? Well time to go twiddle with some spark configuration settings, always fun. Hmm, looks like it works for a small subset of data, but is this scalable? Is this going to work for large training jobs ? Ick. Maybe...maybe if you feed it enough RAM. But standalone spark single spark will never reallyyy scale. What about the other delta readers, delta-rs and the like? Well...same problem for scalability .... Hmm so how can you get the data out of delta lake and into your EC2 instance to train your AI model? Aha! Just ignore vscode and your EC2 instance and use the databricks notebook, with the databricks runtime, in the databricks ui instead!...wait...is that open?...Should you not be able to get your data out to any workspace for ML training?...Hmm.....still no Data for my AI....",Discussion,dataengineering,s56xwg
785,"I have two csv files. 
* F1: will always be less than _NUMBER_ MB
* F2: would go above _NUMBER_ GB. 
To read these two files, I can use either Pandas or Dask module. Pandas with chunking is showing faster reading time as comparison to pandas without chunking. 
Which module should I use to read these two files among the two? 
My current decision is to use pandas with chunking for F1 and dask for F2. Please enlighten me if there's a better option or why should I change my decision etc. 
I will use the two dataframes to do sql query and find a result using aggregation. Thanks.",Discussion,dataengineering,s2u8ij
786,"Are there any drawbacks to doing this to update data pipelines? 
 # fetch latest_sync from local table
 
 insert into dest_table 
 (select * from external_query);
Run this every hour or so. Its incremental so it shouldn't be too taxing. Of course it doesn't account for schema changes but neither do ""official"" CDC solutions like debezium. Why would you use something like debezium when you can just do this instead? 
I know federated queries are not always available. And they aren't as ""real time"" as debezium. But if they are available and if you don't need it to be real time?",Discussion,dataengineering,sbtry1
787,"For people entering the DE world and trying to suss out what would make a job a good learning opportunity vs. what might be a dead-end  - what are the green and red flags? Are they related to:
\- The tech stack?
\- The size of the company?
\- The age of the company?
\- The size of the team?
\- The manager's expertise?
\- The type of company?
\- Whether they have more of one type of role than they have DEs?
\- The job description?
\- The interview process?
\- Terminology used during interviews?
\- Etc.!
",Discussion,dataengineering,sc159z
788,"I've been with Google for just over _NUMBER_ years now, and I've seen that Google seems to consider Data Engineers equally skilled as software engineers. Algorithms and data structures, systems design, memory management, designing distributed systems, etc. Other companies like Meta/Amazon seem to believe that data engineering involves using no-code/low-code tools to build data pipelines.
Do you think Data Engineering is going to align closer to software engineering, or closer to specialized BIEs? Will Analytics Engineer become more popular?
My experience at Google is such that it's actually more difficult to find roles at other companies, and I thought Google would be a great resume builder. I share my experiences here _URL_",Discussion,dataengineering,sghj5q
789,"I’m not sure if this is a good idea but its something I plan to implement in the upcoming project.
The problem I’m trying to solve is to create a mechanism to be able to test our pyspark pipelines for any possible regression.
We use pyspark to build our ETL pipelines. A pipeline isually involves _NUMBER_-_NUMBER_ hive tables and ends up creating one hive dataset as the output.
My plan is,
When I define my datasets, I will have an abstract class defined that needs to be inherited while defining any dataset . One of the abstract functions of such a class would be to use a function called _data_sample. I will need to define a json with one sample row’s data within the class itself!
Now all my _NUMBER_ data sources  have sample data within them. I might as well create a JSON with the expected output  and save it somewhere.
When the pipeline needs to be tested, the code, instead of running spark.table to read data, will read the mock JSONs and proceed with transformations. The end output shall be asserted against the golden copy we had created earlier. This process and be automated and called CI!
What do you guys think?",Discussion,dataengineering,sban6s
790,"why / why not in the comments
View Poll _URL_",Discussion,dataengineering,sdb5y9
791,"My team needs to copy files from S3 to GCS on an hourly basis . The AWS account belongs to a partner company which is not very cooperative, so any solution we come up with has to run on our GCP project. 
We got the AWS credentials pair and are able to list directories and read files  and messages .  But they don't include the **s3:GetBucketLocation** permission which is required by Google Cloud's Transfer Service, and the other company refuses to grant us that permission for ""security reasons"". Without it, the service errors out with a *""Failed to obtain the location of the Google Cloud Storage  bucket pipeline-validation due to insufficient permissions. Please verify that the necessary permissions have been granted.""* error message.
What's the best option left?",Discussion,dataengineering,sh9v4g
792,"Hi all I am a DS interested in data engineering and data ops. One thing I am having trouble understanding well: I am curious to understand say from an ml perspective, what is a world without databricks or Apache spark like vs. with it like? What benefit does it bring to make lives of ml scientist or analytic professions easier? I read a lot of materials and still am confused.",Discussion,dataengineering,sdeakb
793,"So if something breaks, and some data is unavailable to people, they notice of course. They want it back up again quickly to start doing their jobs again. 
But it takes time to fix. So how much time is acceptable wherever you work? An hour? A day? A week? Whats the expectation?",Discussion,dataengineering,s33m26
794,"I normally say ""I enable Data Science / Analytics at scale"" which I think is quite succinct. If they're interested or if I have longer, I'll normally make an analogy with using Excel for data work and how that doesn't scale in the enterprise world and/or when you have large data volumes. 
What're yours?",Discussion,dataengineering,sbq7wd
795,"Has anyone had experience working at Macaw as a Data Engineer, or with Macaw. It is a small / medium sized consultancy company in Holland.
Just want to know if the company would be recommended to work for.",Discussion,dataengineering,sbocda
796,"In simple words, what is the difference between Data Architect and Data Engineer",Discussion,dataengineering,s72ssg
797,"I have two years of experience as a data engineer at a startup. As it's a startupit's involved wearing multiple hats for analyics, bi stuff, and deing. It's involved building pipelines, tracking user flows, developing KPIs, communicating results to stakeholders, etc.
I've been offered to join Meta as DE in their Seattle offices at an IC3 @ 124k pre tax; and an additional 25k in stocks every year + a _NUMBER_-_NUMBER_% bonus depending on performance. I'm wondering if I should try pushing the base salary to 150k? Everything on glassdoor and payscale seems to suggest I should be able tomoreso as the recruiter told me Meta typically pays the 90th percentile or higher of what a DE makes in an area. I just don't want to come across as too greedy, as all in allthis is double what I'm making currently and risk having the offer rescinded.
Edit- I've not been allocated to a specific team.",Discussion,dataengineering,sa5k9x
798,"If you can't think for beginners, what generally do data engineers contribute to in open source for good experience and learning",Discussion,dataengineering,s8j0jq
799,"Introduce this open-source project under Apache License _NUMBER_, hope it's useful and interesting to someone.
_URL_ _URL_
This project connects to streaming data from streaming engines like Kafka or cloud storage like AWS S3 and gives you a built-in UI to analyze data into graphics. It's easy to run either in Kubernetes or hand-crafted clusters with VMs. 
If interested, check out more details on the project page...",Discussion,dataengineering,s2dzos
800,"I have a use case where we have streaming data coming in and I want to aggregate them on _NUMBER_ min, _NUMBER_ mins.
Now what I want to do is stream the data in Kafka and through kafka streams I will do the aggregation on that topic based on the key of that topic. Now I will do this in a window of _NUMBER_ minute and _NUMBER_ minutes, push the aggregated data to Cassandra using Kafka Connect. 
I am somewhat new to the Kafka world, but I feel this use case will give me speed and scale well and if the table designs in Cassandra are fine the end user will hopefully be happy haha 
And I think late data arrival will be a problem so any suggestions on how to handle that? We are fine with missing late data as the place where we will read from its unlikely that the data will be late but I guess there should be a fail check.",Discussion,dataengineering,s9856a
801,"Hi Y'all,
I'm interested in hearing the community's opinion on some Pandas coding convention.
My problem:
I have to make new columns based on other columns. 
The rules are basically the same so I created one function that does this. That's simple.
However the new columns need new names that replace the previous suffix. 
For example given:
df
I'll want to name the new column:
df
Simple enough, but I have 30ish columns which all would look like:
df = funct
Except the names are slightly different.
So in theory I can write a function that takes the name of the string and returns it with the ending being changed. 
Which means I can loop instead of declaring each column.
My intuition is saying don't do this.
Anyone have a good alternative or real reason why I wouldn't do this?",Discussion,dataengineering,s1ubuu
802,"Hi there! I am working on a personal project to get some experience with Apache Airflow. I am wondering which of the options in the title is better for my use case. Here are the details:
I am creating a workflow with just a few steps yet to:  retrieve weather data for several towns via API calls to a server in a 'for' loop and, once downloaded, each JSON file is saved in the file system. The next step  is to get each file, extract some data to normalize and save this output as CSV files again in the file system. The tasks are based on pythonoperator and the number of towns is supposed to be dynamic in the future.
Is it better to have **ONE task with a for loop** or to create **DYNAMIC number of tasks ** to do that? This question if for both the 1st and 2nd step since the work similarly.
Thanks for you answer and suggestions!
Edit: the set up is in a local PC.",Discussion,dataengineering,s6twbv
803,"Like the title says, I've got json I'm pulling from an API. I read that json into databricks and convert to parquet. They I append that to our parquet reporting tables. 
 
However, there's a conditional array with a bool. If every entry for that batch is null, then databricks is unable to infer what type is ought to be a defaults to string. This breaks our downstream appending of the single batches to the larger dataset. 
 
I've found lots of suggestions for turning everything into a string, and that works, but I'd prefer to 'pickle' a schema object from a 'happy path' sample of the json and apply it to all incoming batches. Is that a thing?",Discussion,dataengineering,s285hw
804,"Does my question make sense? I personally build them for the internal business 
OR do you create pipeslines for the software product itself, take spotify for instance",Discussion,dataengineering,s89u76
805,"I am mainly familiar with GCP for data engineering. I am trying to learn AWS for data engineering also. To do so I am trying to implement a basic pipeline which I have implemented in GCP.
My current implementation in GCP is following:
* Files are being uploaded to GCS  in a bucket.
* Each upload triggers an event and publishes a message in Pub/Sub Topic
* There is a Google Dataflow  Job running which is monitoring the above Topic
* Once a new message is read, Dataflow parses the messages and get's the file's bucket location
* Dataflow reads the file and processes the data in it
* Finally it writes the processed data into BigQuery
So it looks like this:
GCS ----> Pub/Sub ----> Dataflow -----> BigQuery
Here is what I could think of based on my some knowledge of AWS:
Replace as follows:
GCS => S3 + Lambdas : Upload a file to S3, trigger lambdas to publish message to MSK Kafka Service
Pub/Sub => MSK : Upload events are stored here
Dataflow => EMR : Have a Spark Job Running which processing the messages in MSK
BigQuery => Redshift : Lands data here for warehouse purpose
I'm still exploring different services in AWS. GCP seems more cleaner approach, can this be done in much cleaner way?
How can one implement the same pipeline in AWS? What different services from AWS can be used? Is there is a similar framework like Apache Beam for AWS?",Discussion,dataengineering,sfcuy2
806,"For example, when you need to check or visualize parsed/scraped content stored as csv, xml or json?",Discussion,dataengineering,s4i4a3
807,"Theres a lot of hype about Data Mesh nowadays. Some people have gone to lengths saying that data warehouse should be replaced with data mesh. 
I dont get it. Data mesh is just a way of doing exactly the same thing using data products. No ? Or am i wrong ?
I dont see how data mesh can replace data warehouse. Can anyone please advice.",Discussion,dataengineering,s61xft
808,"What is the most economical way to store and index event data on the order of millions or billions of events  per day? The use-case is very write-heavy in that reads will be rare  but need to execute in less than _NUMBER_ milliseconds. I also need TTL or similar functionality to handle retention .
I've looked at DynamoDB and Bigtable but they seem expensive. Presto/Trino/Athena and similar seem too slow. RocksDB-Cloud looks interesting, but hard to judge what the costs would be and how hard it would be to maintain. What else is out there?",Discussion,dataengineering,s5krox
809,"Context: I will have a data product that will need to primarily get its data from more than one external 3rd party companies or data ""suppliers"". We will essentially sell the data at a higher price than what was paid for from our data supplier. I think logistically or technically what needs to happen is we would call the data supplier's API then ""wrap"" it to a different, custom response, and make that available to our customers.
I am completely new to this as Im used to having owned or have physical access to the data and on top of that, mostly relational data is what Im all used to. Now, its all REST API driven, JSON data. 
 
Curious if others had to do this also? How was this done? Pitfalls? Lessons learned? It seems like not actually owning the data or having actual physical access to the data is a major hurdle I would think. Also thinking response time would be a concern or consideration. I would imagine depending on what needs to happen in that custom response or what all the ""wrapping"" entails in our API, things may be unacceptably slow.",Discussion,dataengineering,sbsd77
810,"For people out there running Spark on _NUMBER_ c5.2xlarge instances on EMR or Databricks -- why not just use _NUMBER_ c5.8xlarge instance? Everything will just be so much faster. Is there some benefit of using many small instances?
Edit: I see a lot of comments about finer-grain scaling benefits. This is very true. Are there any other benefits?",Discussion,dataengineering,sdqfib
811,"What would be the advantages and disadvantages of each of these tools? I'm using SageMaker training jobs for the actual training of the model, but am not sure how to choose the way in which I perform the data pre processing.
Cheers!",Discussion,dataengineering,sh3ocz
812,"Title. It's easy to find on the internet how to structure your web dev project, or your data science project but I couldn't find any for data engineering project, can anyone share how you do it?",Discussion,dataengineering,s9bv8v
813,I haven’t had the best time reading/writing to and from S3 using Airbyte  and I was wondering if anyone else has had any experiences or recommendations with this combo.,Discussion,dataengineering,sh8dkd
814,"Hi All,
Im newbie in AWS. I have worked on EMR only in dev. Im curious to know, how Amazon EMR instances are created in production and performed big data on it.
Thanks",Discussion,dataengineering,scl3z7
815,"By Hadoop I mean cloudera. Sorry for the error. 
Both started as free open source, one ended in disaster while the other, alone with snowflake, is hot commodity. Will that last? What are the cautionary tales?",Discussion,dataengineering,sg1zhg
816,"Silly q. But how to really comprehend Apache Spark, what is the benefit of products like Data Lakehouse/Data Lake provided by companies like Databricks and Snowflake.
My understanding is Spark is a distributed computing framework, but it does not manage the machines it uses for distributed operations. It needs a cluster manager  to orchestrate the creation and scaling of infrastructure resources. Kubernetes is a popular cluster manager which accomplishes this. So Spark drives Kubes.
Is this understanding correct? It seems to be that Sparks or Databricks and Snowflake's services, which are expensive, are just doing what Kubes are doing already, if that is the case why are people still using them? What are the main difference between a data lakehouse and a data lake, offered by Databricks and Snowflake. And in the end, how beneficial are they? Can someone help me understand this also your data is still stored in the cloud like AWS or Azure, and how do you query from spark to make it faster if Spark doesn’t own the storage. Sorry if this is a dumb q",Discussion,dataengineering,sfqnl1
817,"Beside books and tutorials I'm trying to find some resources that contains end-to-end sample project with explanation so I can visualize what a common full pipeline looks like. After searching for a bit I came across this website _URL_ _URL_ it advertised to have full end-to-end data engineering project with detail tutorial explaining the step. However payment for it is like single option of _NUMBER_$ and it looks pretty sketchy
Do you know any resources that contain a good amount of end-to-end project with detail explanation ?",Discussion,dataengineering,sds8fs
818,"Hi all,
does somebody has experience with Data Vault _NUMBER_ ?
If so, are there best practices for implementing a “Housekeeping Concept“?
Example for Raw Data Vault:
Data is allowed to be kept for _NUMBER_ months.
Business Keys will be loaded into Data Vault Model only when they first appear.
Let‘s say a partition older than _NUMBER_ months is deleted and some business keys are therefore removed from hub tables - how do you deal with Data/records in the satellite tables regarding those deleted business keys, which aren‘t older than _NUMBER_ months and the inconsistency resulting from that?",Discussion,dataengineering,s6vjhn
819,"I am pretty noob in this field. I see sentdex talking and working on his own data. He talks about selling it as well. 
I am scratching my head. How does an individual generate data and where do they sell it?
 I read a few articles about institutions/ company selling data with all the insights and blah blah. But I am curious to know how can one/single individual do it?
If this question is answered before, can you please point to it.
thanx.",Discussion,dataengineering,san4ah
820,I often use CTEs in my SQL scripts that need to be executed across multiple steps. What would an elegant way to handle data transformation operations in Python/Pandas that require multiple steps? Is it simply just defining a bunch of functions and executing them in order?,Discussion,dataengineering,sft6j6
821,"Hello Redditors
So this isn't a job post or technical issue, more like need your creativity to come up with nifi interview task that would be not hard and not easy, about a pipeline from start to finish as my workplace is looking for data engineers with good nifi experience, I am not the one doing the interviews, I am simply helping my colleagues in their interviews
Let us see what you guys can come up with",Discussion,dataengineering,sbqln9
822,I have read a few articles where the authors state that dimensional modeling is dying and does not provide a clear alternative strategy. If dimensional modeling is becoming obsolete then what are the best alternatives?,Discussion,dataengineering,shp8xp
823,"My company has been doing this and it doesn’t seem right. To my knowledge, there’s no real way to work with data that lands as a string, especially if it’s very complex nested JSON. When it lands as a struct, you are able to query it in Databricks with JSONcolumnname.key. You could keep adding periods and keys after to continue to go deeper into the nested JSON.
I’d like to get peoples’ opinions on this topic and whether or not there is a way that I’m not aware of.",Discussion,dataengineering,sd8ai1
824,"Been in a data engineering role for _NUMBER_ months now. My company is trying to make a point to help pay for courses, certificates, workshops, meetups, etc for personal skill development. 
Wanting to take advantage of this I thought I would reach out and see if anyone feels passionate about what helped them learn. 
Current Tech Stack: 
- Snowflake
- Azure
- Matillion 
Current Skills: 
- Python
- SQL 
I have seen the certification exams for Snowflake and Azure Data Engineering so definitely considering those. 
Areas I think I lack in: 
- Data Streaming
- Kafka
- Spark
Any recommendations on education material that you did through your work is appreciated!",Discussion,dataengineering,s82vgj
825,"Hey guys! Recently, I have installed and configured airflow with a VM instance on GCP. As we know we need to execute the command 'airflow scheduler' to make the scheduler run. Is it possible to automate the execution of this command every _NUMBER_ minutes using cloud function and cloud scheduler without starting or stopping the VM instance?
Thanks for any assistance.",Discussion,dataengineering,s60ybg
826,"Curious which you prefer and why...
View Poll _URL_",Discussion,dataengineering,s9on5e
827,"Was limited in options...
View Poll _URL_",Discussion,dataengineering,sbk1ui
828,"I want to ditch crontab for something better, and am trying to find something that meets these requirements:
* A GUI to view/edit jobs and view logs 
* Automatic logging of all jobs including start time, end time, exit code, stdout, stderr, etc. This should include jobs in progress.
* Free and open source
* Easy git version control on all jobs
* Email or push notifications on job failures
* Should work on both Linux and Windows
* Create job flows as DAGs
* Job level user permissions for viewing/editing
The options I'm looking at are Airflow, Dagster, Prefect and Luigi. I've browsed their docs and demos but it's hard to tell... can anyone say which one fufills the most of these features? And are there any good alternatives that I'm overlooking? Some others that I've ruled out are Argo , Kubeflow , MLFlow .
I am asking on this subreddit because a lot of these tools are marketed for ETL workflows, but really I want to replace crontab even for scheduling jobs unrelated to data because most of these features are still very important for building a reliable system.
My team is currently relying on crontab + an assortment of hacks trying to fill these missing features like cronic for emailing and logging by suffixing every line with things like `... >> /var/log/myjob.log _NUMBER_>&_NUMBER_` or building logging into the scripts themselves which becomes hard to maintain and standardize.",Discussion,dataengineering,s78jvx
829,"We are currently planning on creating a Big Data Analytics infrastructure for our company. Since we will be mostly working on Azure, our teamleader proposed us to work with Azure Databricks. Since Azure Synapse Analytics is also an option, I would like to ask if anyone got experience with both and can list some pros and cons. I'm still unsure which one would be the right fit for us. What's your opinion?",Discussion,dataengineering,s9ya6f
830,"Currently our Python scripts for EL part are running as CRON jobs at EC2. For the transformation part, we are executing stored procedures via Python scripts as well. 
Currently it's maintainable but for the future, it probably won't be.
For the orchestration I'm thinking of using Airflow in AWS .
And for transformations I would like to start using DBT.
We are using Snowflake as our warehouse.
What do you think? Are there any better options? What would you do? Thanks!",Discussion,dataengineering,s341m4
831,"Hey guys, I'm looking for small things that most business owners can do today to improve the quality of their data.
One example would be to replace ""free form"" fields from their CRM to ""closed-ended"" fields as much as possible.
Any other examples like this one?",Discussion,dataengineering,seg1j1
832,"Hello,
We are creating a s3 data lake internally at a SaaS company. Want to name it something that represents its purpose - where the org can find data they need to query, analyze etc.
We obviously do not want to call it data lake or put any product name as it will be used across org. Currently scouting for names, I want to propose 'Pacific'  to reflect its deep , big and is fed from multiple sources
Want to take some input on how others name data lakes internal to the org.
TIA",Discussion,dataengineering,s07nev
833,"I wanted to post that I just did the datastax cassandra certification  but I don't think it warrants its own thread. I was happy with it and wanted to share with the community
Anyway, yeah I put in a good _NUMBER_-_NUMBER_ weeks in preparing. I had a bit of Cassandra experience but I wanted to learn it enough so that I can put it on my resume. The datastax site is great and all of the contributors are really cool so thanks guys",Discussion,dataengineering,s0ryvu
834,"I have trained a SpaCy NER model which can identify Name, Address, Institute, Degree, Skill , Company, Designation, School, Society and Location in a Resume. Now I want to structure recognized entities in such a way that CV owners name, address, skills and other details are together & Referees name , address, Designations separately. Is there a way to do it? I mean I want to have CV owners data together and Referees data together.",,LanguageTechnology,sgxqg3
835,"Hello ,
I've developed after my PhD a first version of an algorithm to automatically generate a literature review : _URL_ _URL_ and many remarks were given. I just deployed a new version with much more papers and I'll be thankful if you have any remarks about it :)
More about the new version here : _URL_ _URL_
Hopefully that could be useful for the PhDs  !
Cheers,",,LanguageTechnology,s3whz6
836,"Does anyone know how Twitter generates their “Topics”?
It seems like they could be machine generated ? It would be a lot of labor simply brainstorming a huge ontology of trending concepts in the Twittersphere.
They must have some algorithms for analysing and clustering tweet topics.
And possibly even for automatically suggesting the name of the cluster .
Anyone have any guesses how they do it?
Thanks very much",,LanguageTechnology,s103c7
837,"I'm  starting a master's in Computational Linguistics in the fall, and I'm curious to know what people who have done an undergrad/master/phd in cl, nlp, or even just compsci with an nlp focus end up actually doing after their degrees.",,LanguageTechnology,rn9kbo
838,"Hi all, I've just published another article focusing on fine-tuning a retriever model for open-domain question-answering _URL_ The retriever is a big component of the open-domain QA pipeline, allowing us to retrieve relevant *contexts* from a vector database, which then help us answer a query 
Let me know if you have any questions or feedback, thanks!",,LanguageTechnology,s26r9f
839,"Text-to-Speech  synthesis is achieved using current voice cloning methods for a new voice. They do not, however, manipulate the expressiveness of synthesized sounds. The task of learning to synthesize the speech of an unseen speaker with the least amount of training is known as voice cloning.
UC San Diego researchers propose a Controllable voice cloning method that offers fine-grained control over many style features of synthetic speech for an unseen speaker. The voice synthesis model is explicitly conditioned on a speaker encoding, pitch contour, and latent style tokens during training. ***Continue Reading*** _URL_
Paper: _URL_",,LanguageTechnology,s0qfes
840,"I've never used scatter plots in my life, so I'm trying to wrap my head around it but I'm having problems visualising what I want to do and I don't know if it makes sense.
I used LIWC to analyse positive and negative emotions in a number of texts over the years, and I would like to show how the score changed. Would it make sense to use a scatter plot? 
Besides, I also have different sources of text that I would like to compare . Would it be too much to put in one scatter plot?",,LanguageTechnology,rz9cri
841,"I've been Wikipedia-diving some of the history of NLP recently, and I'd like to know if anyone has any interesting stories about researchers/experiments in the field. You know, like when your high school history teacher goes on a random tangent about all the different torture methods throughout the ages. Thanks!",,LanguageTechnology,ro6v4v
842,"Hi everyone!
Are there any corpora or treebanks that are labeled with Penn Treebank-style constituency trees ?
I'm investigating the usage of a certain syntactic construction in English. I'm using the PTB constituency trees, but it would be nice to have as much data as possible. I found the Georgetown University Multilayer Corpus , and I'm wondering if there are others.
Thanks in advance!",,LanguageTechnology,shq4rt
843,"Hello all,
I'm trying to get smarter on mimicing writing style based on sample input text. The hope is a system like this:
**Inputs:**
* Input tagged sample writing/letters/emails/dialogue from desired author.
* Basic sentence to be rewrite .
**Output:**
* Translated sentence written in sample author's writing style.
I'm assuming this is a bit of an ambitious lift and may require some training on my own. Curious if anyone has any insights on stylometry papers written. Even something on generative text that is just meant to replicate an author's style could be a helpful starting point.
Thanks!",,LanguageTechnology,s7a0rx
844,"I'm using the Stanford NLP to help with a personal project. Last week I couldn't tell you what a Verb or Adverb was to be quite honest. I've been feeding my C# program titles/comments from stock investing subs to get the ""Sentiment"". Someone posted:
To r/ALL: _NUMBER_ Year ago, the most unprecedented move in the history of the stock market happened. \ 
The Buy Button was turned off for a specific stock. \ 
_NUMBER_ year later and there have been NO CONSEQUENCES. \ 
No one went to jail. \ 
Was there even a fine? \ 
Why? \ 
How is the answer not going to sound like a conspiracy \
I understand the algorithm is evaluating each sentence. How would one go about determining the full context? Clearly this example has a neutral/negative assumption. 
Further more, bias... so if I have a left or right leaning comment to which I could tell the program i'm in favour of L/R then the algorithm would deem that as a postitive to my bias.",,LanguageTechnology,s392e0
845,"I am trying to understand the masking in BERT model.
I have confusion in following line taken from paper
>The training data generator chooses _NUMBER_% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with  the \ token _NUMBER_% of the time  a random token _NUMBER_% of the time  the unchanged i-th token _NUMBER_% of the time
at point _NUMBER_ it say unchanged token  _NUMBER_% time. If we have to use original token _NUMBER_% of _NUMBER_% tokens, then why we need to mask it.
This can be more clear in **Attempt _NUMBER_: Masked LM with Random Words and Unmasked Words** section of this guide _URL_
The guide say
>So if we have a sequence of length _NUMBER_, we will mask _NUMBER_ tokens, and in those _NUMBER_ tokens, _NUMBER_ tokens would be replaced by random words, and _NUMBER_ tokens  will be used as it is.
So if we have to use _NUMBER_ tokens as it is, then why we masked them first?",,LanguageTechnology,ses9wt
846,I’m just wondering if anybody knows of any good Farsi  > English translation models? I’ve tried a few of the multilingual ones from Huggingface but the quality isn’t the best,,LanguageTechnology,s41hmv
847,I plotted MDS plot for word embeddings obtained from BERT. The \ token is plotted on the middle of the figure and other word embeddings are scattered . Is it supposed to be in middle? Is there any significance to it? MDS plot was plotted on the basis of pairwise cosine similarity.,,LanguageTechnology,s51i8l
848,"Hello, I'm a postgraduate student and I have a NLP project that I have to come up with and do . What are some really interesting ideas that you could recommend? 
An example of an interesting and good project is: _URL_ _URL_ .",,LanguageTechnology,scfo4t
849,"I’m trying to learn how to segment text into significant clauses.
Here’s a promising approach:
_URL_
chunks = 
for sent in doc.sents:
 heads = 
What are the children of a sentence’s root? Does that mean every possible lowest level syntactic element like “D”, “Quantifier”, “N”, etc.?
So the author decided to find conjunctions?
What about just looking at the syntax tree and breaking it on a lateral level - like the three elements on one level down from the root, make those the segments?
Or what about just pure machine learning for this? Just train a custom segmenter by showing it where you would break sentences, and don’t do any explicit syntax parsing?
 for head in heads:
 words = 
 for word in words:
 seen.add
 chunk = (' '.join)
 chunks.append(  )
 unseen = 
 chunk = ' '.join
 chunks.append(  )
chunks = sorted
for ii, chunk in chunks:
 print
Is this just going to the break-points in the sentence the program identified and pulling out all the words consecutively? Or, what is this doing?
Thank you",,LanguageTechnology,rs22ru
850,"It seems like things like HuggingFace and Spacy and whatever have done some harm to NLP as a whole.
for instance, I've heard NLP engineers have less pay potential compared to computer vision folk due to most models just being run through their pipelines.
also, it seems difficult to find tutorials post _NUMBER_ on topics like NER and such from scratch.
Everything is getting abstracted to API's and fewer people are learning things from the ground up.
What do you think?",,LanguageTechnology,rsrwqa
851,"With recent breakthroughs in AI, humans have become more reliant on AI to address real-world problems. This makes humans’ ability to learn and act on knowledge just as essential as a computer’s. Humans learn and gather information through learning and experience to understand everything from their immediate surroundings. The ability to comprehend and solve issues, and separate facts from absurdities, increases as the knowledge base grows. However, such knowledge is lacking in AI systems, restricting their ability to adapt to atypical problem data.
Previous studies show that pre-trained language models improve performance on various natural language interpretation and generating tasks.
A recent work of researchers at Baidu, in collaboration with Peng Cheng Laboratory , release PCL-BAIDU Wenxin , a pre-training language model with _NUMBER_ billion parameters. It is the world’s first knowledge-enhanced multi-hundred billion parameter model and its largest Chinese singleton model. 
You can read the short summary here: _URL_ _URL_ 
Paper: _URL_",,LanguageTechnology,rrgrnf
852,Has anyone here used Snorkel AI's LabelModel for automatically labeling text? Have you found it to be super slow?,,LanguageTechnology,s7vype
853,"What would be the steps involved in doing such a thing? Basically I have data from a research, 
 the form of a dataframe including: participant ID, response ID, single word object, and response, which consists of a description of the function of the single word object. Some paricipants have had more than one response from the same object. Object is the same in the whole dataset, and category codings have been done by someone else. Basically responses need to be categorized into these coded categories, denoting similar responses. I want to construct embeddings of the responses and feed them into a CNN. How can this be done quickest? I don't have much knowledge and all the information online is very overwhelming.
Basically, I want to use the bert uncased model, , but I guess I do need to average the world embeddings. Also, how do I tokenize the whole column of responses, and how to add the object into the mix since it is needed as the response and object are connected. Don't expect someone to give me a tutorial in the comments, but a list of general steps to take in this context would be incredibly helpful. You will save my life and my graduation.",,LanguageTechnology,s0p3yw
854,"I currently have a list of sentence fragments that loosely describe listing for sales for houses/apt/mansions etc. 
They might look something like this:
*\*
*\*
*\*
I want to apply labels  to these fragments to ""standardized"" the language which I can then use to process later. Knowing to group the following is important:
""large pool"" --> has swimming pool""
""garden with swim area"" --> ""has swimming pool""
The ""keywords"" I might want to use for the examples:
_NUMBER_. \ ---> \
_NUMBER_. \---> \
_NUMBER_. \---> \
I do not need to ""capture"" all the descriptions from the sentence fragments. And at least, I want to be able to grab the lowest hanging fruit first 
I see that I have some issues:
_NUMBER_. How do I break down these ""sentence fragments""? So that analysis can be done?
_NUMBER_. How can I ""group"" text that shows up so that I know what categories I want to create? Even better, if groupings can be automatically created/suggested
_NUMBER_. Even if I have ""labels"" that I want to assign a set of fragments how do I train a model to actually do this? (Like if I spent _NUMBER_ hours  labeling some very basic categories.... how do I use this?)
One possible wrinkle I have, is that I do not care which ""sentence fragment"" correspond to which label.  - therefore it is difficult for me to map a ""sentence fragment DIRECTLY to a group with heuristics"" . In the end, I do not necessarily care  which of the sentence fragments actually correspond to the label, just that this example should have the given labels.
I hope my problem description makes sense, and looking for any type of directed help/ approaches. I have looked at ""tokenization"", ""word count"", ""bag of words"" etc but I am unable to understand it enough to see the full picture of how to use it.
Any comments appreciated!
\",,LanguageTechnology,rqe7js
855,"Hi there, 
I've recently graduated with a BA in Linguistics and I'm currently pursuing a career in Computational Linguistics. I plan on applying to an Msc in a CompLing related degree in a year or two, but I'm currently taking some time off to relax and also learn Python Coding and polish my math skills. 
However, learning Python from scratch and also learning it independently has been really difficult as I find myself stuck often with nobody that I could talk to about Python, and also I find myself lacking the motivation to keep going. 
It would be really nice and helpful if I had a few people I can go to regarding Python-related things. We could motivate each / help each other out etc. 
Please let me know if you're interested!",,LanguageTechnology,rpmr79
856,"For example, let's say I have the first sense of the verb ""go"" and the first sense of the verb ""walk"". I want to see the connection between these two words. In other words, if I start with go#_NUMBER_ , how can I arrive at walk#_NUMBER_ ?",,LanguageTechnology,sc2u7d
857,"In the last few decades, neural networks have been used for a wide range of tasks, including image segmentation, natural language processing, and time-series forecasting. 
One promising use of deep neural networks is embedding, a method for representing discrete variables as continuous vectors. An embedding is a low-dimensional space into which high-dimensional vectors can be translated, making it easy for computers to understand the relationships between those concepts. Numerically similar embeddings are also semantically identical. Word embeddings for machine translation and entity embeddings for categorical data are two applications of this approach. **Continue Reading** _URL_
Paper: _URL_
Documentation: _URL_",,LanguageTechnology,sdbq8w
858,"QQP is a dataset of duplicate and non-duplicate question pairs from Quora. I think it was originally developed as part of a Kaggle competition:
_URL_
The competition released a training set with labels, and a test set without labels. QQP has subsequently been used in many papers developing new architectures for document similarity and duplicate detection, but unfortunately, as far as I can tell, there is no standard train/dev/test split of the dataset *that has labels for each of train, dev, and test*, and therefore people make up their own splits on the Kaggle training set. Am I mistaken? Is there a widely agreed on train/dev/split of this dataset with labels for each split?",,LanguageTechnology,sbpmnj
859,"Hello fellow enthusiasts, 
I have a corpus of 150k documents, and their respective OCR outputs. 
I'd like to assign a Readability score to each document, is there a metric out there for something like that? 
In retrospect to my OCR extraction, which took almost a month of runtime to run, I *could* have extracted an OCR-accuracy score along with my strings. I'd like to find an alternative solution instead of re-running it. Knowledge for next time, anyways...
I'm open to all thoughts and considerations.",,LanguageTechnology,s7r2pj
860,"Hello fellow researchers!
Do you read a lot of Scientific Papers?
Have you ever wondered what are the overarching themes in the papers that you've read and how all the papers are semantically connected to one another?
Look no further!
Leverage the power of NLP Topic Modeling, Semantic Similarity, and Network analysis to study the themes and semantic relations within a corpus of research papers. Just \`pip install stripnet\`
_EMOJI_ Generate the STriP Network on your own collection of research papers with just three lines of code!
_EMOJI_ Interactive plots to quickly identify research themes and most important papers
_EMOJI_ This is only the initial release, with lots of work planned.
_EMOJI_ Github: _URL_ _URL_
 
_EMOJI_ If you get the chance to play around, please share your feedback. Please leave a ⭐ to let me know that STriP Net has been helpful to you so that I can dedicate more of my time working on it.",,LanguageTechnology,rxbn4x
861,"Hi all, I'm working on a project to build a set of language models for the Maldivian language of Dhivehi. It's a lot of fun and super interesting, the first step  has been building a tokenizer that handles the language and its unique Thaana script. I just published a video](_URL_ and (_URL_ ([link if you hit paywall _URL_ explaining the steps and each of the components in a tokenizer .
I hope some of you find it useful, lmk what you think - thanks!",,LanguageTechnology,rqitqe
862,"Has work been done on selecting a minimum subset of relation types?
Ideally it could be reduced to just one. It would probably be one of the first words that that children learn. Something like ""is"" or ""has"".
Having just one type of relation would greatly simplify the representation.
""Is"" could represent categories ""cat is animal""
""Has"" could represent parts ""cat has tongue""
So what I'm thinking is that ""has"" would be a prime candidate as a single sufficient relation type, because categories and subcategories could be determined easily without any relation between entities: if one entity  has a subset of relations that another entity  has, then it means that ""cat is animal"".
To take the other route and use only ""is""  to infer parts from it -- I don't know how it could be easily done.
Anyway, perhaps it is possible to use any _NUMBER_ relation and infer all others based on that, the question is which is more natural for language as we commonly use it.",,LanguageTechnology,rpuft7
863,"Does anybody know of a leading AI model for glossary creation?
I’m considering using Spacy for this but so far I found their entity recognition and even their segmentation to be good but not necessarily flawless.
I could stick it custom trained models for sure, it honestly might not be that hard.
I’m wondering if anybody has gone before me here, though.
An auto-glossary creation tool at minimum should:
_NUMBER_. Recognise terms, not necessarily entities. Entities appear to be more trivial, like even just years and numbers come up sometimes. Terms are important keywords.
_NUMBER_. Retrieve context/example sentences from the source documents for each word.. AI is not strictly necessary for this, but it could be leveraged in deciding which sentence containing a term is most “representative”. Plus, AI would come in handy for lemma-matching - it should be able to search for any grammatical form of a word in source text, and not match “crudely” as in maybe a homonym of a word.
_NUMBER_. Ideally, it should auto-categorize terms . 
So: this is the project I’m currently working on. Has anybody already done something like this, ready to go?
Thank you",,LanguageTechnology,rqdq05
864,"Hello Geeks, I am trying to solve the problem of dangling modifiers and have not been able to think of any solutions or a better way to put this would be from where to start to solve the problem. If you people have any solution or any pointers which you could share it will be really helpful.
Dangling Modifier Examples -
_NUMBER_. Orig - Fumbling in her purse, the keys could not be found.
_NUMBER_. Modified - Fumbling in her purse, she could not find the keys.
_NUMBER_. Orig - Having injured his dominant hand, it was difficult to write the exam.
_NUMBER_. Modified - Having injured his dominant hand, John had difficulty writing the exam.
Thank you.",,LanguageTechnology,sc54sq
865,view of liwc dictionary _URL_,,LanguageTechnology,sbkae4
866,"Has anyone worked there as a linguist, ML engineer, etc.? What is the environment/ culture  like?",,LanguageTechnology,s7mosh
867,"Hey Everyone,
I'm building a plugin for end-to-end neural search  in Opensearch and I'd love to hear any suggestions from the NLP community of what you might find useful or about issues you've had with Opensearch/neural search in the past.
Despite the Elasticsearch website claiming in many places that you can do ""machine learning"" with Elasticsearch, I've found that it's not straight forward at all to use neural search algos with ES/Opensearch. In most cases , you have to implement the ML algorithm yourself and you only get to use ES as a storage layer. Some 3rd party plug and play frameworks that support ES seem quite promising but also lack functionality in terms of data retrieval - for example, Cherche seems to enforce that users first use tf-idf to retrieve documents before putting them through neural search.
I want to build a plugin/service that will allow users to more readily take advantage of the vector functionality and neural search in general. I've found the following issues:
_NUMBER_. Opensearch/ES have added a great deal in terms of functionality to allow for vector search , but it seems entirely up to the user to encode the word embeddings. Therefore, users must add code to manually encode any documents and queries into their chosen embeddings before searching/adding data. I think users having their embeddings is generally a good idea if they want a high level of optimisation, but for many use cases, pretrained embeddings should be a ""good enough"" solution.
_NUMBER_. If the data is in text format, it cannot easily be converted into a format to be used with algorithms like SBERT etc without reindexing the entire index and running it through a custom script to change the data into a vector format.
_NUMBER_. I'd suspect for many users who arn't NLP experts, navigating all of the potential options for embeddings/Neural Search architecture could be quite overwhelming. Having a configurable plugin where they can try different options would likely help them to accelerate getting started.
I think letting users have their own embeddings makes sense from an optimisation perspective but I think also it would be amazing to have an end to end solution where you can connect different algos directly into Opensearch. I'm also exploring extending this and allowing users to refresh/update these embeddings to continually improve them.
Let me know what you think, open to any suggestions! If you want to keep up to date with this, here is a google form _URL_ _URL_",,LanguageTechnology,s5ethj
868,"hi r/LanguageTechnology,
I'm working on a text classification problem, where I want to classify the textual data into different domains/categories.
We have tried couple of different approaches, like Topic Modelling and BERT. Topic Modelling didn't give out the expected results and in the case of BERT the accuracy were not at the desired level.
What are the other methodology which I can look into for this particular task?
Are there any ways in which we can improve the accuracy with on these models?",,LanguageTechnology,rx9rmp
869,"So, say I'm attempting to label a training data set for a sentence classification model. What would the best tool to load a bunch of documents, have each document be split into sentences, and then show me each sentence so I can label it myself. Any ideas on what I should use?",,LanguageTechnology,rrn0jg
870,"Large-scale language model scaling has resulted in considerable quality gains in natural language understanding , generation , and multilingual neural machine translation . One typical method for creating a more extensive model is to increase the depth  and breadth , essentially expanding the network’s existing dimensions. Such dense models take an input sequence  and route each token through the whole network, activating every layer and parameter. While these big, dense models have shown cutting-edge outcomes on various natural language processing  applications, their training costs rise linearly with model size.
Building sparsely activated models based on a mixture of experts  , where each token supplied to the network follows a distinct subnetwork by bypassing some of the model parameters, is an alternative and more common technique. Small router networks that are educated with the rest decide how to distribute input tokens to each subnetwork . This enables researchers to increase the model size  without increasing training costs proportionally. ***Continue Reading*** _URL_
Paper: _URL_ _URL_",,LanguageTechnology,s9sjqz
871,"Hi all, 
So I've been trying to download the wikipedia split that is used for evaluating Dense Passage Retrieval  _URL_ 
I just pulled up a google colaboratory session and followed their instructions to download the dataset as shown below.
`python data/download_data.py \`
 `--resource  \`
 ``
I don't know why, but for some reason, the colab automatically exits the download cell , and all I get is a .tmp file.
I believe I should be getting a .tsv file as instructed here  :
_URL_ _URL_
I am attaching the .ipynb file here too for convenience.
_URL_ _URL_
Anyone know what I'm doing wrong?
Thanks.",,LanguageTechnology,sa9n1f
872,"Hey there! I'm trying to build an HMM tagger from scratch in Python, but I'm not so sure of how to go about it. Do you know of any good resources or guides that could be useful to me?",,LanguageTechnology,rzs4ah
873,"Hi, I am trying to get into NLP with knowledge in CV. In CV, tasks like object detection have SOTA ensemble methods like Weighted Box Fusion. I was wondering if NER has the equivalent to WBF in terms of ensembling Transformer models of different folds.",,LanguageTechnology,rxiklt
874,"Apologies if this is really simple, I just want to double check. If I have a contextualized representation of a sentence, say ""Look at this cat."", and I want to extract the contextualized token representation of ""cat"", how would I go about doing this?
I was thinking of first extracting the word ID with `id = tokenizer.convert_tokens_to_ids(tokenizer.tokenize)`, then match the word ID to its token position in the sentence with `tok_index = input_ids.index`. Later, after passing the sentence through the encoder, I can use tok\_index to access the specific embedding. I'm wondering if this idea makes sense. Thanks in advance!",,LanguageTechnology,s2ufj6
875,"Hi, I have a project in mind and the first ""mini-project"" within it is to assign a Score to a text depending on the depth of the vocabulary. Similar to what Grammarly does. I know I have to use a dictionary, but beyond that I don't have much.
 A bonus would be to also assign a ""Class"" to the text depending on the vocabulary used; ex: While a Scientist and a Writer might have very similar ""depth"" Scores, their vocabularies are not the same, the program should assign to which ""Class"" does the text belong. But this might be a bit hard.",,LanguageTechnology,rs5rqj
876,"Hi,
I’m part of an art group from Switzerland currently studying at HSLU Design & Arts (_URL_ _URL_
The group consists of:
Karim Beji (_URL_ [_URL_ _URL_
Emanuel Bohnenblust (_URL_ _URL_
Lea Karabash (_URL_ _URL_
Yen Shih-hsuan (_URL_ [_URL_ _URL_
At the moment, we are working on a project on the topic if AI can augment the happiness of humans. To answer this question, we are mainly working with chatbots. The end result is going to be an exhibition at the end of March. 
For that exhibition, we want to conduct a trial in which people from over the world chat with a chatbot to find out if and how it augments the mood of the participants. 
We would give you access to a GPT-_NUMBER_  chatbot and ask you to a) record yourself through a webcam  while you are chatting and b) simultaneously screen record the chat window. 
In the exhibition we would have a) a book with all the chats and b) small videos with your faces  to assess your mood. 
We would have a Zoom meeting beforehand to discuss everything.
Looking forward to your message!",,LanguageTechnology,sgypv1
877,"Dear all, 
If there are any NLP/ML engineers, DS, or researchers out there, I could really use some advice. 
I am graduating from my MS in Economics with a full-time job lined job as a DS at a well-known fintech company. However, it is driving me crazy to find a clear path forward to pursue a more NLP-involved job down the line. 
Here is what I currently have that can be classified as NLP ""experiences"": 
_NUMBER_. Past Internships! I have done anything from Product management intern for data products powered by NLP to Management Consultant doing research on the data collection strategies that a client could take to improve their NLP classification outcome 
_NUMBER_. Research! I am writing a paper with researchers from NLP for applying NLP techniques to public policy related documents and is due to publish in the next couple of months 
_NUMBER_. Current job! The team that I am currently on and hired into  uses a lot of NLP for insights discovery. We also plan on launching a large scale NLP product down the line which I will be very involved in given our very lean corporate structure 
Why I think I will have a hard time advancing in the field: 
_NUMBER_. I do not have a CS undergrad or MS in CS 
_NUMBER_. My background in economics dictated that I am good at math but not at linguistics 
_NUMBER_. I do not come from a hyper prestigious school like Stanford or MIT but a mid-tier school in the East Coast 
I feel everyone in the field is so overqualified for what they are doing ! I have no clue what to do ??? 
Should I go get an MSCS to compete down the line? How does moving up in NLP careers work? Can any folks shine some light on a very confused young person! 
I will literally take any suggestions or advice haha. thank u y'all!",,LanguageTechnology,rp96xk
878,Hi everyone! I am a BA graduate with background in linguistics and self-taught programmer. I have been looking at two programs in language technology Msc. in Speech and Language Processing in Edinburgh (_URL_ and Human Language Technology ([_URL_ _URL_ in the States and I was wondering I anyone has any advice in which would be a better fit? I am hoping to start working in the industry after graduating  but I am worried about which one would help me be more prepared. The one in Edinburgh is a one-year program while the one in AZ is a two-year program. Would appreciate any help with your viewpoints on this :),,LanguageTechnology,s1wvwl
879,"Example: Qual ity -> quality
I'm using pytesseract to transcribe pdfs, and unfortunately one of the issues is PDF often splits up words at the end of column in two parts . I'm trying to figure out a way to detect when words don't make sense separately but make a normal word combined ",,LanguageTechnology,s0l796
880,"This feels intentional... no?
Also: _URL_ _URL_
Found some others that are sorely lacking too—what a joke.",,LanguageTechnology,rynylf
881,"Hello,
I am trying to do some basic preprocessing on _NUMBER_.5GB of text. More specifically, I want to do tokenization, lower casing, remove stop words and top-k words. I need to use spacy because the dataset is in greek and I think other libraries can't support this.
However, when I try to apply what the spacy documentation or most of the guides/resources mention, it takes forever to complete even half of the techniques that I mentioned above. I stop the execution every time.
Could you provide me with some resources that I might have missed, in order to make this procedure run faster?
Thanks in advance",,LanguageTechnology,rxtn0v
882,"Hello everyone! 
Did you hear about word embeddings? 
Nooo? Then you need to learn about it. 
But if seriously, I'm in process of create some corpus for word embedding for romanian language . 
That why I decide to start this project. 
What are the goals I am pursuing? 
_NUMBER_. The text must be clean; 
_NUMBER_. To be learned on a lot of text; 
_NUMBER_. And the corpus must be accurate. 
Here you can see some of them:
_URL_ _URL_ 
The rest will appear in the near future, and I will try to do them as soon as possible. Maybe some changes and fixes will appear, but I'll keep you posted. 
And of course you can leave a comment, what you like or dislike. I will be very grateful. 
Respectfully",,LanguageTechnology,rphw8k
883,"I am trying to implement word2vec for large corpus may be in billions of words if possible. I am following Word2vec tensorflow _URL_ as a reference, where they have used a vocab size of _NUMBER_ and sequence length of _NUMBER_. My question is, if I use other corpus with billions of words should I limit the vocab size to some numbers like _NUMBER_ and sequence length or create vocab for all the unique words present in the corpus ?
I want to know how did gensim and other library trained their model on large corpus, did they limit the size of vocabulary or trained on all the unique words present in the corpus ?",,LanguageTechnology,s6qaog
884,"I did a search in this sub for similar posts, and have gathered that the two best beginner resources in general, as recommended by this sub, would be the NTLK docs, _URL_ and the Jurafsky/Martin book [_URL_ _URL_ .
However, as an outsider to this field, I fear I'll start going through all of this learning material and exercises that are based around processing the English language, and then not have much to apply when it comes to working with the Chinese language.
I'd be really appreciative for:
_NUMBER_. Bloggers, resources, youtube videos that do a decent job of how to build up knowledge of this domain from _NUMBER_, but specifically with NLP of Chinese in mind.
_NUMBER_. If this doesn't exisit, then if someone would be so kind as to tell me which chapters/sections of the above resources  would **not** be relevant for doing NLP of Chinese?",,LanguageTechnology,sexrvx
885,"So I'm working on some early English text. For example, sometimes ""up"" is spelled ""vp"", or ""himself"" might be ""himselfe""... or it might not. Is there any advice or good practice for how to handle stemming/lemmata etc.? Has anyone got experience doing word embeddings with this kind of data?",,LanguageTechnology,rwihy9
886,"Hey Im searching the web for a document with following criteria for a nlp model 
_NUMBER_+ pages 
contain :
knowledge
rules 
instructions
\--- no speculation unclear content inside like research papers 
\------ mostly text no relevant pictures or formulas 
I would greatly appropriate any help and tips",,LanguageTechnology,s6c319
887,For example a project similar to scanning for cyberbullying comments but one that has been done already,,LanguageTechnology,s47pz3
888,"Hi everybody,
I've become interested in NLP and would like to get started on preparing for some master's courses. My background is modern languages, and I've been teaching myself Python alongside my internship. I'd love to work in machine translation, or even build my own machine translation engine. What advice do you all have for a clueless boi on getting started in NLP? Thank you!",,LanguageTechnology,s99cke
889,"Recent advancements in end-to-end deep learning models have enabled new and intriguing Text-to-Speech  use-cases with excellent natural-sounding outcomes. However, the majority of these models are trained on large datasets recorded with a single speaker in a professional setting. Expanding solutions to numerous languages and speakers is not viable for everyone in this situation. It is more challenging for low-resource languages not often studied by mainstream research.
Coqui’s team has designed ‘YourTTS](_URL_ to overcome these limits and provide zero-shot TTS to low-resource languages. It can synthesize voices in various languages and drastically reduce data requirements by transferring information between the training set. [***Continue Reading*** _URL_
Paper: _URL_
Github: _URL_",,LanguageTechnology,rz8epu
890,"Hi all, I put together an article on applying AugSBERT _URL_ from Thakur, Reimers, etc for domain transfer tasks. Great for improving sentence transformer performance in a domain where we don't have data, but we *do have* data in another similar domain. I hope it's useful, let me know if you have any questions, ideas, etc - thanks!",,LanguageTechnology,rvy09b
891,"Hi everyone!
Can someone explain to me how query, key and value vectors are received from the input word embeddings to an encoder or decoder layer? I see how they  are used in the multihead attention layer, but I dont understand where they come from. They have to depend on the input word embedding, but how?
In the original transformer paper  _URL_ I only found those vectors mentioned in chapter _NUMBER_:
*An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key* 
If anyone could help me answering this question, it would be great!
EDIT:
My thanks to /u/Brudaks, /u/boodleboodle and /u/mehtajineshs for clarification on this by providing explanations and resources.
I do understand now, that the vectors depend on the output from previous layers and are received by multiplying previous layer output  with randomly initialized matrices, like other weights in an FF network for example are initialized randomly as well.
And like weights in a feed forward network, the matrices for receiving QKV vectors are learned by backprop.",,LanguageTechnology,rri6dm
892,"Hi,
How to measure the accuracy of a generative chat bot or any generative model?
Is it possible?",,LanguageTechnology,rnnw4s
893,"I have a question around personalization for semantic searchwhen using semantic search api through a third party, for instance pinecone or any vector db company, how is personalization possible? Do you also pass the last _NUMBER_ session actions/ or info about the user through the api?",,LanguageTechnology,saiq21
894,"Hello all,
I would like to ask you which kind of tools do you use to search for something that is not in the training model when you do a computational analysis.
Let's say that I want to search for errors in a corpus , while the lemmatization procedure fails because these elements are wrong - how can you deal automatically with such events?
I hope that the question is clear enough :)",,LanguageTechnology,rqegda
895,"Hello guys, i'm data science student, i'm trying to replicate WebNLG _NUMBER_ challenge with OpenNMT-tf.
I have already performed the same challenge with OpenNMT-py and everything went well.
When using the tensoflow version, some doubts arose:
* how to build vocabularies from webnlg\_baseline\_input.py output files: \. since in the tensorflow version a transformation step in a bpe file is required;
* how to build the default model of openNMT-py ;
I tried to follow this notebook but, given the doubts expressed above, the results were not the same.
(_URL_ _URL_ 
How can I do? Thanks all.",,LanguageTechnology,sesv7z
896,"Hi,
I am looking at multi-lable classification for Twitter-like data. Does anybody know if any open source project  has any pre-trained models ready to use?
I am looking at classification taxonomy such as IAB v2 categories _URL_ or Wikipedia categories. 
Thanks!",,LanguageTechnology,s300v8
897,"ML and NLP Research Highlights of _NUMBER_ _URL_ by Sebastian Ruder, actually research scientist at Google in London, ex-DeepMind: Universal Models, Massive Multi-task Learning, Beyond the Transformer , Prompting, Efficient Methods, Benchmarking, Conditional Image Generation, ML for Science, Code Synthesis, Bias, Retrieval Augmentation, Token-free Models, Temporal Adaptation, Importance of Data and Meta-learning.",,LanguageTechnology,sds8q8
898,"So, I was in an interview and I was asked so many questions about statistical details on text data. For example 
_NUMBER_. How would you sample million sentences from billions of sentences? What strategies will you use for sampling?
_NUMBER_. Having sampled, how would determine that the sampled data follows actual data distribution? 
Follow up for these questions were, When will you decide to re-train your model. 
Now I am confused about how to perform such statistical analysis over text data. I have understanding about DL approaches within NLP, but stats is something bugging me a lot during the interviews. 
Please advice me how to solve these about mentioned questions as well as where should I start working/learning on stats for such questions. Will be very helpful.",,LanguageTechnology,sbhz36
899,"Hi, 
I'm looking for the latest papers on different subjects  . 
Typing on google is not helping. I find only commercial solutions. 
Thank you",,LanguageTechnology,s0q1m4
900,"So, for context, I've only just started learning NLP, and I've just encountered the word2vec algorithm for the first time. This algorithm calculates the probability of a word appearing at a position in a sentence as a function of what it's surrounding words are, weighted by the distance from that central word, learned from a large corpus of language. So for instance, if you fed it an incomplete sentence: ""the cat jumped over the ... "", it would assign high probabilities to words like ""table"", ""mat"", ""bed"", and assign low probabilities to words like ""blue"", ""boil"", ""running"". 
Are there any human languages in the world for which the assumptions which the algorithm are built on break? For example, any languages for which the context of a word is *inversely* proportional to it's semantic meaning, rather than proportional as this algorithm assumes? 
Are there any other interesting concepts in NLP which work for some languages, but not others?",,LanguageTechnology,rzp9h6
901,I’m planning to add this feature to a search bar. Any starting package/model/tool suggestion?,,LanguageTechnology,sczm2z
902,"Hey Everyone,
I'm building a plugin for end-to-end neural search  in Opensearch and I'd love to hear any suggestions from the NLP community of what you might find useful or about issues you've had with Opensearch/neural search in the past.
Despite the Elasticsearch website claiming in many places that you can do ""machine learning"" with Elasticsearch, I've found that it's not straight forward at all to use neural search algos with ES/Opensearch. In most cases , you have to implement the ML algorithm yourself and you only get to use ES as a storage layer. Some 3rd party plug and play frameworks that support ES seem quite promising but also lack functionality in terms of data retrieval - for example, Cherche seems to enforce that users first retrieve documents using an algorithm like tf-idf before putting them through neural search.
I want to build a plugin/service that will allow users to more readily take advantage of the vector functionality and neural search in general. I've found the following issues:
_NUMBER_. Opensearch/ES have added a great deal in terms of functionality to allow for vector search , but it seems entirely up to the user to encode the word embeddings. Therefore, users must add code to manually encode any documents and queries into their chosen embeddings before searching/adding data. I think users having their embeddings is generally a good idea if they want a high level of optimisation, but for many use cases, pretrained embeddings should be a ""good enough"" solution.
_NUMBER_. If the data is in text format, it cannot easily be converted into a format to be used with algorithms like SBERT etc without reindexing the entire index and running it through a custom script to change the data into a vector format.
_NUMBER_. I'd suspect for many users who arn't NLP experts, navigating all of the potential options for embeddings/Neural Search architecture could be quite overwhelming. Having a configurable plugin where they can try different options would likely help them to accelerate getting started.
I think letting users have their own embeddings makes sense from an optimisation perspective but I think also it would be amazing to have an end to end solution where you can connect different algos directly into Opensearch. I'm also exploring extending this and allowing users to refresh/update these embeddings to continually improve them.
Let me know what you think, open to any suggestions! If you want to keep up to date with this, here is a google form _URL_ _URL_",,LanguageTechnology,s5ethj
903,"As compared to multiple. My tokenizer currently splits “can’t” to “can”, “‘“, and “t”. I’m using Python",,LanguageTechnology,rx30rj
904,"I am quite used to the Machine learning aspects of NLP, but I am lacking knowledge on how to make raw texts accessible and how to handle meta data. Is there a good book on this - preferably in Python?",,LanguageTechnology,sh08de
905,Hi all. I was wondering what are the most interesting academic ethics issues we have to account for while researching in this area?,,LanguageTechnology,sg4teu
906,"I am looking for a good way to use pre-trained embedding models  in my C++ application running on a local CPU.
My solution so far: I am using a compiled Tensorflow C DLL in combination with cppflow (_URL_ However, I get problems when I take models which use operations from the tensorflow_text python module since I don’t know how to get their C++ API.
Has somebody experience in doing so? Or in general, has someone used local embedding models in a C++ app before?",,LanguageTechnology,sff0hf
907,"I am trying to vectorize _NUMBER_-news group data 
 using tensorflow TextVectorization layer 
 but in TextVectorization layer 
 if I limit the vocab size to some number say _NUMBER_ then it works fine. However if I preprocess the data or do not set the vocab size to some number then I get UnicodeDecodeError: 'utf-_NUMBER_' codec can't decode byte 0xfe in position _NUMBER_: invalid start byte 
 error.
My question is have I done something wrong in preprocessing? Because if I set the vocab size to _NUMBER_ and do the same preprocessing then this wont work. Also, do I need to set vocab size in 'TextVectorization\` but the docs says it can have unlimited size?
 
Here is what I did :
i. Get the list of files:
 train_dir_list = 
 for i in os.listdir:
 f = os.path.join
 for j in os.listdir:
 train_dir_list.append(os.path.join) 
ii. Create tensorflow data
 train_data = tf.data.TextLineDataset 
iii. Preprocess data
 def preprocess:
 lower = tf.strings.lower
 # remove emails
 email_removed = tf.strings.regex_replace
 # remove numbers
 number_removed = tf.strings.regex_replace
 # remove punctuations
 punctuation_removed = tf.strings.regex_replace( number_removed, '' % re.escape, ' ' )
 # remove multiple blank spaces
 multiple_space_removed =tf.strings.reduce_join(tf.strings.split, separator="" "")
 return multiple_space_removed 
iv. Create vectorizer: In here if I remove the standardize=preprocess 
 and keep the vocab\_size and sequence\_length 
 it works fine. But if I use standardize=preprocess 
 either with same vocab\_size and sequence\_length 
 or do not use standardize=preprocess 
 but keep the vocab\_size and sequence\_length 
 empty or as default then it gives the UnicodeDecodeError:
 vocab_size = _NUMBER_ sequence_length = _NUMBER_ 
This works fine:
 vectorize_layer = layers.TextVectorization 
This will throw error:
 vectorize_layer = layers.TextVectorization 
This will also give error :
 vectorize_layer = layers.TextVectorization 
v. calling adapt works fine
 vectorize_layer.adapt(train_data.batch) 
vi. This is were the error is thrown
 vectorize_layer.get_vocabulary 
Also, on looking up the vocab size when the error is produced, the vocab size is only around _NUMBER_-_NUMBER_",,LanguageTechnology,s6wvu7
908,Also possible in Python programming language,,LanguageTechnology,s2egjq
909,"Hello all,
Many of us are having a hard time speeding up our Transformer based NLP models for inference in production.
So I thought it would be worth writing an article that summarizes the options one should consider :
_URL_ _URL_
I hope you'll find it useful. If you can think of additional options, please let me know and I'll add them to the article!
Julien",,LanguageTechnology,s0h5nv
910,"What model would you recommend for extractive summarization?
I have a dataset of restaurant menus and I want to extract the dishes with their prices.
So the input will be the entire menu and the output will csv like text with the dishes as the first column and their prices as the second.
I was thinking of T5 but I just dabble in NLP maybe you have a better idea?
Thanks",,LanguageTechnology,rrehwy
911,"I tried using different packages but they all still just return a ""None"" value whenever i try to stem a word. Is it because of my python version ?",,LanguageTechnology,rsx4cz
912,"Hello Everyone,
I am a newbie in NLP research. My question is - How should we benchmark a new Language dataset/corpus  when there is no publicly available dataset for that particular language? Also what are the possible directions to perform evaluation on the newly prepared dataset. Need suggestions, please.",,LanguageTechnology,sdurp5
913,"Recently, I was reading some literature about text summarization and came across its evaluation metric, the ""ROUGE"" score. From what I understood from preliminary reading, the ROUGE score only measures n-gram overlap between candidate summary and reference summary which wrongly penalizes abstractive summaries containing different n-grams but conveying the same meaning. There's also a **BERTScore** metric  that does not suffer from these issues of ROUGE and computes contextual similarity rather than just n-gram overlap. How can I assess if BERTScore is a better evaluation metric compared to ROUGE? ",,LanguageTechnology,s5ao3z
914,"I'm thinking of creating a ChatBot. The function of the Chatbot will be the following:
Keywords: 
The keyword is used to identify the sentences which will contain salary information
Case _NUMBER_:
User: My Salary is 300K per month
Chatbot: Store _NUMBER_ in the database
Case _NUMBER_:
User: My monthly compensation is 2Lkh
ChatBot: Store _NUMBER_ in database
Explanation: 2Lkh is an amount that equates to _NUMBER_. Compensation is a word similar to one of the keywords
Case _NUMBER_:
User: My salary is quite low
ChatBot: No action
Case _NUMBER_:
User: My salary is 1M per year
ChatBot: Store _NUMBER_ in the database
For now, I'm only considering currency in rupees, and salary is stored regardless of whether it is monthly or yearly. Can you suggest to me a pipeline/Steps that I have to create in order to come up with this ChatBot?
I'm also not able to find the proper dataset for this task.",,LanguageTechnology,s7nrja
915,"The problem with my university is we have to first write proposal and only after approved we get a supervisor on thesis domain. But the proposal take lot of effort and time. Even choosing topic takes lot of time. So reddit is the only option I have.
I wanted my master thesis to be moderatly hard, so choosed to do empirical study instead and was finding hard to choose the topic. I found some research papers do empirical study, for eg: _URL_ _URL_ What do you guys think? Is doing empirical study on open source model like BERT is appropriate for master thesis?",,LanguageTechnology,s2gd3g
916,"Hello all,
Which are your favorite tools to visualize a corpus? 
Do you prefer a desktop or web solution? And which kind of analyses can you perform with such tools ?
Thanks in advance",,LanguageTechnology,rxc8t8
917,"I was offered a dream  NLP developer role in an AI project after many years in non-tech positions , which I was told required some knowledge of Python, which I have been teaching myself for the past _NUMBER_ months. I just realised however that their NLP tools are basically automated/low code. Will this hinder my progression into a NLP career? How do I leverage my experience from that type of role into something more er, substantial in the NLP space? My plan was/ is to teach myself Pandas, NumPy, Scikit-learn, NLTK, TensorFlow, etc. this year and complete some projects on my own. I hope that's sufficient for the next step.",,LanguageTechnology,s98t4o
918,"Do you know of any such experiments?
I keep reading about LLMs using external memory resources, but could they also be used to generate resources such as Knowledge Graphs on a huge scale?
Edit: preliminary results from a little experimentation with GPT-_NUMBER_ 
**Prompt**
knowledge graph described by a list of relations:
finger -> part of -> hand
finger -> subclass of -> digit
music -> subclass of -> sound
Earth -> instance of -> terrestrial planet
green -> subclass of -> color
pathogen -> opposite of -> nonpathogenic organism
color -> subclass of -> property
music -> part of -> culture
culture -> opposite of -> nature",,LanguageTechnology,rm5pzm
919,"Hey,
Could someone please simply explain the CKY algorithm, how it works?
let the input be a string I consisting of n characters: a1 ... an.
So the input is just characters, not tokenized words?
let the grammar contain r nonterminal symbols R1 ... Rr, with start symbol R1.
What are non terminal symbols?
let P be an array of booleans. 
This is a three dimensional array? Why does n occur twice?
Initialize all elements of P to false.
for each s = _NUMBER_ to n
 for each unit production Rv → as
 set P = true
What is unit production?
for each l = _NUMBER_ to n -- Length of span
 for each s = _NUMBER_ to n-l+_NUMBER_ -- Start of span
 for each p = _NUMBER_ to l-_NUMBER_ -- Partition of span
 for each production Ra → Rb Rc
 if P and P then set P = true
if P is true then
 I is member of language
else
 I is not member of language
_URL_
Thanks very much",,LanguageTechnology,sh0cip
920,"Is there any way to differentiate between the absence or inclusion of an object in a sentence? For example: there is not a cat in my yard, the cat disappeared from my yard, a cat does not exist in my yard, etc. versus there is a cat in my yard, the cat appeared in my yard, a cat exists in my yard.
Any help would be greatly appreciated!",,LanguageTechnology,s8j2qk
921,"_URL_ _URL_
I have put up some  parse trees online as a dataset. Not something as substantial as Penn Treebank, since the trees have NOT been human-edited, but still way more parse trees than from Penn to feed into your later-stage NLP algorithms, free of charge or hassle.
The current format is straight from where they were generated. Suggestions of alternative formats based on ease of use would be heavily appreciated!",,LanguageTechnology,s1d4p9
922,"I have to process undergraduate and postgraduate student essays using spaCy. One of my first step is to remove citations, both narrative and parenthetical ones. And I am using regex to do this. My regex is getting longer and longer and becoming very unwieldy. Moreover, I am assuming students are using APA 7th and not earlier versions or other styles entirely.
I am unable to get good results using NER or POS so have to rely on regex.
Are there any python NLP packages that will recognise academic citations, both narrative and parenthetical ones? E.g. ""Lee  said ..."", ""... in the study conducted "".",,LanguageTechnology,rvp049
923,I have _NUMBER_ sentences and _NUMBER_ classes . What can I do to learn something from them?,,LanguageTechnology,s9kbsr
924,"I want to use pre-trained fasttext model to load word embeddings for all words  in English. I have tried to read up how to do this , and so far haven't been able to get it to work.
I would appreciate it if someone could give me the code for it or point me to some tutorial that would help me do it. 
I am from a Statistics background, and a new Python User. Trying to navigate through everything to get things done !",,LanguageTechnology,sg20z5
925,"I’d like to challenge myself to write my own dependency parser for natural language  in Python.
I’m picturing taking in the sentence one word at a time and somehow working backwards to figure out the tree structure of the sentence.
Of course, it should start with tokenization. Perhaps part-of-speech tagging is a necessary next step, to attempt to group certain parts of speech that never dislocate from their complements, like “the”, or other words with predictable behavior, like “and” and conjunctions and so on?
Like the game “Mindmaster”, one can work backward layer upon layer to deduct what the original structure of the sentence is.
However, maybe I need effective segmentation for this to work? I’m wondering how periods will affect this procedure. I could ignore them and hope the dependency parse can still work. For example, “and” never appears at the end of a sentence.
Another idea is to try to design a neural network myself rather than using a library like Spacy. I just need to know what architecture is optimal and practice training it a bit.
Anyone have any recommendations on this?
Thanks very much",,LanguageTechnology,sd3sh6
926,"Hi, I'm looking for an Authorship Attribution dataset with small-medium texts . Looked everywhere but couldn't find any, found a great dataset for large texts  but none for small texts. Would like to search if one exists to hopefully not have to scrap it myself.",,LanguageTechnology,sac5np
927,"In the BLEU paper _URL_ _URL_ , why **In Example _NUMBER_, Candidate _NUMBER_ achieves a modified unigram precision of _NUMBER_/_NUMBER_; whereas Candidate _NUMBER_ achieves a modified unigram precision of _NUMBER_/_NUMBER_.** ?",,LanguageTechnology,s2neoo
928,"I’ve been interested in this for a long time, hoping to finally crack the code here.
In Swedish an adjective can be determined or not. Like in English the article “the” vs “a”.
This is a more obscure case, a phrase in a product catalog “On hard floor”.
Not sure what grammarians would say is going on here. It’s a general case so I would say it’s non-determined and they dropped the “a” just for brevity.
On the other hand it is more comparable to the general case which commonly uses the plural in English: “on hard floors”.
I would like to have a convenient system to check what is done in Swedish without just leafing through grammar websites and so on.
I want to access a most convenient Swedish corpus - not a database requiring a sign up but just an easily downloadable dataset, maybe Kaggle?, or as part of some software package like Spacy.
Then I want to execute a formula like “show me matches of sentences of the form “preposition determined adjective noun””. I can develop it from here but this would be a good start.
Does anyone have a suggestion for an accessible corpus with syntax parsing and searching?
Thank you very much",,LanguageTechnology,rmrmk1
929,"In the BERT paper _URL_ it says that during training it mask a fraction of the words and replaces them with random words:
> The training data generator chooses _NUMBER_% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with  the \ token _NUMBER_% of the time  a random token _NUMBER_% of the time  the unchanged i-th token _NUMBER_% of the time.
I can't wrap my head about the explaination it gives, can somebody point me somewhere about this part?
EDIT: What I don't understand is the justification to do the random word thing.",,LanguageTechnology,rvfi1w
930,"Hi everyone!
I am currently studying smoothing techniques, specifically Kneser-Ney smoothing. I understand that it helps to handle the case where the next word hasn't appeared in the given context previously. For eg, the corpus could have non zero trigram counts of 'This is a', but no occurrence of the _NUMBER_-gram 'This is a car'.
The count C is captured in the denominator of the lambda term as well, and this lambda term is multiplied with the recursion term. My question is, what if that particular count is actually zero? I hope the following the mini example can make my question clearer - 
Corpus:
<s> <s> You are my friend </s> </s>
<s> <s> They are my enemies </s> </s>
<s> <s> I have friends and enemies </s> </s>
Say we would like to find the probability of the trigram 'are you friend', or P. As per the formulation given in page _NUMBER_ of the document:
_URL_ _URL_
The denominator of two terms consists of the count of the bigram 'are you'. But from the corpus this is zero, and at the same time, each of the individual words 'are' and 'you' aren't UNK, as their unigram counts are _NUMBER_ and _NUMBER_ respectively. So how does the recursion proceed now, since we cannot divide by zero? 
Thank you!",,LanguageTechnology,scyef9
931,"I am thinking of creating model for extracting entities in a cv such as 
_NUMBER_. Name
_NUMBER_. Address
_NUMBER_. Institute
_NUMBER_. Degree
_NUMBER_. Skill
_NUMBER_. Company
_NUMBER_. School
_NUMBER_. Designation
_NUMBER_. Society - Ex: sport clubs, school societies... . In spaCy there are a very limited no of entities. What about training a model with these data ?",,LanguageTechnology,rm1jdc
932,"Hi all! I'm proving out an idea for emotion detection using smartphone recordings. Ideally I would like to gather recordings using a web-based application and the MediaRecorder API with smartphones . Does anyone have experience with doing so? Are the results good enough to work with, or am I better off working on a dedicated app with more control over recording?",,LanguageTechnology,rtngyl
933,"Hi everyone!
I'm an experimental social science researcher who is trying to get into some very basic NLP as a supplementary skillset.
I learned how to use LIWC  during my doctoral program, but haven't done anything related to NLP for at least _NUMBER_ years. I've skimmed through some posts here and someone said ""NLP has progressed a lot more from LIWC since the past couple years"" so I'm trying to get reacclimated with NLP.
Do you guys have any suggestions  on where to start? My goal is first to learn how to do the most basic sentiment analysis and/or any other elementary analyses using R, and then once comfortable, gradually move on to more advanced topics  ).
Another question I had was whether there was anything similar to LIWC in R, but again others seem to have commented on this subreddit that there are better tools than LIWC these days..?
Sorry for the really vague / general question - I would appreciate any comments or pointers!",,LanguageTechnology,rr9wgp
934,"Long-form question-answering , a paragraph-length answer created in response to an open-ended question, is a growing difficulty in NLP. LFQA systems hold the potential to become one of the most important ways for people to learn about the world, yet their performance currently lags behind that of humans. Existing research has tended to concentrate on two key aspects of the task: information retrieval and synthesis.
Researchers at OpenAI have recently developed WebGPT. They outsource document retrieval to the Microsoft Bing Web Search API and use unsupervised pre-training to produce high-quality synthesis by fine-tuning GPT-_NUMBER_. Rather than striving to improve these factors, they concentrate on integrating them with more consistent training goals. The team leverages human feedback to directly enhance the quality of answers, allowing them to compete with humans in terms of performance.
In this paper, the team offers two significant contributions. They create a text-based web-browsing environment that can be interacted with by a fine-tuned language model. This enables the use of general approaches like imitation learning and reinforcement learning to improve both retrieval and synthesis in an end-to-end manner. The team also creates replies with references, sections collected by the model when exploring web pages. This is critical because it allows labelers to assess the factual accuracy of answers without having to engage in a time-consuming and subjective independent research procedure.
Quick Read: _URL_ _URL_ 
Paper: _URL_ _URL_
Open AI Blog: _URL_",,LanguageTechnology,rmmn9e
935,"Prod2Vec or Item2Vec produces embedding for items in a latent space. The method is capable of inferring item-item relations even when user information is not available. It's based on NLP model Word2Vec. Click here _URL_ to know more
This project provide a class that encapsulates Item2Vec model (word2vec] as a (_URL_ and [BayesSearchCV _URL_ to find the optimal hyperparameters
_URL_ _URL_",,LanguageTechnology,s8o288
936,Stanford Online students will present original projects developed in the Natural Language Understanding professional course. Q&A to follow. Register here _URL_,,LanguageTechnology,s0vkki
937,Feedback appreciated.,,LanguageTechnology,s7jgxr
938,"I am searching for a natural text to speech or voice cloning program at least of the quality of synthesia.io _URL_ , Don't need the video part though. Preferably open source or something cheap.",,LanguageTechnology,rq2adm
939,"Hi, 
I am making a chatbot with the use of dialoGPT, but I want it to give different responses even if the same question is asked. 
eg: 
* How are you doing -> I am good
* How are you doing -> I am doing good
* How are you doing -> I am fine, how are you
something like that, so it doesn't seem repetitive.
This question might be stupid tho _EMOJI__EMOJI_",,LanguageTechnology,rmbz8l
940,"Hi guys, I'm a data science student and i'm trying to build a new dataset using OpenAi framework.
My idea is to use _NUMBER_ phrases and triples to train a data-to-text OpenAi model which given a triple generates of the text.
reading the OpneAi documentation, I found this script for reference:
 import os
 import openai
 
 openai.api_key = os.getenv
 
 response = openai.Completion.create
i would like that the above script follows this scructure: 
prompt = ""Triples: subject, predicate, object.\\nEnglish:text generated""
 import os
 import openai
 
 openai.api_key = ""mykey""
 
 response = openai.Completion.create
expected output from the last triple :
 The Phoenix is a fast food place in the riverside area.
For example, given the webnlg dataset  with this strcuture:
 <entry category=""Airport"" eid=""Id1"" size=""_NUMBER_"">
 <originaltripleset>
 <otriple>Aarhus_Airport | cityServed | ""Aarhus, Denmark""_USER_</otriple>
 </originaltripleset>
 <modifiedtripleset>
 <mtriple>Aarhus_Airport | cityServed | ""Aarhus, Denmark""</mtriple>
 </modifiedtripleset>
 <lex comment=""good"" lid=""Id1"">The Aarhus is the airport of Aarhus, Denmark.</lex>
 <lex comment=""good"" lid=""Id2"">Aarhus Airport serves the city of Aarhus, Denmark.</lex>
 </entry>
 
I would like to randomly extract sentences and their triples from this dataset and use them as training. How can I insert them into the ""prompt"" variable automatically without writing by hand ?.
Thanks!",,LanguageTechnology,s7niat
941,"January _NUMBER_, _NUMBER_ online: ""Compositional Natural Language Processing on Quantum Computers"" with Konstantinos Meichanetzidis, Cambridge Quantum](_URL_ & (_URL_ Info & RSVP: [_URL_ _URL_
\#NLProc #QuantumComputing",,LanguageTechnology,royiix
942,"Does anybody know if you can just import part of the Spacy library you need?
I find “import spacy” to be the slowest part of using spacy.
What takes so long to load? All the pipeline scripts or something? Because loading the language model with spacy.load and constructing the doc object with doc are faster than the initial import.
I’d like to just load the pipelines I need or something from the module, like “from spacy import ...”
Does anyone know of this is possible?
Thanks very much",,LanguageTechnology,sd2kmw
943,"I want to create a model that can trigger specific actions based on the input given to the model. For example, if the user asks where is the nearest petrol pump the model will trigger the google maps API and calculate the distance.",,LanguageTechnology,sccj0j
944,"Hi everyone, could use some input/thoughts on an NLP workflow I am helping to design based on a pretty unique situation .
Essentially this is a binary classification problem; we have around _NUMBER_ documents and because of the sheer messiness of the text, a team has provided us with the key phrases taken from these documents that denote whether the document should be a 'Yes'. As opposed to labeling the entire document itself, if that makes sense. 
Based on this, I was thinking that instead of using the entire set of documents as a corpus , that we would take the collection of key phrases as a corpus instead. Then vectorize it using the standard TF-IDF approach, then use that as the input to the classifier.
Couple questions on this:
_NUMBER_) Firstly, does the construction of the corpus in this manner even make sense, since we're not using the entirety of the document?
_NUMBER_) Is there a way to incorporate BERT embeddings into this? My supervisor is very keen on the cutting edge stuff and wants to incorporate BERT, but best I can tell there's not really a way to do this based on the workflows I have seen. The standard TF-IDF approaches seem different from the BERT workflow, which typically involves fine tuning on my corpus and then making predictions from there. 
Thanks!",,LanguageTechnology,sb6xiq
945,"I have a task where I need to come up with a system that takes sentences from dialogues and makes them more simple and clear. Sentences during dialogues are often filled with filler words such as uhm, uh and repetitions and my goal is to extract the information of the sentence and present it in a clear form.
E.g:
Original: ""And then I drew it on the board uhm white board because that's easier to understand for the student uh i mean students""
Simplified: ""I drew it on the whiteboard because it's easier to understand for the students""
I just took an introductory NLP course in my undergraduate degree so I'm still relatively new to the subject. Any learning resources and advice on how to do this would be greatly appreciated.",,LanguageTechnology,s2t02e
946,"I am doing a Master’s conversion course in Computer Science. Previously I did Linguistics for my undergrad. Due to my background, I am interested in conducting my thesis on Natural Language Processing. However, as I’ve never conducted a thesis, I don’t know where to start.
What are some good research topics I could read through to get a good idea? Any suggestions on where to start for a beginner in this field would be very much appreciated.",,LanguageTechnology,s0uwco
947,"Hello everyone!
After lots of research and failure, I finally was able to use BERT for classifying text in my dataset.
However, I feel like a dog that finally caught the car he has been chasing, because I am not sure what to do next.
I had a series of questions that I want to pursue but was hoping for a professional opinion.
First, I want to be able to look at some metrics for seeing how well my model performed. What are good metrics for a multiclass classification task? I know for a fact my classes are imbalanced, so what would be the best way to move forward with this?
In short, what do you ask yourselves once the model is done training and what do you do to evaluate it? How can I improve?
I am a nuclear engineer by trade and NLP/DL is still a very new concept and I was hoping to get insight from the masters out there.
Thanks in advance and happy new year!",,LanguageTechnology,rt6pvi
948,"I need to identify the \~ _NUMBER_ most common words in the English language as of some reliable / well-established corpus. I intended to use this repo _URL_ _URL_
but I realized that it is somewhat outdated. Any ideas where I could find a comprehensive, up-to-date list of the most common words in English? Thanks :)",,LanguageTechnology,sh7h2x
949,"Hey everyone,
Kaldi is a really powerful toolkit for ASR and related NLP tasks, but I've found that the learning curve is a bit steep.
**Here's** _URL_ **a tutorial I made that takes you through installation and transcription using pre-trained models**, but the cool part is that you can decide how advanced you want it to be!
Included are Python scripts to automate the entire process, so you can generate transcriptions in just a few lines of code, but I also dive into the code itself to explain what's going on under the hood!
I'd love to hear any thoughts and feedback, or future topics you want to see covered!",,LanguageTechnology,sbnic5
950,"I'm using symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli](_URL_ from HuggingFace. After multiple tries with different batch sizes, epochs, learning rates and even different unsupervised learning models methods such as [this _URL_ I couldn't get my sentence transformer to perform better than raw model straight from HuggingFace. I'm not sure what I'm doing wrong. I'm sure there are no bugs in my code since I followed the sentence transformer model documentation almost verbatim.
background on my task: my datasets consists of a list of sentences and a person will enter a query of say _NUMBER_-_NUMBER_ sentences and I'm supposed to find the ""correct"" matches for the query.
Currently, the base model isn't amazing but it's also not too bad. Hence, I expected better performance once I fine-tune it. However, upon fine-tuning, cosine similarity scores all drop and the fine-tuned model has never made a better prediction than the original model with no fine tuning.
I'd like to know why that might be the case and if that's a normal thing that usually happen with nlp models. My dataset is very small so my guess is my training parameters were bad? or is my training data so insignificant that my fine-tuning simply doesn't matter?",,LanguageTechnology,s6y0ah
951,"Hi all, I put together some material on fine-tuning reader models for open-domain question-answering _URL_ . ODQA is an increasingly popular approach to building more human/natural language information retrieval tools. Allowing users to store massive amounts of text data, and then search using natural language questions, it is one of the technologies that powers Google search. Reader models are the final step in an ODQA pipeline, allowing us to extract very specific answers to questions.
Let me know what you think, I hope it's useful, thanks!",,LanguageTechnology,s726uh
952,"In recent years, neural machine translation  has attracted a lot of attention and has had a lot of success. While traditional NMT is capable of translating a single language pair, training a separate model for each language pair is time-consuming, especially given the world’s thousands of languages. As a result, multilingual NMT is designed to handle many language pairs in a single model, lowering the cost of offline training and online deployment significantly. Furthermore, parameter sharing in multilingual neural machine translation promotes positive knowledge transfer between languages and is advantageous for low-resource translation.
Despite the advantages of cooperative training with a completely shared model, the MNMT approach has a model capacity problem. The shared parameters are more likely to preserve broad knowledge while ignoring language-specific knowledge. To improve the model capacity, researchers use heuristic design to create extra language-specific components and build a Multilingual neural machine translation  model with a mix of shared and language-specific characteristics, such as the language-specific attention, lightweight language adaptor, or language-specific routing layer. Continue Reading _URL_
Paper: _URL_
Github: _URL_",,LanguageTechnology,rvkz91
953,"Hi everyone!
I have a question for you. For context, we aggregate on a platform the various AI APIs on the market  and including NLP APIs . The idea is that a developer doesn't have to create accounts with different providers and can have them all on one API to test, compare and change whenever he wants.
However, many customers ask us how to mix the ""statistical"" approach behind these APIs with expert systems and how to achieve hybridization.
Do you have any idea how to do this?
Thanks,",,LanguageTechnology,rv4mf8
954,"I came across an interesting problem, Let's say given two sentences, ""Meet Harry and David and take them to London and Athens. These two cities are worth exploring. Here, the second sentence mentions that the two entities are cities. Is there any method to assign these two entities to category city? I am more concerned about different approaches we can use, may be rule based methods to deep learning based methods.",,LanguageTechnology,se48zj
955,"Hello, people. I still have some questions after reading the paper about Big Bird model  and will be happy if some Big Bird specialists will help me to understand this model better.
_NUMBER_. Is distribution of random attention (Figure _NUMBER_ ) fixed from advance for all inputs, or it somehow can be different for different inputs even on the same head?
_NUMBER_. In BIGBIRD-ETC, do they add some additional global tokens, aside of \?
_NUMBER_. In BIGBIRD-ITC, how is the subset of tokens for global attention chosen?
_NUMBER_. Why is infinite precision necessary for sparse transformer to be Turing complete?
Thank you.",,LanguageTechnology,s8lxk7
956,"The different categories include:
_NUMBER_. Posts that were deleted by the user themselves.
_NUMBER_. Posts that were banned by the Community moderators.
_NUMBER_. Posts that were banned by the Platform moderators.
_NUMBER_. Pages or communities containing posts that were banned by the Platform moderators.
I'm fairly uncertain about whether all the data that was pulled contains reasons for the ban they faced. In the case of deleted posts, there's no such label available.
Any idea how to go about this? Any link to cited paperwork that has faced and dealt with similar problems would be great. Links or mentions of authors who might have faced this issue also help as I can try reaching out to them. I'm having some trouble finding sources.
Even similar datasets links would be great as I can do a comparison study on this.
Thanks. :D",,LanguageTechnology,s4cjy3
957,"Newbie to this field, but nonetheless BERT was trained on _NUMBER_ billion+ words, when I do a masked learning task it is fairly successful on my healthcare dataset without fine tuning. However, when I fine tune the dataset, maybe adding only additional _NUMBER_ million words , suddenly the same task is significantly more accurate.
I understand that fine tuning is training the model on my specific task, so of course results will improve, but it is almost as if the model weights the fine-tuned additional words over itself. 
Why can such a small number of additional words improve the model so drastically?",,LanguageTechnology,rzuyqo
958,"Usually you move from rule-based segmentation to just machine learning when it gets too complicated.
But if we consider all rule based methods at our disposal - not just regular expressions but also having a corpus of words and symbols loaded as identifiers, plus various statistical methods that were used before machine learning like N-grams and so on -
I think it’s conceivable that if we give an AI certain parameters to play with from the gamut of rule-based segmentation methods, it could think of extraordinarily complex yet relatively effective explicit segmentation scripts.
Sort of like how Ramanujan could think of extremely complicated formulae in number theory that produced perfect results, and DeepMind recently tried to emulate a complex mind like that to discover new theorems in mathematics.
Does anyone know of any projects like this?
Thank you",,LanguageTechnology,rzuiy5
959,"Hi there, I'm trying to tackle quite a difficult problem with the help of sentence-transformer-models.
Ive got a bunch of JSON  files from different domains, which contain basically entities as JSON schemas consisting of data fields and descriptions. The entities can be ordered in kind of a hierarchical structure, which is not really strict though and may differ from file to file.
I assume that there exist common patterns between those files, precisely how the entities can be ordered in a semantically ""meaningful"" way . I would like to either
**a) Cluster the schemas to identify similarities between those entities**
What I tried: clustering the descriptions with KMEANS and SentenceTransformers. Problems here:
\- If I use only the descriptions they get clustered mostly by domain
\- If I try to cluster the ""raw"" JSON, most models don't find any similarity 
=> My idea here would be to fine-tune a model which encodes always two JSON parts as sentence input and I use the description similarity to generate either a classification score or even NLI scores, to train the model on this data, would this be a valid approach or what could be better ideas?
**b) More of a crazy but interesting idea: If I assume that the ""structure"" can be modeled as a ""sentence"" which consists of ""words""  than probably some sort of model could learn those ""sentences"".**
=> How to create ""words"" from sentences? I thought about creating sentence embeddings for all entities, and then building ""entity-sentences"" from the CLS-tokens? How to build a classifier for such ""sentences""? Are there any good approaches or is there any previous work done?
=> Does it make sense to create the model from scratch or would it be helpful to fine-tune an existing model with this approach?
=> Would it make sense to look at a completely different sort of ML technology?",,LanguageTechnology,rs2c2c
960,"Hi all. A little background: my mother is a Chinese immigrant who is always lacking self-esteem in her ability to speak ""correct"" English. Whenever she sends a text over to someone who is a native English speaker, she always bugs me to correct her sentences so it sounds more ""natural."" Her English is honestly fine at a conversational level, but could definitely use some editing.
I am wondering if there are NLP tools out there that can help my mom with this? Like if someone types a sentence like ""Hi, I almost done"" we can change it to something like ""Hi, I *am* almost done""?
Thanks in advance.",,LanguageTechnology,ruwav3
961,"Hello! I'm looking to annotate alignment matrices for neural machine translation. Does anyone know of a tool for doing this quickly? I mainly need a GUI interface where I can click and annotate. 
Couldn't find anything relevant from my initial Google searches.",,LanguageTechnology,rwsjix
962,"The demand for intelligent technologies that can interpret brief text has increased. As increasingly sophisticated solutions are produced, there is a greater need to improve and facilitate the creation of these complex situations. These scenarios are intelligent customer assistance bots to independent computers that interpret human input. The Language Cognitive Service has opted to employ a multilingual transformer-based paradigm to deal with such problems. When using this model, customers will notice a considerable increase in performance over the old Language Understanding Service .
Microsoft has released the next generation Conversational Language Understanding client library, allowing developers to use the Azure Cloud Conversational Language Understanding service to train models and use them in applications to provide related language services. Developers can use .NET or Python, and these libraries are currently under beta development.
Quick Read: _URL_ _URL_ 
* CLU documentation _URL_
* .NET reference documentation _URL_
* Python reference documentation _URL_
Microsoft Blog: _URL_",,LanguageTechnology,rpjdav
963,"Suppose you are writing a paper with some new transformer-variant that does well on classification tasks. You want to have a LSTM baseline. How would you go about choosing that LSTM architecture? What about training hyperparameters? Is there a standard, should it be grounded with respect to another paper, does it not matter as long as one explains what the architecture is?
Thanks!",,LanguageTechnology,s66i5v
964,"Hi NLP community,
Probably much of a newbie here and need some guidance. I am doing a personal project that aims to predict a person's industry from their short biography.
For example:
"" I am a retired engineer and company manager. I do not have a financial background or offer financial advice. blah blah "" => **Prediction:** ENGINEERING
and
"" Damon makes his living as a gap trader, an earnings trader, and an interday trader. In his free time, he writes for ABC, where he focuses on seasonal investing, market timing, and earnings analyses. "" => **Prediction:** FINANCE
I wanted to ask what approach should i do to make such predictions ? And what kind of public dataset would be useful to train a ML model for such task ?
Thank you so much !",,LanguageTechnology,s3iw6i
965,"How hard are automatic grammar checkers?
Over and over again I find that it'd be much smarter to have a program suggest whether your grammar is sound instead of trusting a person to keep track of all nuances over the course of a large number of sentences.
But I wonder how hard is it to write an automatic grammar checker.
Is it easier for languages like Lojban?",,LanguageTechnology,rywwfl
966,"txtai _NUMBER_ has been released with a number of new features.
* **Content Storage** \- Content can now be stored alongside embeddings vectors. No longer required to have external data storage.
* **Query with SQL** \- txtai supports both natural language queries and SQL queries 
`embeddings.search` 
`SELECT id, text, score FROM txtai WHERE similar AND score >= _NUMBER_`
* **Object Storage** \- Store binary objects alongside embeddings vectors
* **Reindex** \- Indexes can be rebuilt using stored content, no need to resend data to txtai
* **Index Compression** \- Indexes can be compressed using GZ/XZ/BZ2/ZIP
* **External Vectors** \- Use external vector models from an API or an external library. Centralize building vectors on GPU servers leaving index servers to be powered by more modest hardware.
More information can be found in following links.
* GitHub Project _URL_
* _NUMBER_ Release Notes _URL_
* What's new in txtai _NUMBER_ _URL_
* Documentation _URL_
* Examples _URL_",,LanguageTechnology,s25i2x
967,"Hello all, 
I am trying to pursue my MS in Computer Science and for funding I have been talking to Professor. He is asking me to brush up my knowledge on Deep Learning and NLP. I am scaring to my death. I have _NUMBER_ years of working experience but working in company is different from research. In company we just use library with surface understanding and working mechanism to get things done.
So, what might be the expectation of any Professor who is hiring any research assistant. What can be the possible questions any Professors ask in Deep Learning and NLP ?
Thanks in advance. Your help and support means a lot to me.",,LanguageTechnology,rxwrax
968,"Hi, I am looking for leads on how to tackle a NLP problem. 
In short: I have _NUMBER_ fixed  questions where I extracted in sum _NUMBER_ answers to from _NUMBER_ longer text documents .
I want to build a sort of question answering model that gets another text document  and suggests the question answering paragraphs of the document.
During Research the following questions arised: 
_NUMBER_. Looking through question answering models I am not sure if my text input might be too big?
_NUMBER_. Also is the problem too ""supervised"" for classical question answering approaches? 
_NUMBER_. Is there a way to transfer learn the supervised question answers from the document into existing models, any experiences on similar approaches?
Thank you all, appreciate your feedback.",,LanguageTechnology,saqjvi
969,"Hi All,
I am asking a pretty practical questions, so apologies if it seems overly simple. I want to unify a database of company names such that I get a list of all the parent companies. So if I have
Kroger Foods, Walmart Foods, Walmart Construction, Home Depot Construction, KMart Foods, KMart Toys, I would end up with: Kroger, Walmart, Home Depot, KMart.
I would also like to not be insensitive to word order. Just doing a BOW and filtering out common words gets me a big step there but not quite since there are always common words I haven't included. I am thinking a normalized vector with some sort of filtering out of words that have low normalized values  might be another way to do it but dont have good train/test data. My last thought was to use one of these fuzzy logic guides. Does anyone have an alternate idea or a good fuzzy logic tutorial that they recommend?",,LanguageTechnology,sd8hq7
970,"A lot of cloud computing services have a lot of vendors in the field but is there any other API for machine translation like Google Translate? DeepL or any others?
Thanks very much",,LanguageTechnology,s2gg6u
971,"Are there any open source Chinese language thesauruses? Akin to CEDICT, but with synonyms?
I have an application that could really make use of something like that, and without one existing, we'll essentially have to do it by hand, which is fairly laborious.",,LanguageTechnology,rvc54a
972,"Spacy is an industry standard, and I have been using it ever since I've been in the field. One thing I have always wanted is for spacy to have fit and predict methods, much like sklearn. I understand spacy has its own forms, like the evaluate method. Of course, it probably won't be hard to build a fit predict method based wrapper around spacy, but **I am wondering if anyone has ever come across any such wrapper?** 
Benefit of such a wrapper would be that when building retraining pipelines with models from various libraries, most of which use fit and predict, being able to call fit and predict on a spacy model would simplify things.",,LanguageTechnology,se6nzq
973,"Is there an equivalent concept in NLP to what high-level computer languages  do to manage user error?
That is:
* natural languages may see users doing errors 
In computer languages, however:
* low-level languages  see users doing errors 
* therefore people design higher level languages  that have features that correct these errors
Is there an equivalent concept for natural languages in NLP?",,LanguageTechnology,ryyhke
974,"I’m using KeyBERT to extract _NUMBER_ keywords from a file.
It was pretty slow when I did it for only _NUMBER_ keywords.
For _NUMBER_ it ran for almost _NUMBER_ minutes before I terminated it, I believe it was processing the entire time but it’s just a massive computation.
Can anyone advise me on speeding this up? I’m using a Digital Ocean Droplet.
What specs do I need to do something like this in hopefully a few seconds? Are we talking _NUMBER_-core CPU or a certain GPU or something?
Or is there any advice on how I can be certain it’s still running, even after like _NUMBER_ minutes?
How long would you expect an execution like this to take and why? What is it about BERT that is so computation-intensive?
Thank you",,LanguageTechnology,ruzq2s
975,"I have a postgres database that I want to use to store raw documents. These documents may contain lots of special characters.
I'm trying to insert the documents into a postgres db and I keep getting syntax errors. Not sure what the best approach to this is. 
Here is the code I'm using with psycopg2
sql\_statement = """""" 
PREPARE fooplan AS 
INSERT INTO ocr  VALUES ; 
EXECUTE fooplan;"""""".format 
cur.execute",,LanguageTechnology,rv4c4r
976,"I'm currently in the process of learning NLP. I am using catalyst on c#. 
I was able to run the sample programs and it was able to determine if the word is an noun, adjective, etc. But I can't find any sample for what I need.
Here is a summary of what I would like to achieve. 
I would like to extract certain information on a sentence. Lets say i have the following texts:
""Sally ate an orange this morning. ""
Or 
""Sally is hiding behind the cabinet and she is eating an orange. "" 
How do i use the nlp to extract what sally ate?",,LanguageTechnology,saas64
977,"I’ve been trying to do some basic keyword extraction and finding it harder than expected.
KeyBERT seems good but it requires a powerful GPU to be usably fast. That’s possible with AWS, but there’s a bit more set up.
I just tried PyTextRank, and I was surprised at the quality of the output - I wouldn’t say it was perfect either. Maybe I should set a threshold, like choose the top _NUMBER_ ranked keywords? It’s fine if we exclude potential good keywords just to have a smaller list of good ones.
Here’s a good article about _NUMBER_ different methods, which is helpful -
_URL_
In theory, Spacy and BERT seem like the best options but they’re both a little complex. 
I think KW extraction really only needs a few layers or as Spacy would call them pipelines.
_NUMBER_. accurate tokenization of words and punctuation symbols
_NUMBER_. accurate recognition of multi-word expressions  - think of it as “chunking”
_NUMBER_. Strong assessment of keyword “candidacy” for each MWE 
Of course, a good algorithm can often skip steps. Like BERT is so smart it doesn’t need anything but the input text.
Does anyone know of a simplest way to run a fast, effective keyword extraction?
I’m talking _NUMBER_ keywords in one second on a fast CPU.
Thanks very much",,LanguageTechnology,s7qml8
978,This _URL_ position is currently open and I wanted to share with you!,,LanguageTechnology,s30ccv
