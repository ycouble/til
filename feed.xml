<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://ycouble.github.io/til/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ycouble.github.io/til/" rel="alternate" type="text/html" /><updated>2021-12-05T05:57:10-06:00</updated><id>https://ycouble.github.io/til/feed.xml</id><title type="html">yco-til</title><subtitle>My Today I Learned blog</subtitle><entry><title type="html">La Famille BERT et les Transformers</title><link href="https://ycouble.github.io/til/fr/nlp/2021/11/23/bert.html" rel="alternate" type="text/html" title="La Famille BERT et les Transformers" /><published>2021-11-23T00:00:00-06:00</published><updated>2021-11-23T00:00:00-06:00</updated><id>https://ycouble.github.io/til/fr/nlp/2021/11/23/bert</id><author><name></name></author><category term="fr" /><category term="nlp" /><summary type="html"></summary></entry><entry><title type="html">Transfer Learning</title><link href="https://ycouble.github.io/til/en/ml/raw/2021/02/17/transfer_learning.html" rel="alternate" type="text/html" title="Transfer Learning" /><published>2021-02-17T00:00:00-06:00</published><updated>2021-02-17T00:00:00-06:00</updated><id>https://ycouble.github.io/til/en/ml/raw/2021/02/17/transfer_learning</id><author><name></name></author><category term="en" /><category term="ml" /><category term="raw" /><summary type="html"></summary></entry><entry><title type="html">Interpretability (Part 3)</title><link href="https://ycouble.github.io/til/xai/en/ml/2020/12/04/interpretability_chapter12_interpretable_models_logistic_regression.html" rel="alternate" type="text/html" title="Interpretability (Part 3)" /><published>2020-12-04T00:00:00-06:00</published><updated>2020-12-04T00:00:00-06:00</updated><id>https://ycouble.github.io/til/xai/en/ml/2020/12/04/interpretability_chapter12_interpretable_models_logistic_regression</id><author><name></name></author><category term="xai" /><category term="en" /><category term="ml" /><summary type="html">Some quotations A change in a feature by one unit changes the odds ratio (multiplicative) by a factor of exp(βj). We could also interpret it this way: A change in xj by one unit increases the log odds ratio by the value of the corresponding weight. These are the interpretations for the logistic regression model with different feature types: Numerical feature: If you increase the value of feature xj by one unit, the estimated odds change by a factor of exp(βj) Binary categorical feature: One of the two values of the feature is the reference category (in some languages, the one encoded in 0). Changing the feature xj from the reference category to the other category changes the estimated odds by a factor of exp(βj). Categorical feature with more than two categories: One solution to deal with multiple categories is one-hot-encoding, meaning that each category has its own column. You only need L-1 columns for a categorical feature with L categories, otherwise it is over-parameterized. The L-th category is then the reference category. You can use any other encoding that can be used in linear regression. The interpretation for each category then is equivalent to the interpretation of binary features. Intercept β0: When all numerical features are zero and the categorical features are at the reference category, the estimated odds are exp(β0). The interpretation of the intercept weight is usually not relevant.</summary></entry><entry><title type="html">Interpretability (Part 2)</title><link href="https://ycouble.github.io/til/xai/en/ml/2020/12/02/interpretability_chapter11_interpretable_models_linear_reg.html" rel="alternate" type="text/html" title="Interpretability (Part 2)" /><published>2020-12-02T00:00:00-06:00</published><updated>2020-12-02T00:00:00-06:00</updated><id>https://ycouble.github.io/til/xai/en/ml/2020/12/02/interpretability_chapter11_interpretable_models_linear_reg</id><author><name></name></author><category term="xai" /><category term="en" /><category term="ml" /><summary type="html">Interpretable Models - Linear Regression Linear regressions are simple models, which force the output to be a linear combination of the inputs. This means that the model is additive and thus easily explainable. The weights are the explaination.</summary></entry><entry><title type="html">Interpretability (Part 1)</title><link href="https://ycouble.github.io/til/xai/en/ml/2020/11/20/interpretability_chapter0.html" rel="alternate" type="text/html" title="Interpretability (Part 1)" /><published>2020-11-20T00:00:00-06:00</published><updated>2020-11-20T00:00:00-06:00</updated><id>https://ycouble.github.io/til/xai/en/ml/2020/11/20/interpretability_chapter0</id><author><name></name></author><category term="xai" /><category term="en" /><category term="ml" /><summary type="html">Interpretable Machine Learning Some notes on the small book Interpretable Machine Learning by Christoph Molnar.</summary></entry></feed>